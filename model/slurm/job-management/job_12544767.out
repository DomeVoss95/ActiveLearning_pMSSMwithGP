Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: uproot in /raven/u/dvoss/.local/lib/python3.9/site-packages (5.3.10)
Requirement already satisfied: numpy in /raven/u/system/soft/SLE_15/packages/x86_64/anaconda/3/2021.11/lib/python3.9/site-packages (from uproot) (1.20.3)
Requirement already satisfied: cramjam>=2.5.0 in /raven/u/dvoss/.local/lib/python3.9/site-packages (from uproot) (2.8.3)
Requirement already satisfied: awkward>=2.4.6 in /raven/u/dvoss/.local/lib/python3.9/site-packages (from uproot) (2.6.6)
Requirement already satisfied: typing-extensions>=4.1.0 in /raven/u/dvoss/.local/lib/python3.9/site-packages (from uproot) (4.12.2)
Requirement already satisfied: fsspec in /raven/u/dvoss/.local/lib/python3.9/site-packages (from uproot) (2024.6.1)
Requirement already satisfied: packaging in /raven/u/system/soft/SLE_15/packages/x86_64/anaconda/3/2021.11/lib/python3.9/site-packages (from uproot) (21.0)
Requirement already satisfied: awkward-cpp==35 in /raven/u/dvoss/.local/lib/python3.9/site-packages (from awkward>=2.4.6->uproot) (35)
Requirement already satisfied: importlib-metadata>=4.13.0 in /raven/u/dvoss/.local/lib/python3.9/site-packages (from awkward>=2.4.6->uproot) (8.0.0)
Requirement already satisfied: zipp>=0.5 in /raven/u/system/soft/SLE_15/packages/x86_64/anaconda/3/2021.11/lib/python3.9/site-packages (from importlib-metadata>=4.13.0->awkward>=2.4.6->uproot) (3.6.0)
Requirement already satisfied: pyparsing>=2.0.2 in /raven/u/system/soft/SLE_15/packages/x86_64/anaconda/3/2021.11/lib/python3.9/site-packages (from packaging->uproot) (3.0.4)
Defaulting to user installation because normal site-packages is not writeable
Collecting argparse
  Using cached argparse-1.4.0-py2.py3-none-any.whl (23 kB)
Installing collected packages: argparse
Successfully installed argparse-1.4.0
Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: pyslha in /raven/u/dvoss/.local/lib/python3.9/site-packages (3.2.6)
Requirement already satisfied: tex2pix>=0.1.5 in /raven/u/dvoss/.local/lib/python3.9/site-packages (from pyslha) (0.3.1)
Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: structlog in /raven/u/dvoss/.local/lib/python3.9/site-packages (24.4.0)
Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: imageio[ffmpeg] in /raven/u/system/soft/SLE_15/packages/x86_64/anaconda/3/2021.11/lib/python3.9/site-packages (2.9.0)
Requirement already satisfied: Pillow in /raven/u/system/soft/SLE_15/packages/x86_64/anaconda/3/2021.11/lib/python3.9/site-packages (8.4.0)
Requirement already satisfied: numpy in /raven/u/system/soft/SLE_15/packages/x86_64/anaconda/3/2021.11/lib/python3.9/site-packages (from imageio[ffmpeg]) (1.20.3)
Requirement already satisfied: imageio-ffmpeg in /raven/u/dvoss/.local/lib/python3.9/site-packages (from imageio[ffmpeg]) (0.5.1)
Requirement already satisfied: setuptools in /raven/u/system/soft/SLE_15/packages/x86_64/anaconda/3/2021.11/lib/python3.9/site-packages (from imageio-ffmpeg->imageio[ffmpeg]) (58.0.4)
Starting iteration 1
Initial Training points:  tensor([0.8528, 0.5812, 0.1617, 0.0046], device='cuda:0') torch.Size([4])
These training_points are used in the GP tensor([0.8528, 0.5812, 0.1617, 0.0046], device='cuda:0')
Point: tensor([0.0000, 0.0010, 0.0020, 0.0030, 0.0040, 0.0050, 0.0060, 0.0070, 0.0080,
        0.0090, 0.0100, 0.0110, 0.0120, 0.0130, 0.0140, 0.0150, 0.0160, 0.0170,
        0.0180, 0.0190, 0.0200, 0.0210, 0.0220, 0.0230, 0.0240, 0.0250, 0.0260,
        0.0270, 0.0280, 0.0290, 0.0300, 0.0310, 0.0320, 0.0330, 0.0340, 0.0350,
        0.0360, 0.0370, 0.0380, 0.0390, 0.0400, 0.0410, 0.0420, 0.0430, 0.0440,
        0.0450, 0.0460, 0.0470, 0.0480, 0.0490, 0.0501, 0.0511, 0.0521, 0.0531,
        0.0541, 0.0551, 0.0561, 0.0571, 0.0581, 0.0591, 0.0601, 0.0611, 0.0621,
        0.0631, 0.0641, 0.0651, 0.0661, 0.0671, 0.0681, 0.0691, 0.0701, 0.0711,
        0.0721, 0.0731, 0.0741, 0.0751, 0.0761, 0.0771, 0.0781, 0.0791, 0.0801,
        0.0811, 0.0821, 0.0831, 0.0841, 0.0851, 0.0861, 0.0871, 0.0881, 0.0891,
        0.0901, 0.0911, 0.0921, 0.0931, 0.0941, 0.0951, 0.0961, 0.0971, 0.0981,
        0.0991, 0.1001, 0.1011, 0.1021, 0.1031, 0.1041, 0.1051, 0.1061, 0.1071,
        0.1081, 0.1091, 0.1101, 0.1111, 0.1121, 0.1131, 0.1141, 0.1151, 0.1161,
        0.1171, 0.1181, 0.1191, 0.1201, 0.1211, 0.1221, 0.1231, 0.1241, 0.1251,
        0.1261, 0.1271, 0.1281, 0.1291, 0.1301, 0.1311, 0.1321, 0.1331, 0.1341,
        0.1351, 0.1361, 0.1371, 0.1381, 0.1391, 0.1401, 0.1411, 0.1421, 0.1431,
        0.1441, 0.1451, 0.1461, 0.1471, 0.1481, 0.1491, 0.1502, 0.1512, 0.1522,
        0.1532, 0.1542, 0.1552, 0.1562, 0.1572, 0.1582, 0.1592, 0.1602, 0.1612,
        0.1622, 0.1632, 0.1642, 0.1652, 0.1662, 0.1672, 0.1682, 0.1692, 0.1702,
        0.1712, 0.1722, 0.1732, 0.1742, 0.1752, 0.1762, 0.1772, 0.1782, 0.1792,
        0.1802, 0.1812, 0.1822, 0.1832, 0.1842, 0.1852, 0.1862, 0.1872, 0.1882,
        0.1892, 0.1902, 0.1912, 0.1922, 0.1932, 0.1942, 0.1952, 0.1962, 0.1972,
        0.1982, 0.1992, 0.2002, 0.2012, 0.2022, 0.2032, 0.2042, 0.2052, 0.2062,
        0.2072, 0.2082, 0.2092, 0.2102, 0.2112, 0.2122, 0.2132, 0.2142, 0.2152,
        0.2162, 0.2172, 0.2182, 0.2192, 0.2202, 0.2212, 0.2222, 0.2232, 0.2242,
        0.2252, 0.2262, 0.2272, 0.2282, 0.2292, 0.2302, 0.2312, 0.2322, 0.2332,
        0.2342, 0.2352, 0.2362, 0.2372, 0.2382, 0.2392, 0.2402, 0.2412, 0.2422,
        0.2432, 0.2442, 0.2452, 0.2462, 0.2472, 0.2482, 0.2492, 0.2503, 0.2513,
        0.2523, 0.2533, 0.2543, 0.2553, 0.2563, 0.2573, 0.2583, 0.2593, 0.2603,
        0.2613, 0.2623, 0.2633, 0.2643, 0.2653, 0.2663, 0.2673, 0.2683, 0.2693,
        0.2703, 0.2713, 0.2723, 0.2733, 0.2743, 0.2753, 0.2763, 0.2773, 0.2783,
        0.2793, 0.2803, 0.2813, 0.2823, 0.2833, 0.2843, 0.2853, 0.2863, 0.2873,
        0.2883, 0.2893, 0.2903, 0.2913, 0.2923, 0.2933, 0.2943, 0.2953, 0.2963,
        0.2973, 0.2983, 0.2993, 0.3003, 0.3013, 0.3023, 0.3033, 0.3043, 0.3053,
        0.3063, 0.3073, 0.3083, 0.3093, 0.3103, 0.3113, 0.3123, 0.3133, 0.3143,
        0.3153, 0.3163, 0.3173, 0.3183, 0.3193, 0.3203, 0.3213, 0.3223, 0.3233,
        0.3243, 0.3253, 0.3263, 0.3273, 0.3283, 0.3293, 0.3303, 0.3313, 0.3323,
        0.3333, 0.3343, 0.3353, 0.3363, 0.3373, 0.3383, 0.3393, 0.3403, 0.3413,
        0.3423, 0.3433, 0.3443, 0.3453, 0.3463, 0.3473, 0.3483, 0.3493, 0.3504,
        0.3514, 0.3524, 0.3534, 0.3544, 0.3554, 0.3564, 0.3574, 0.3584, 0.3594,
        0.3604, 0.3614, 0.3624, 0.3634, 0.3644, 0.3654, 0.3664, 0.3674, 0.3684,
        0.3694, 0.3704, 0.3714, 0.3724, 0.3734, 0.3744, 0.3754, 0.3764, 0.3774,
        0.3784, 0.3794, 0.3804, 0.3814, 0.3824, 0.3834, 0.3844, 0.3854, 0.3864,
        0.3874, 0.3884, 0.3894, 0.3904, 0.3914, 0.3924, 0.3934, 0.3944, 0.3954,
        0.3964, 0.3974, 0.3984, 0.3994, 0.4004, 0.4014, 0.4024, 0.4034, 0.4044,
        0.4054, 0.4064, 0.4074, 0.4084, 0.4094, 0.4104, 0.4114, 0.4124, 0.4134,
        0.4144, 0.4154, 0.4164, 0.4174, 0.4184, 0.4194, 0.4204, 0.4214, 0.4224,
        0.4234, 0.4244, 0.4254, 0.4264, 0.4274, 0.4284, 0.4294, 0.4304, 0.4314,
        0.4324, 0.4334, 0.4344, 0.4354, 0.4364, 0.4374, 0.4384, 0.4394, 0.4404,
        0.4414, 0.4424, 0.4434, 0.4444, 0.4454, 0.4464, 0.4474, 0.4484, 0.4494,
        0.4505, 0.4515, 0.4525, 0.4535, 0.4545, 0.4555, 0.4565, 0.4575, 0.4585,
        0.4595, 0.4605, 0.4615, 0.4625, 0.4635, 0.4645, 0.4655, 0.4665, 0.4675,
        0.4685, 0.4695, 0.4705, 0.4715, 0.4725, 0.4735, 0.4745, 0.4755, 0.4765,
        0.4775, 0.4785, 0.4795, 0.4805, 0.4815, 0.4825, 0.4835, 0.4845, 0.4855,
        0.4865, 0.4875, 0.4885, 0.4895, 0.4905, 0.4915, 0.4925, 0.4935, 0.4945,
        0.4955, 0.4965, 0.4975, 0.4985, 0.4995, 0.5005, 0.5015, 0.5025, 0.5035,
        0.5045, 0.5055, 0.5065, 0.5075, 0.5085, 0.5095, 0.5105, 0.5115, 0.5125,
        0.5135, 0.5145, 0.5155, 0.5165, 0.5175, 0.5185, 0.5195, 0.5205, 0.5215,
        0.5225, 0.5235, 0.5245, 0.5255, 0.5265, 0.5275, 0.5285, 0.5295, 0.5305,
        0.5315, 0.5325, 0.5335, 0.5345, 0.5355, 0.5365, 0.5375, 0.5385, 0.5395,
        0.5405, 0.5415, 0.5425, 0.5435, 0.5445, 0.5455, 0.5465, 0.5475, 0.5485,
        0.5495, 0.5506, 0.5516, 0.5526, 0.5536, 0.5546, 0.5556, 0.5566, 0.5576,
        0.5586, 0.5596, 0.5606, 0.5616, 0.5626, 0.5636, 0.5646, 0.5656, 0.5666,
        0.5676, 0.5686, 0.5696, 0.5706, 0.5716, 0.5726, 0.5736, 0.5746, 0.5756,
        0.5766, 0.5776, 0.5786, 0.5796, 0.5806, 0.5816, 0.5826, 0.5836, 0.5846,
        0.5856, 0.5866, 0.5876, 0.5886, 0.5896, 0.5906, 0.5916, 0.5926, 0.5936,
        0.5946, 0.5956, 0.5966, 0.5976, 0.5986, 0.5996, 0.6006, 0.6016, 0.6026,
        0.6036, 0.6046, 0.6056, 0.6066, 0.6076, 0.6086, 0.6096, 0.6106, 0.6116,
        0.6126, 0.6136, 0.6146, 0.6156, 0.6166, 0.6176, 0.6186, 0.6196, 0.6206,
        0.6216, 0.6226, 0.6236, 0.6246, 0.6256, 0.6266, 0.6276, 0.6286, 0.6296,
        0.6306, 0.6316, 0.6326, 0.6336, 0.6346, 0.6356, 0.6366, 0.6376, 0.6386,
        0.6396, 0.6406, 0.6416, 0.6426, 0.6436, 0.6446, 0.6456, 0.6466, 0.6476,
        0.6486, 0.6496, 0.6507, 0.6517, 0.6527, 0.6537, 0.6547, 0.6557, 0.6567,
        0.6577, 0.6587, 0.6597, 0.6607, 0.6617, 0.6627, 0.6637, 0.6647, 0.6657,
        0.6667, 0.6677, 0.6687, 0.6697, 0.6707, 0.6717, 0.6727, 0.6737, 0.6747,
        0.6757, 0.6767, 0.6777, 0.6787, 0.6797, 0.6807, 0.6817, 0.6827, 0.6837,
        0.6847, 0.6857, 0.6867, 0.6877, 0.6887, 0.6897, 0.6907, 0.6917, 0.6927,
        0.6937, 0.6947, 0.6957, 0.6967, 0.6977, 0.6987, 0.6997, 0.7007, 0.7017,
        0.7027, 0.7037, 0.7047, 0.7057, 0.7067, 0.7077, 0.7087, 0.7097, 0.7107,
        0.7117, 0.7127, 0.7137, 0.7147, 0.7157, 0.7167, 0.7177, 0.7187, 0.7197,
        0.7207, 0.7217, 0.7227, 0.7237, 0.7247, 0.7257, 0.7267, 0.7277, 0.7287,
        0.7297, 0.7307, 0.7317, 0.7327, 0.7337, 0.7347, 0.7357, 0.7367, 0.7377,
        0.7387, 0.7397, 0.7407, 0.7417, 0.7427, 0.7437, 0.7447, 0.7457, 0.7467,
        0.7477, 0.7487, 0.7497, 0.7508, 0.7518, 0.7528, 0.7538, 0.7548, 0.7558,
        0.7568, 0.7578, 0.7588, 0.7598, 0.7608, 0.7618, 0.7628, 0.7638, 0.7648,
        0.7658, 0.7668, 0.7678, 0.7688, 0.7698, 0.7708, 0.7718, 0.7728, 0.7738,
        0.7748, 0.7758, 0.7768, 0.7778, 0.7788, 0.7798, 0.7808, 0.7818, 0.7828,
        0.7838, 0.7848, 0.7858, 0.7868, 0.7878, 0.7888, 0.7898, 0.7908, 0.7918,
        0.7928, 0.7938, 0.7948, 0.7958, 0.7968, 0.7978, 0.7988, 0.7998, 0.8008,
        0.8018, 0.8028, 0.8038, 0.8048, 0.8058, 0.8068, 0.8078, 0.8088, 0.8098,
        0.8108, 0.8118, 0.8128, 0.8138, 0.8148, 0.8158, 0.8168, 0.8178, 0.8188,
        0.8198, 0.8208, 0.8218, 0.8228, 0.8238, 0.8248, 0.8258, 0.8268, 0.8278,
        0.8288, 0.8298, 0.8308, 0.8318, 0.8328, 0.8338, 0.8348, 0.8358, 0.8368,
        0.8378, 0.8388, 0.8398, 0.8408, 0.8418, 0.8428, 0.8438, 0.8448, 0.8458,
        0.8468, 0.8478, 0.8488, 0.8498, 0.8509, 0.8519, 0.8529, 0.8539, 0.8549,
        0.8559, 0.8569, 0.8579, 0.8589, 0.8599, 0.8609, 0.8619, 0.8629, 0.8639,
        0.8649, 0.8659, 0.8669, 0.8679, 0.8689, 0.8699, 0.8709, 0.8719, 0.8729,
        0.8739, 0.8749, 0.8759, 0.8769, 0.8779, 0.8789, 0.8799, 0.8809, 0.8819,
        0.8829, 0.8839, 0.8849, 0.8859, 0.8869, 0.8879, 0.8889, 0.8899, 0.8909,
        0.8919, 0.8929, 0.8939, 0.8949, 0.8959, 0.8969, 0.8979, 0.8989, 0.8999,
        0.9009, 0.9019, 0.9029, 0.9039, 0.9049, 0.9059, 0.9069, 0.9079, 0.9089,
        0.9099, 0.9109, 0.9119, 0.9129, 0.9139, 0.9149, 0.9159, 0.9169, 0.9179,
        0.9189, 0.9199, 0.9209, 0.9219, 0.9229, 0.9239, 0.9249, 0.9259, 0.9269,
        0.9279, 0.9289, 0.9299, 0.9309, 0.9319, 0.9329, 0.9339, 0.9349, 0.9359,
        0.9369, 0.9379, 0.9389, 0.9399, 0.9409, 0.9419, 0.9429, 0.9439, 0.9449,
        0.9459, 0.9469, 0.9479, 0.9489, 0.9499, 0.9510, 0.9520, 0.9530, 0.9540,
        0.9550, 0.9560, 0.9570, 0.9580, 0.9590, 0.9600, 0.9610, 0.9620, 0.9630,
        0.9640, 0.9650, 0.9660, 0.9670, 0.9680, 0.9690, 0.9700, 0.9710, 0.9720,
        0.9730, 0.9740, 0.9750, 0.9760, 0.9770, 0.9780, 0.9790, 0.9800, 0.9810,
        0.9820, 0.9830, 0.9840, 0.9850, 0.9860, 0.9870, 0.9880, 0.9890, 0.9900,
        0.9910, 0.9920, 0.9930, 0.9940, 0.9950, 0.9960, 0.9970, 0.9980, 0.9990,
        1.0000], device='cuda:0') - Variance: tensor([[0.0116],
        [0.0116],
        [0.0116],
        [0.0116],
        [0.0116],
        [0.0339],
        [0.0116],
        [0.0116],
        [0.0242],
        [0.0611],
        [0.0116],
        [0.0143],
        [0.0516],
        [0.0282],
        [0.0650],
        [0.1004],
        [0.0782],
        [0.1131],
        [0.1467],
        [0.1257],
        [0.1587],
        [0.1906],
        [0.2213],
        [0.2021],
        [0.2323],
        [0.2614],
        [0.2893],
        [0.1808],
        [0.2118],
        [0.1923],
        [0.2229],
        [0.2523],
        [0.2807],
        [0.3079],
        [0.3341],
        [0.3594],
        [0.3836],
        [0.4070],
        [0.4295],
        [0.4511],
        [0.4719],
        [0.4919],
        [0.5112],
        [0.5585],
        [0.5753],
        [0.5914],
        [0.6068],
        [0.6217],
        [0.6584],
        [0.6713],
        [0.6837],
        [0.6956],
        [0.7251],
        [0.7354],
        [0.7453],
        [0.7547],
        [0.7784],
        [0.7866],
        [0.8071],
        [0.8141],
        [0.8207],
        [0.8051],
        [0.8118],
        [0.8295],
        [0.8352],
        [0.8504],
        [0.8550],
        [0.8682],
        [0.8718],
        [0.8831],
        [0.8858],
        [0.8954],
        [0.9038],
        [0.9052],
        [0.9123],
        [0.9184],
        [0.9185],
        [0.9235],
        [0.9277],
        [0.9312],
        [0.9295],
        [0.9320],
        [0.9339],
        [0.9352],
        [0.9360],
        [0.9319],
        [0.9317],
        [0.9309],
        [0.9297],
        [0.9280],
        [0.9258],
        [0.9231],
        [0.9200],
        [0.9163],
        [0.8991],
        [0.8938],
        [0.8880],
        [0.8815],
        [0.8743],
        [0.8665],
        [0.8579],
        [0.8485],
        [0.8384],
        [0.8382],
        [0.8270],
        [0.8149],
        [0.8018],
        [0.7878],
        [0.7868],
        [0.7714],
        [0.7550],
        [0.7535],
        [0.7357],
        [0.7164],
        [0.7146],
        [0.6938],
        [0.6715],
        [0.6692],
        [0.6450],
        [0.6425],
        [0.6164],
        [0.5883],
        [0.5854],
        [0.5550],
        [0.5518],
        [0.5190],
        [0.5155],
        [0.5120],
        [0.4072],
        [0.4029],
        [0.3593],
        [0.3546],
        [0.3498],
        [0.3024],
        [0.2973],
        [0.2921],
        [0.2869],
        [0.2349],
        [0.2292],
        [0.2235],
        [0.2178],
        [0.1608],
        [0.1546],
        [0.1483],
        [0.1420],
        [0.1357],
        [0.1293],
        [0.1229],
        [0.1164],
        [0.1098],
        [0.1032],
        [0.0380],
        [0.0899],
        [0.0832],
        [0.0764],
        [0.0116],
        [0.0116],
        [0.0116],
        [0.0116],
        [0.0116],
        [0.0116],
        [0.0116],
        [0.0116],
        [0.0116],
        [0.0116],
        [0.0116],
        [0.0235],
        [0.0162],
        [0.0396],
        [0.0325],
        [0.0116],
        [0.0194],
        [0.0121],
        [0.0356],
        [0.0585],
        [0.0809],
        [0.0741],
        [0.0962],
        [0.1177],
        [0.1387],
        [0.1592],
        [0.1792],
        [0.1987],
        [0.2178],
        [0.2364],
        [0.2777],
        [0.2950],
        [0.2679],
        [0.2853],
        [0.3024],
        [0.3401],
        [0.3559],
        [0.3712],
        [0.4053],
        [0.4195],
        [0.4510],
        [0.4641],
        [0.4932],
        [0.5207],
        [0.5322],
        [0.5576],
        [0.5817],
        [0.5917],
        [0.6139],
        [0.6115],
        [0.6327],
        [0.6415],
        [0.6611],
        [0.6796],
        [0.6971],
        [0.7137],
        [0.7293],
        [0.7442],
        [0.7659],
        [0.7787],
        [0.7909],
        [0.8025],
        [0.8134],
        [0.8293],
        [0.8388],
        [0.8478],
        [0.8517],
        [0.8600],
        [0.8720],
        [0.8792],
        [0.8897],
        [0.8959],
        [0.9050],
        [0.9104],
        [0.9183],
        [0.9256],
        [0.9299],
        [0.9362],
        [0.9419],
        [0.9472],
        [0.9521],
        [0.9565],
        [0.9592],
        [0.9603],
        [0.9641],
        [0.9676],
        [0.9718],
        [0.9746],
        [0.9772],
        [0.9796],
        [0.9818],
        [0.9837],
        [0.9862],
        [0.9878],
        [0.9893],
        [0.9911],
        [0.9924],
        [0.9939],
        [0.9949],
        [0.9961],
        [0.9963],
        [0.9974],
        [0.9982],
        [0.9989],
        [0.9997],
        [1.0003],
        [1.0009],
        [1.0014],
        [1.0019],
        [1.0023],
        [1.0027],
        [1.0030],
        [1.0033],
        [1.0036],
        [1.0038],
        [1.0041],
        [1.0043],
        [1.0044],
        [1.0045],
        [1.0047],
        [1.0048],
        [1.0049],
        [1.0050],
        [1.0051],
        [1.0052],
        [1.0053],
        [1.0054],
        [1.0054],
        [1.0055],
        [1.0055],
        [1.0056],
        [1.0056],
        [1.0056],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0056],
        [1.0056],
        [1.0056],
        [1.0055],
        [1.0055],
        [1.0054],
        [1.0054],
        [1.0053],
        [1.0053],
        [1.0052],
        [1.0051],
        [1.0050],
        [1.0049],
        [1.0048],
        [1.0046],
        [1.0045],
        [1.0043],
        [1.0041],
        [1.0039],
        [1.0037],
        [1.0034],
        [1.0031],
        [1.0028],
        [1.0024],
        [1.0020],
        [1.0016],
        [1.0011],
        [1.0006],
        [1.0000],
        [0.9993],
        [0.9986],
        [0.9979],
        [0.9970],
        [0.9961],
        [0.9951],
        [0.9939],
        [0.9927],
        [0.9914],
        [0.9900],
        [0.9884],
        [0.9867],
        [0.9849],
        [0.9829],
        [0.9807],
        [0.9784],
        [0.9759],
        [0.9732],
        [0.9703],
        [0.9672],
        [0.9638],
        [0.9602],
        [0.9564],
        [0.9523],
        [0.9479],
        [0.9432],
        [0.9382],
        [0.9329],
        [0.9273],
        [0.9213],
        [0.9150],
        [0.9083],
        [0.9012],
        [0.8937],
        [0.8858],
        [0.8774],
        [0.8686],
        [0.8594],
        [0.8500],
        [0.8397],
        [0.8294],
        [0.8182],
        [0.8069],
        [0.7947],
        [0.7821],
        [0.7694],
        [0.7558],
        [0.7421],
        [0.7276],
        [0.7131],
        [0.6976],
        [0.6821],
        [0.6656],
        [0.6495],
        [0.6322],
        [0.6152],
        [0.5971],
        [0.5793],
        [0.5607],
        [0.5414],
        [0.5240],
        [0.5046],
        [0.4844],
        [0.4644],
        [0.4443],
        [0.4261],
        [0.4057],
        [0.3858],
        [0.3652],
        [0.3470],
        [0.3272],
        [0.3074],
        [0.2870],
        [0.2702],
        [0.2510],
        [0.2320],
        [0.2134],
        [0.1988],
        [0.1809],
        [0.1643],
        [0.1474],
        [0.1350],
        [0.1192],
        [0.1048],
        [0.0921],
        [0.0791],
        [0.0694],
        [0.0598],
        [0.0483],
        [0.0404],
        [0.0341],
        [0.0280],
        [0.0218],
        [0.0156],
        [0.0150],
        [0.0126],
        [0.0116],
        [0.0116],
        [0.0131],
        [0.0127],
        [0.0162],
        [0.0196],
        [0.0250],
        [0.0303],
        [0.0356],
        [0.0480],
        [0.0569],
        [0.0657],
        [0.0781],
        [0.0867],
        [0.0988],
        [0.1108],
        [0.1260],
        [0.1473],
        [0.1620],
        [0.1763],
        [0.1904],
        [0.2074],
        [0.2241],
        [0.2434],
        [0.2622],
        [0.2777],
        [0.3036],
        [0.3209],
        [0.3404],
        [0.3594],
        [0.3778],
        [0.3980],
        [0.4176],
        [0.4366],
        [0.4590],
        [0.4787],
        [0.4977],
        [0.5160],
        [0.5355],
        [0.5542],
        [0.5722],
        [0.5895],
        [0.6060],
        [0.6262],
        [0.6428],
        [0.6613],
        [0.6763],
        [0.6931],
        [0.7067],
        [0.7219],
        [0.7364],
        [0.7520],
        [0.7650],
        [0.7790],
        [0.7906],
        [0.8031],
        [0.8135],
        [0.8247],
        [0.8353],
        [0.8464],
        [0.8569],
        [0.8656],
        [0.8748],
        [0.8824],
        [0.8905],
        [0.8981],
        [0.9052],
        [0.9125],
        [0.9193],
        [0.9256],
        [0.9309],
        [0.9363],
        [0.9414],
        [0.9461],
        [0.9505],
        [0.9549],
        [0.9590],
        [0.9627],
        [0.9662],
        [0.9694],
        [0.9723],
        [0.9750],
        [0.9775],
        [0.9799],
        [0.9820],
        [0.9842],
        [0.9861],
        [0.9879],
        [0.9894],
        [0.9909],
        [0.9923],
        [0.9935],
        [0.9947],
        [0.9958],
        [0.9967],
        [0.9976],
        [0.9984],
        [0.9992],
        [0.9998],
        [1.0004],
        [1.0010],
        [1.0014],
        [1.0019],
        [1.0023],
        [1.0027],
        [1.0030],
        [1.0033],
        [1.0036],
        [1.0038],
        [1.0040],
        [1.0042],
        [1.0044],
        [1.0046],
        [1.0047],
        [1.0049],
        [1.0050],
        [1.0051],
        [1.0051],
        [1.0052],
        [1.0053],
        [1.0054],
        [1.0054],
        [1.0055],
        [1.0055],
        [1.0055],
        [1.0056],
        [1.0056],
        [1.0056],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0056],
        [1.0056],
        [1.0056],
        [1.0055],
        [1.0055],
        [1.0055],
        [1.0054],
        [1.0054],
        [1.0053],
        [1.0052],
        [1.0051],
        [1.0050],
        [1.0049],
        [1.0048],
        [1.0047],
        [1.0045],
        [1.0044],
        [1.0042],
        [1.0040],
        [1.0039],
        [1.0036],
        [1.0034],
        [1.0030],
        [1.0027],
        [1.0023],
        [1.0019],
        [1.0015],
        [1.0008],
        [1.0003],
        [0.9997],
        [0.9990],
        [0.9983],
        [0.9975],
        [0.9963],
        [0.9953],
        [0.9941],
        [0.9937],
        [0.9928],
        [0.9914],
        [0.9898],
        [0.9881],
        [0.9862],
        [0.9841],
        [0.9825],
        [0.9800],
        [0.9772],
        [0.9751],
        [0.9718],
        [0.9693],
        [0.9654],
        [0.9624],
        [0.9577],
        [0.9542],
        [0.9522],
        [0.9483],
        [0.9440],
        [0.9374],
        [0.9323],
        [0.9269],
        [0.9211],
        [0.9149],
        [0.9051],
        [0.8977],
        [0.8898],
        [0.8813],
        [0.8721],
        [0.8623],
        [0.8517],
        [0.8455],
        [0.8337],
        [0.8325],
        [0.8197],
        [0.8122],
        [0.7980],
        [0.7827],
        [0.7737],
        [0.7566],
        [0.7383],
        [0.7275],
        [0.7070],
        [0.6949],
        [0.6721],
        [0.6586],
        [0.6445],
        [0.6179],
        [0.6022],
        [0.5859],
        [0.5960],
        [0.5794],
        [0.5480],
        [0.5295],
        [0.5102],
        [0.4902],
        [0.4693],
        [0.4476],
        [0.4250],
        [0.4015],
        [0.3770],
        [0.3717],
        [0.3461],
        [0.3193],
        [0.2916],
        [0.2855],
        [0.2564],
        [0.2743],
        [0.2681],
        [0.2383],
        [0.2318],
        [0.2005],
        [0.1936],
        [0.1608],
        [0.1536],
        [0.1464],
        [0.1116],
        [0.1041],
        [0.0964],
        [0.0888],
        [0.0516],
        [0.0436],
        [0.1235],
        [0.1160],
        [0.1085],
        [0.0426],
        [0.0933],
        [0.0856],
        [0.0778],
        [0.0699],
        [0.0620],
        [0.0540],
        [0.0460],
        [0.0379],
        [0.0297],
        [0.0214],
        [0.0732],
        [0.0653],
        [0.0574],
        [0.0494],
        [0.0997],
        [0.0920],
        [0.0843],
        [0.0765],
        [0.1254],
        [0.1180],
        [0.1105],
        [0.1576],
        [0.1504],
        [0.2927],
        [0.2867],
        [0.2806],
        [0.3188],
        [0.3130],
        [0.3494],
        [0.3439],
        [0.3787],
        [0.4117],
        [0.4067],
        [0.4382],
        [0.4334],
        [0.4636],
        [0.4921],
        [0.4878],
        [0.5150],
        [0.5409],
        [0.5369],
        [0.5616],
        [0.5850],
        [0.5815],
        [0.6038],
        [0.6250],
        [0.6450],
        [0.6640],
        [0.6611],
        [0.6793],
        [0.6964],
        [0.7127],
        [0.7282],
        [0.7428],
        [0.7566],
        [0.7697],
        [0.7822],
        [0.8194],
        [0.8292],
        [0.8385],
        [0.8473],
        [0.8556],
        [0.8635],
        [0.8710],
        [0.8781],
        [0.8849],
        [0.8982],
        [0.9038],
        [0.9092],
        [0.9143],
        [0.9191],
        [0.9286],
        [0.9327],
        [0.9366],
        [0.9402],
        [0.9474],
        [0.9505],
        [0.9534],
        [0.9592],
        [0.9616],
        [0.9640],
        [0.9686],
        [0.9705],
        [0.9744],
        [0.9761],
        [0.9793],
        [0.9807],
        [0.9835],
        [0.9847],
        [0.9870],
        [0.9901],
        [0.9919],
        [0.9934],
        [0.9941],
        [0.9953],
        [0.9959],
        [0.9970],
        [0.9980],
        [0.9988],
        [0.9992],
        [0.9999],
        [1.0006],
        [1.0012],
        [1.0014],
        [1.0019],
        [1.0023],
        [1.0027],
        [1.0031],
        [1.0032],
        [1.0035],
        [1.0037],
        [1.0040],
        [1.0042],
        [1.0044],
        [1.0045],
        [1.0047],
        [1.0048],
        [1.0049],
        [1.0050],
        [1.0051],
        [1.0052],
        [1.0053],
        [1.0053],
        [1.0054],
        [1.0055],
        [1.0055],
        [1.0056],
        [1.0056],
        [1.0056],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058],
        [1.0058]], device='cuda:0') 
Smoothed_batch_entropy:  tensor([0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0084, 0.0030, 0.0030, 0.0060,
        0.0147, 0.0029, 0.0036, 0.0124, 0.0069, 0.0153, 0.0228, 0.0180, 0.0252,
        0.0315, 0.0275, 0.0337, 0.0390, 0.0440, 0.0408, 0.0456, 0.0498, 0.0536,
        0.0367, 0.0418, 0.0385, 0.0433, 0.0476, 0.0515, 0.0550, 0.0582, 0.0611,
        0.0637, 0.0660, 0.0681, 0.0701, 0.0717, 0.0733, 0.0746, 0.0780, 0.0789,
        0.0798, 0.0805, 0.0811, 0.0829, 0.0832, 0.0835, 0.0838, 0.0847, 0.0847,
        0.0848, 0.0848, 0.0852, 0.0850, 0.0851, 0.0849, 0.0847, 0.0842, 0.0839,
        0.0838, 0.0835, 0.0833, 0.0829, 0.0826, 0.0822, 0.0817, 0.0813, 0.0808,
        0.0803, 0.0797, 0.0791, 0.0786, 0.0779, 0.0773, 0.0767, 0.0761, 0.0754,
        0.0747, 0.0740, 0.0734, 0.0727, 0.0718, 0.0711, 0.0704, 0.0697, 0.0689,
        0.0682, 0.0674, 0.0666, 0.0658, 0.0644, 0.0635, 0.0626, 0.0617, 0.0607,
        0.0597, 0.0587, 0.0577, 0.0566, 0.0562, 0.0551, 0.0539, 0.0527, 0.0515,
        0.0512, 0.0499, 0.0485, 0.0482, 0.0468, 0.0453, 0.0450, 0.0434, 0.0418,
        0.0415, 0.0398, 0.0395, 0.0377, 0.0358, 0.0355, 0.0335, 0.0332, 0.0310,
        0.0307, 0.0304, 0.0240, 0.0236, 0.0210, 0.0206, 0.0203, 0.0175, 0.0171,
        0.0168, 0.0165, 0.0134, 0.0131, 0.0127, 0.0123, 0.0091, 0.0087, 0.0083,
        0.0080, 0.0076, 0.0072, 0.0068, 0.0065, 0.0061, 0.0057, 0.0021, 0.0050,
        0.0046, 0.0042, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006,
        0.0006, 0.0006, 0.0006, 0.0006, 0.0013, 0.0009, 0.0021, 0.0017, 0.0006,
        0.0010, 0.0006, 0.0019, 0.0031, 0.0043, 0.0039, 0.0051, 0.0063, 0.0073,
        0.0084, 0.0095, 0.0105, 0.0116, 0.0125, 0.0147, 0.0156, 0.0142, 0.0151,
        0.0160, 0.0180, 0.0188, 0.0196, 0.0215, 0.0222, 0.0239, 0.0246, 0.0262,
        0.0277, 0.0283, 0.0296, 0.0309, 0.0315, 0.0327, 0.0325, 0.0337, 0.0341,
        0.0352, 0.0362, 0.0372, 0.0381, 0.0389, 0.0397, 0.0410, 0.0417, 0.0424,
        0.0430, 0.0436, 0.0445, 0.0451, 0.0456, 0.0458, 0.0462, 0.0469, 0.0473,
        0.0479, 0.0483, 0.0488, 0.0491, 0.0496, 0.0500, 0.0502, 0.0506, 0.0509,
        0.0513, 0.0515, 0.0518, 0.0519, 0.0519, 0.0522, 0.0524, 0.0526, 0.0528,
        0.0529, 0.0530, 0.0531, 0.0532, 0.0534, 0.0535, 0.0535, 0.0536, 0.0537,
        0.0537, 0.0538, 0.0538, 0.0538, 0.0538, 0.0538, 0.0538, 0.0538, 0.0538,
        0.0538, 0.0538, 0.0538, 0.0538, 0.0538, 0.0538, 0.0537, 0.0537, 0.0537,
        0.0536, 0.0536, 0.0536, 0.0535, 0.0535, 0.0534, 0.0534, 0.0533, 0.0533,
        0.0532, 0.0532, 0.0531, 0.0531, 0.0530, 0.0529, 0.0529, 0.0528, 0.0528,
        0.0527, 0.0526, 0.0526, 0.0525, 0.0525, 0.0524, 0.0523, 0.0523, 0.0522,
        0.0521, 0.0521, 0.0520, 0.0519, 0.0519, 0.0518, 0.0517, 0.0517, 0.0516,
        0.0515, 0.0515, 0.0514, 0.0513, 0.0513, 0.0512, 0.0511, 0.0511, 0.0510,
        0.0509, 0.0509, 0.0508, 0.0507, 0.0507, 0.0506, 0.0505, 0.0505, 0.0504,
        0.0503, 0.0503, 0.0502, 0.0501, 0.0501, 0.0500, 0.0499, 0.0499, 0.0498,
        0.0497, 0.0497, 0.0496, 0.0495, 0.0495, 0.0494, 0.0493, 0.0493, 0.0492,
        0.0491, 0.0491, 0.0490, 0.0489, 0.0489, 0.0488, 0.0488, 0.0487, 0.0486,
        0.0486, 0.0485, 0.0484, 0.0484, 0.0483, 0.0482, 0.0482, 0.0481, 0.0481,
        0.0480, 0.0479, 0.0479, 0.0478, 0.0477, 0.0477, 0.0476, 0.0475, 0.0475,
        0.0474, 0.0474, 0.0473, 0.0472, 0.0472, 0.0471, 0.0471, 0.0470, 0.0469,
        0.0469, 0.0468, 0.0468, 0.0467, 0.0466, 0.0466, 0.0465, 0.0464, 0.0464,
        0.0463, 0.0463, 0.0462, 0.0461, 0.0461, 0.0460, 0.0460, 0.0459, 0.0459,
        0.0458, 0.0457, 0.0457, 0.0456, 0.0456, 0.0455, 0.0454, 0.0454, 0.0453,
        0.0453, 0.0452, 0.0451, 0.0451, 0.0450, 0.0450, 0.0449, 0.0448, 0.0448,
        0.0447, 0.0447, 0.0446, 0.0445, 0.0445, 0.0444, 0.0444, 0.0443, 0.0442,
        0.0442, 0.0441, 0.0441, 0.0440, 0.0439, 0.0439, 0.0438, 0.0438, 0.0437,
        0.0436, 0.0436, 0.0435, 0.0434, 0.0434, 0.0433, 0.0432, 0.0431, 0.0431,
        0.0430, 0.0429, 0.0429, 0.0428, 0.0427, 0.0426, 0.0425, 0.0425, 0.0424,
        0.0423, 0.0422, 0.0421, 0.0420, 0.0419, 0.0418, 0.0417, 0.0416, 0.0415,
        0.0414, 0.0413, 0.0412, 0.0411, 0.0410, 0.0409, 0.0407, 0.0406, 0.0405,
        0.0403, 0.0402, 0.0400, 0.0399, 0.0397, 0.0396, 0.0394, 0.0392, 0.0390,
        0.0389, 0.0387, 0.0385, 0.0383, 0.0381, 0.0378, 0.0376, 0.0374, 0.0372,
        0.0369, 0.0367, 0.0364, 0.0361, 0.0359, 0.0356, 0.0353, 0.0350, 0.0347,
        0.0344, 0.0340, 0.0337, 0.0334, 0.0330, 0.0262, 0.0257, 0.0252, 0.0247,
        0.0242, 0.0237, 0.0231, 0.0226, 0.0221, 0.0216, 0.0210, 0.0205, 0.0199,
        0.0194, 0.0188, 0.0183, 0.0177, 0.0172, 0.0166, 0.0161, 0.0155, 0.0149,
        0.0144, 0.0139, 0.0133, 0.0128, 0.0123, 0.0117, 0.0112, 0.0107, 0.0102,
        0.0097, 0.0092, 0.0087, 0.0082, 0.0078, 0.0073, 0.0069, 0.0065, 0.0060,
        0.0056, 0.0052, 0.0048, 0.0045, 0.0041, 0.0037, 0.0035, 0.0031, 0.0028,
        0.0025, 0.0023, 0.0020, 0.0018, 0.0015, 0.0013, 0.0011, 0.0010, 0.0008,
        0.0007, 0.0006, 0.0005, 0.0004, 0.0003, 0.0002, 0.0002, 0.0002, 0.0002,
        0.0002, 0.0002, 0.0003, 0.0003, 0.0004, 0.0005, 0.0006, 0.0008, 0.0009,
        0.0011, 0.0013, 0.0014, 0.0016, 0.0018, 0.0021, 0.0024, 0.0027, 0.0029,
        0.0032, 0.0035, 0.0038, 0.0042, 0.0045, 0.0048, 0.0053, 0.0056, 0.0060,
        0.0064, 0.0068, 0.0072, 0.0076, 0.0080, 0.0085, 0.0090, 0.0094, 0.0099,
        0.0103, 0.0108, 0.0112, 0.0117, 0.0121, 0.0127, 0.0131, 0.0137, 0.0141,
        0.0146, 0.0150, 0.0155, 0.0160, 0.0165, 0.0169, 0.0174, 0.0179, 0.0183,
        0.0187, 0.0192, 0.0196, 0.0201, 0.0205, 0.0209, 0.0213, 0.0217, 0.0221,
        0.0225, 0.0228, 0.0232, 0.0236, 0.0240, 0.0243, 0.0246, 0.0249, 0.0252,
        0.0255, 0.0258, 0.0261, 0.0264, 0.0267, 0.0269, 0.0272, 0.0274, 0.0276,
        0.0279, 0.0281, 0.0283, 0.0285, 0.0287, 0.0289, 0.0290, 0.0292, 0.0293,
        0.0295, 0.0296, 0.0298, 0.0299, 0.0300, 0.0302, 0.0303, 0.0304, 0.0304,
        0.0305, 0.0306, 0.0307, 0.0308, 0.0308, 0.0309, 0.0310, 0.0310, 0.0311,
        0.0311, 0.0311, 0.0312, 0.0312, 0.0313, 0.0313, 0.0313, 0.0313, 0.0314,
        0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314,
        0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314,
        0.0314, 0.0314, 0.0314, 0.0313, 0.0313, 0.0313, 0.0313, 0.0313, 0.0313,
        0.0313, 0.0313, 0.0313, 0.0313, 0.0313, 0.0313, 0.0312, 0.0312, 0.0312,
        0.0312, 0.0312, 0.0312, 0.0312, 0.0312, 0.0312, 0.0313, 0.0313, 0.0313,
        0.0313, 0.0313, 0.0313, 0.0313, 0.0313, 0.0313, 0.0313, 0.0314, 0.0314,
        0.0314, 0.0314, 0.0315, 0.0315, 0.0315, 0.0315, 0.0316, 0.0316, 0.0317,
        0.0317, 0.0317, 0.0317, 0.0318, 0.0318, 0.0318, 0.0319, 0.0319, 0.0320,
        0.0320, 0.0320, 0.0321, 0.0321, 0.0321, 0.0321, 0.0322, 0.0322, 0.0322,
        0.0322, 0.0322, 0.0322, 0.0322, 0.0322, 0.0321, 0.0321, 0.0321, 0.0320,
        0.0320, 0.0319, 0.0318, 0.0317, 0.0315, 0.0315, 0.0313, 0.0312, 0.0311,
        0.0309, 0.0307, 0.0304, 0.0302, 0.0299, 0.0295, 0.0293, 0.0288, 0.0285,
        0.0280, 0.0276, 0.0273, 0.0265, 0.0261, 0.0256, 0.0258, 0.0253, 0.0244,
        0.0238, 0.0231, 0.0225, 0.0217, 0.0209, 0.0201, 0.0192, 0.0183, 0.0180,
        0.0170, 0.0159, 0.0147, 0.0144, 0.0131, 0.0139, 0.0136, 0.0123, 0.0119,
        0.0105, 0.0101, 0.0085, 0.0082, 0.0078, 0.0061, 0.0057, 0.0052, 0.0048,
        0.0029, 0.0024, 0.0066, 0.0062, 0.0058, 0.0024, 0.0050, 0.0046, 0.0042,
        0.0038, 0.0034, 0.0029, 0.0025, 0.0021, 0.0016, 0.0012, 0.0039, 0.0035,
        0.0031, 0.0027, 0.0053, 0.0049, 0.0045, 0.0040, 0.0065, 0.0061, 0.0057,
        0.0080, 0.0076, 0.0139, 0.0136, 0.0133, 0.0148, 0.0146, 0.0160, 0.0158,
        0.0170, 0.0182, 0.0180, 0.0190, 0.0188, 0.0198, 0.0207, 0.0205, 0.0213,
        0.0220, 0.0219, 0.0226, 0.0232, 0.0231, 0.0236, 0.0241, 0.0245, 0.0250,
        0.0249, 0.0252, 0.0256, 0.0259, 0.0261, 0.0264, 0.0266, 0.0268, 0.0269,
        0.0274, 0.0275, 0.0276, 0.0276, 0.0277, 0.0277, 0.0278, 0.0278, 0.0278,
        0.0278, 0.0278, 0.0278, 0.0278, 0.0278, 0.0277, 0.0277, 0.0277, 0.0276,
        0.0276, 0.0275, 0.0275, 0.0274, 0.0273, 0.0273, 0.0272, 0.0272, 0.0271,
        0.0270, 0.0269, 0.0269, 0.0268, 0.0267, 0.0266, 0.0265, 0.0264, 0.0263,
        0.0263, 0.0262, 0.0261, 0.0261, 0.0260, 0.0259, 0.0259, 0.0258, 0.0257,
        0.0256, 0.0256, 0.0255, 0.0255, 0.0254, 0.0253, 0.0253, 0.0252, 0.0252,
        0.0251, 0.0251, 0.0250, 0.0250, 0.0249, 0.0249, 0.0248, 0.0248, 0.0248,
        0.0247, 0.0247, 0.0246, 0.0246, 0.0245, 0.0245, 0.0245, 0.0244, 0.0244,
        0.0244, 0.0243, 0.0243, 0.0243, 0.0242, 0.0242, 0.0242, 0.0241, 0.0241,
        0.0241, 0.0240, 0.0240, 0.0240, 0.0240, 0.0239, 0.0239, 0.0239, 0.0239,
        0.0238, 0.0238, 0.0238, 0.0237], device='cuda:0')
Selected points (indices): {56, 239}
Selected new x values (normalized): tensor([0.0561, 0.2392], device='cuda:0')
Corresponding new x values (unnormalized): tensor([-1775.7758, -1043.0430], device='cuda:0')
No root_file_path provided; skipping ROOT file data plotting.
Plot saved to /raven/u/dvoss/al_pmssmwithgp/model/plots/Iter1/gp_plot.png
YAML file has been created: /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/data/new_config.yaml
Note: detected 144 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
Note: NumExpr detected 144 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
NumExpr defaulting to 8 threads.

  ___              ____ __  __          _       _   ___            
 | _ \ _  _  _ _  |__ /|  \/  | ___  __| | ___ | | / __| ___  _ _  
 |   /| || || ' \  |_ \| |\/| |/ _ \/ _` |/ -_)| || (_ |/ -_)| ' \ 
 |_|_\ \_,_||_||_||___/|_|  |_|\___/\__,_|\___||_| \___|\___||_||_|
                                                                   

2024-08-28 19:50:12 [info     ] Initialised ModelGenerator with: [Run3ModelGen.modelgen]
2024-08-28 19:50:12 [info     ] 	config_file = /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/data/new_config.yaml [Run3ModelGen.modelgen]
2024-08-28 19:50:12 [info     ] 	scan_dir = /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/scans/scan_1 [Run3ModelGen.modelgen]
2024-08-28 19:50:12 [info     ] 	rawfilen = /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/data/raw.slha [Run3ModelGen.modelgen]
2024-08-28 19:50:12 [info     ] 	seed = 123                    [Run3ModelGen.modelgen]
2024-08-28 19:50:12 [info     ] 	points = {}                   [Run3ModelGen.modelgen]
2024-08-28 19:50:12 [info     ] 	prior = fixed                 [Run3ModelGen.modelgen]
2024-08-28 19:50:12 [info     ] 	num_models = 2                [Run3ModelGen.modelgen]
2024-08-28 19:50:12 [info     ] 	isGMSB = False                [Run3ModelGen.modelgen]
2024-08-28 19:50:12 [info     ] 	parameters =                  [Run3ModelGen.modelgen]
2024-08-28 19:50:12 [info     ] 		tanb: [60, 60]               [Run3ModelGen.modelgen]
2024-08-28 19:50:12 [info     ] 		M_1: [-1775.78, -1043.04]    [Run3ModelGen.modelgen]
2024-08-28 19:50:12 [info     ] 		M_2: [2000, 2000]            [Run3ModelGen.modelgen]
2024-08-28 19:50:12 [info     ] 		M_3: [4000, 4000]            [Run3ModelGen.modelgen]
2024-08-28 19:50:12 [info     ] 		AT: [4000, 4000]             [Run3ModelGen.modelgen]
2024-08-28 19:50:12 [info     ] 		Ab: [2000, 2000]             [Run3ModelGen.modelgen]
2024-08-28 19:50:12 [info     ] 		Atau: [2000, 2000]           [Run3ModelGen.modelgen]
2024-08-28 19:50:12 [info     ] 		mu: [2000, 2000]             [Run3ModelGen.modelgen]
2024-08-28 19:50:12 [info     ] 		mA: [2000, 2000]             [Run3ModelGen.modelgen]
2024-08-28 19:50:12 [info     ] 		meL: [2000, 2000]            [Run3ModelGen.modelgen]
2024-08-28 19:50:12 [info     ] 		mtauL: [2000, 2000]          [Run3ModelGen.modelgen]
2024-08-28 19:50:12 [info     ] 		meR: [2000, 2000]            [Run3ModelGen.modelgen]
2024-08-28 19:50:12 [info     ] 		mtauR: [2000, 2000]          [Run3ModelGen.modelgen]
2024-08-28 19:50:12 [info     ] 		mqL1: [4000, 4000]           [Run3ModelGen.modelgen]
2024-08-28 19:50:12 [info     ] 		mqL3: [4000, 4000]           [Run3ModelGen.modelgen]
2024-08-28 19:50:12 [info     ] 		muR: [4000, 4000]            [Run3ModelGen.modelgen]
2024-08-28 19:50:12 [info     ] 		mtR: [4000, 4000]            [Run3ModelGen.modelgen]
2024-08-28 19:50:12 [info     ] 		mdR: [4000, 4000]            [Run3ModelGen.modelgen]
2024-08-28 19:50:12 [info     ] 		mbR: [4000, 4000]            [Run3ModelGen.modelgen]
2024-08-28 19:50:12 [info     ] 	steps =                       [Run3ModelGen.modelgen]
2024-08-28 19:50:12 [info     ] 		{'name': 'prep_input', 'output_dir': 'input', 'prefix': 'IN'} [Run3ModelGen.modelgen]
2024-08-28 19:50:12 [info     ] 		{'name': 'SPheno', 'input_dir': 'input', 'output_dir': 'SPheno', 'log_dir': 'SPheno_log', 'prefix': 'SP'} [Run3ModelGen.modelgen]
2024-08-28 19:50:12 [info     ] 		{'name': 'softsusy', 'input_dir': 'input', 'output_dir': 'softsusy', 'prefix': 'SS'} [Run3ModelGen.modelgen]
2024-08-28 19:50:12 [info     ] 		{'name': 'micromegas', 'input_dir': 'SPheno', 'output_dir': 'micromegas', 'prefix': 'MO'} [Run3ModelGen.modelgen]
2024-08-28 19:50:12 [info     ] 		{'name': 'superiso', 'input_dir': 'SPheno', 'output_dir': 'superiso', 'prefix': 'SI'} [Run3ModelGen.modelgen]
2024-08-28 19:50:12 [info     ] 		{'name': 'gm2calc', 'input_dir': 'SPheno', 'output_dir': 'gm2calc', 'prefix': 'GM2'} [Run3ModelGen.modelgen]
2024-08-28 19:50:12 [info     ] 		{'name': 'evade', 'input_dir': 'SPheno', 'output_dir': 'evade', 'prefix': 'EV'} [Run3ModelGen.modelgen]
2024-08-28 19:50:12 [info     ] Starting Model Generation...   [Run3ModelGen.modelgen]
2024-08-28 19:50:12 [info     ] Setting up output directories for scan [Run3ModelGen.modelgen]
2024-08-28 19:50:13 [info     ] Generating Model: 0.slha (1/2) [Run3ModelGen.modelgen]
2024-08-28 19:50:13 [info     ] 	running step: prep_input      [Run3ModelGen.modelgen]
2024-08-28 19:50:13 [info     ] 	running step: SPheno          [Run3ModelGen.modelgen]
2024-08-28 19:50:13 [info     ] 	running step: softsusy        [Run3ModelGen.modelgen]
2024-08-28 19:50:13 [info     ] 	running step: micromegas      [Run3ModelGen.modelgen]
2024-08-28 19:50:14 [info     ] 	running step: superiso        [Run3ModelGen.modelgen]
2024-08-28 19:50:15 [info     ] 	running step: gm2calc         [Run3ModelGen.modelgen]
2024-08-28 19:50:15 [info     ] 	running step: evade           [Run3ModelGen.modelgen]
2024-08-28 19:50:15 [info     ] Generating Model: 1.slha (2/2) [Run3ModelGen.modelgen]
2024-08-28 19:50:15 [info     ] 	running step: prep_input      [Run3ModelGen.modelgen]
2024-08-28 19:50:15 [info     ] 	running step: SPheno          [Run3ModelGen.modelgen]
2024-08-28 19:50:15 [info     ] 	running step: softsusy        [Run3ModelGen.modelgen]
2024-08-28 19:50:15 [info     ] 	running step: micromegas      [Run3ModelGen.modelgen]
2024-08-28 19:50:15 [info     ] 	running step: superiso        [Run3ModelGen.modelgen]
2024-08-28 19:50:15 [info     ] 	running step: gm2calc         [Run3ModelGen.modelgen]
2024-08-28 19:50:15 [info     ] 	running step: evade           [Run3ModelGen.modelgen]
2024-08-28 19:50:15 [info     ] Finished generating models! Stats for successful generations (successful/attempted): [Run3ModelGen.modelgen]
2024-08-28 19:50:15 [info     ] 	prep_input: 2/2.              [Run3ModelGen.modelgen]
2024-08-28 19:50:15 [info     ] 	SPheno: 2/2.                  [Run3ModelGen.modelgen]
2024-08-28 19:50:15 [info     ] 	softsusy: 2/2.                [Run3ModelGen.modelgen]
2024-08-28 19:50:15 [info     ] 	micromegas: 2/2.              [Run3ModelGen.modelgen]
2024-08-28 19:50:15 [info     ] 	superiso: 2/2.                [Run3ModelGen.modelgen]
2024-08-28 19:50:15 [info     ] 	gm2calc: 2/2.                 [Run3ModelGen.modelgen]
2024-08-28 19:50:15 [info     ] 	evade: 0/2.                   [Run3ModelGen.modelgen]
2024-08-28 19:50:15 [info     ] Will make NTuple for: /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/scans/scan_1, with 2 models [Run3ModelGen.ntupling]
2024-08-28 19:50:15 [info     ] 	Filling values for model: 0   [Run3ModelGen.ntupling]
2024-08-28 19:50:15 [info     ] Saving NTuple in /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/scans/scan_1/ntuple.0.0.root ... [Run3ModelGen.ntupling]
Completed iteration 1
Starting iteration 2
Initial Training points:  tensor([0.8528, 0.5812, 0.1617, 0.0046], device='cuda:0') torch.Size([4])
Training points after adding:  tensor([0.1617, 0.0046, 0.0561, 0.2392, 0.5812, 0.8528], device='cuda:0') torch.Size([6])
These training_points are used in the GP tensor([0.1617, 0.0046, 0.0561, 0.2392, 0.5812, 0.8528], device='cuda:0')
Point: tensor([0.0000, 0.0010, 0.0020, 0.0030, 0.0040, 0.0050, 0.0060, 0.0070, 0.0080,
        0.0090, 0.0100, 0.0110, 0.0120, 0.0130, 0.0140, 0.0150, 0.0160, 0.0170,
        0.0180, 0.0190, 0.0200, 0.0210, 0.0220, 0.0230, 0.0240, 0.0250, 0.0260,
        0.0270, 0.0280, 0.0290, 0.0300, 0.0310, 0.0320, 0.0330, 0.0340, 0.0350,
        0.0360, 0.0370, 0.0380, 0.0390, 0.0400, 0.0410, 0.0420, 0.0430, 0.0440,
        0.0450, 0.0460, 0.0470, 0.0480, 0.0490, 0.0501, 0.0511, 0.0521, 0.0531,
        0.0541, 0.0551, 0.0561, 0.0571, 0.0581, 0.0591, 0.0601, 0.0611, 0.0621,
        0.0631, 0.0641, 0.0651, 0.0661, 0.0671, 0.0681, 0.0691, 0.0701, 0.0711,
        0.0721, 0.0731, 0.0741, 0.0751, 0.0761, 0.0771, 0.0781, 0.0791, 0.0801,
        0.0811, 0.0821, 0.0831, 0.0841, 0.0851, 0.0861, 0.0871, 0.0881, 0.0891,
        0.0901, 0.0911, 0.0921, 0.0931, 0.0941, 0.0951, 0.0961, 0.0971, 0.0981,
        0.0991, 0.1001, 0.1011, 0.1021, 0.1031, 0.1041, 0.1051, 0.1061, 0.1071,
        0.1081, 0.1091, 0.1101, 0.1111, 0.1121, 0.1131, 0.1141, 0.1151, 0.1161,
        0.1171, 0.1181, 0.1191, 0.1201, 0.1211, 0.1221, 0.1231, 0.1241, 0.1251,
        0.1261, 0.1271, 0.1281, 0.1291, 0.1301, 0.1311, 0.1321, 0.1331, 0.1341,
        0.1351, 0.1361, 0.1371, 0.1381, 0.1391, 0.1401, 0.1411, 0.1421, 0.1431,
        0.1441, 0.1451, 0.1461, 0.1471, 0.1481, 0.1491, 0.1502, 0.1512, 0.1522,
        0.1532, 0.1542, 0.1552, 0.1562, 0.1572, 0.1582, 0.1592, 0.1602, 0.1612,
        0.1622, 0.1632, 0.1642, 0.1652, 0.1662, 0.1672, 0.1682, 0.1692, 0.1702,
        0.1712, 0.1722, 0.1732, 0.1742, 0.1752, 0.1762, 0.1772, 0.1782, 0.1792,
        0.1802, 0.1812, 0.1822, 0.1832, 0.1842, 0.1852, 0.1862, 0.1872, 0.1882,
        0.1892, 0.1902, 0.1912, 0.1922, 0.1932, 0.1942, 0.1952, 0.1962, 0.1972,
        0.1982, 0.1992, 0.2002, 0.2012, 0.2022, 0.2032, 0.2042, 0.2052, 0.2062,
        0.2072, 0.2082, 0.2092, 0.2102, 0.2112, 0.2122, 0.2132, 0.2142, 0.2152,
        0.2162, 0.2172, 0.2182, 0.2192, 0.2202, 0.2212, 0.2222, 0.2232, 0.2242,
        0.2252, 0.2262, 0.2272, 0.2282, 0.2292, 0.2302, 0.2312, 0.2322, 0.2332,
        0.2342, 0.2352, 0.2362, 0.2372, 0.2382, 0.2392, 0.2402, 0.2412, 0.2422,
        0.2432, 0.2442, 0.2452, 0.2462, 0.2472, 0.2482, 0.2492, 0.2503, 0.2513,
        0.2523, 0.2533, 0.2543, 0.2553, 0.2563, 0.2573, 0.2583, 0.2593, 0.2603,
        0.2613, 0.2623, 0.2633, 0.2643, 0.2653, 0.2663, 0.2673, 0.2683, 0.2693,
        0.2703, 0.2713, 0.2723, 0.2733, 0.2743, 0.2753, 0.2763, 0.2773, 0.2783,
        0.2793, 0.2803, 0.2813, 0.2823, 0.2833, 0.2843, 0.2853, 0.2863, 0.2873,
        0.2883, 0.2893, 0.2903, 0.2913, 0.2923, 0.2933, 0.2943, 0.2953, 0.2963,
        0.2973, 0.2983, 0.2993, 0.3003, 0.3013, 0.3023, 0.3033, 0.3043, 0.3053,
        0.3063, 0.3073, 0.3083, 0.3093, 0.3103, 0.3113, 0.3123, 0.3133, 0.3143,
        0.3153, 0.3163, 0.3173, 0.3183, 0.3193, 0.3203, 0.3213, 0.3223, 0.3233,
        0.3243, 0.3253, 0.3263, 0.3273, 0.3283, 0.3293, 0.3303, 0.3313, 0.3323,
        0.3333, 0.3343, 0.3353, 0.3363, 0.3373, 0.3383, 0.3393, 0.3403, 0.3413,
        0.3423, 0.3433, 0.3443, 0.3453, 0.3463, 0.3473, 0.3483, 0.3493, 0.3504,
        0.3514, 0.3524, 0.3534, 0.3544, 0.3554, 0.3564, 0.3574, 0.3584, 0.3594,
        0.3604, 0.3614, 0.3624, 0.3634, 0.3644, 0.3654, 0.3664, 0.3674, 0.3684,
        0.3694, 0.3704, 0.3714, 0.3724, 0.3734, 0.3744, 0.3754, 0.3764, 0.3774,
        0.3784, 0.3794, 0.3804, 0.3814, 0.3824, 0.3834, 0.3844, 0.3854, 0.3864,
        0.3874, 0.3884, 0.3894, 0.3904, 0.3914, 0.3924, 0.3934, 0.3944, 0.3954,
        0.3964, 0.3974, 0.3984, 0.3994, 0.4004, 0.4014, 0.4024, 0.4034, 0.4044,
        0.4054, 0.4064, 0.4074, 0.4084, 0.4094, 0.4104, 0.4114, 0.4124, 0.4134,
        0.4144, 0.4154, 0.4164, 0.4174, 0.4184, 0.4194, 0.4204, 0.4214, 0.4224,
        0.4234, 0.4244, 0.4254, 0.4264, 0.4274, 0.4284, 0.4294, 0.4304, 0.4314,
        0.4324, 0.4334, 0.4344, 0.4354, 0.4364, 0.4374, 0.4384, 0.4394, 0.4404,
        0.4414, 0.4424, 0.4434, 0.4444, 0.4454, 0.4464, 0.4474, 0.4484, 0.4494,
        0.4505, 0.4515, 0.4525, 0.4535, 0.4545, 0.4555, 0.4565, 0.4575, 0.4585,
        0.4595, 0.4605, 0.4615, 0.4625, 0.4635, 0.4645, 0.4655, 0.4665, 0.4675,
        0.4685, 0.4695, 0.4705, 0.4715, 0.4725, 0.4735, 0.4745, 0.4755, 0.4765,
        0.4775, 0.4785, 0.4795, 0.4805, 0.4815, 0.4825, 0.4835, 0.4845, 0.4855,
        0.4865, 0.4875, 0.4885, 0.4895, 0.4905, 0.4915, 0.4925, 0.4935, 0.4945,
        0.4955, 0.4965, 0.4975, 0.4985, 0.4995, 0.5005, 0.5015, 0.5025, 0.5035,
        0.5045, 0.5055, 0.5065, 0.5075, 0.5085, 0.5095, 0.5105, 0.5115, 0.5125,
        0.5135, 0.5145, 0.5155, 0.5165, 0.5175, 0.5185, 0.5195, 0.5205, 0.5215,
        0.5225, 0.5235, 0.5245, 0.5255, 0.5265, 0.5275, 0.5285, 0.5295, 0.5305,
        0.5315, 0.5325, 0.5335, 0.5345, 0.5355, 0.5365, 0.5375, 0.5385, 0.5395,
        0.5405, 0.5415, 0.5425, 0.5435, 0.5445, 0.5455, 0.5465, 0.5475, 0.5485,
        0.5495, 0.5506, 0.5516, 0.5526, 0.5536, 0.5546, 0.5556, 0.5566, 0.5576,
        0.5586, 0.5596, 0.5606, 0.5616, 0.5626, 0.5636, 0.5646, 0.5656, 0.5666,
        0.5676, 0.5686, 0.5696, 0.5706, 0.5716, 0.5726, 0.5736, 0.5746, 0.5756,
        0.5766, 0.5776, 0.5786, 0.5796, 0.5806, 0.5816, 0.5826, 0.5836, 0.5846,
        0.5856, 0.5866, 0.5876, 0.5886, 0.5896, 0.5906, 0.5916, 0.5926, 0.5936,
        0.5946, 0.5956, 0.5966, 0.5976, 0.5986, 0.5996, 0.6006, 0.6016, 0.6026,
        0.6036, 0.6046, 0.6056, 0.6066, 0.6076, 0.6086, 0.6096, 0.6106, 0.6116,
        0.6126, 0.6136, 0.6146, 0.6156, 0.6166, 0.6176, 0.6186, 0.6196, 0.6206,
        0.6216, 0.6226, 0.6236, 0.6246, 0.6256, 0.6266, 0.6276, 0.6286, 0.6296,
        0.6306, 0.6316, 0.6326, 0.6336, 0.6346, 0.6356, 0.6366, 0.6376, 0.6386,
        0.6396, 0.6406, 0.6416, 0.6426, 0.6436, 0.6446, 0.6456, 0.6466, 0.6476,
        0.6486, 0.6496, 0.6507, 0.6517, 0.6527, 0.6537, 0.6547, 0.6557, 0.6567,
        0.6577, 0.6587, 0.6597, 0.6607, 0.6617, 0.6627, 0.6637, 0.6647, 0.6657,
        0.6667, 0.6677, 0.6687, 0.6697, 0.6707, 0.6717, 0.6727, 0.6737, 0.6747,
        0.6757, 0.6767, 0.6777, 0.6787, 0.6797, 0.6807, 0.6817, 0.6827, 0.6837,
        0.6847, 0.6857, 0.6867, 0.6877, 0.6887, 0.6897, 0.6907, 0.6917, 0.6927,
        0.6937, 0.6947, 0.6957, 0.6967, 0.6977, 0.6987, 0.6997, 0.7007, 0.7017,
        0.7027, 0.7037, 0.7047, 0.7057, 0.7067, 0.7077, 0.7087, 0.7097, 0.7107,
        0.7117, 0.7127, 0.7137, 0.7147, 0.7157, 0.7167, 0.7177, 0.7187, 0.7197,
        0.7207, 0.7217, 0.7227, 0.7237, 0.7247, 0.7257, 0.7267, 0.7277, 0.7287,
        0.7297, 0.7307, 0.7317, 0.7327, 0.7337, 0.7347, 0.7357, 0.7367, 0.7377,
        0.7387, 0.7397, 0.7407, 0.7417, 0.7427, 0.7437, 0.7447, 0.7457, 0.7467,
        0.7477, 0.7487, 0.7497, 0.7508, 0.7518, 0.7528, 0.7538, 0.7548, 0.7558,
        0.7568, 0.7578, 0.7588, 0.7598, 0.7608, 0.7618, 0.7628, 0.7638, 0.7648,
        0.7658, 0.7668, 0.7678, 0.7688, 0.7698, 0.7708, 0.7718, 0.7728, 0.7738,
        0.7748, 0.7758, 0.7768, 0.7778, 0.7788, 0.7798, 0.7808, 0.7818, 0.7828,
        0.7838, 0.7848, 0.7858, 0.7868, 0.7878, 0.7888, 0.7898, 0.7908, 0.7918,
        0.7928, 0.7938, 0.7948, 0.7958, 0.7968, 0.7978, 0.7988, 0.7998, 0.8008,
        0.8018, 0.8028, 0.8038, 0.8048, 0.8058, 0.8068, 0.8078, 0.8088, 0.8098,
        0.8108, 0.8118, 0.8128, 0.8138, 0.8148, 0.8158, 0.8168, 0.8178, 0.8188,
        0.8198, 0.8208, 0.8218, 0.8228, 0.8238, 0.8248, 0.8258, 0.8268, 0.8278,
        0.8288, 0.8298, 0.8308, 0.8318, 0.8328, 0.8338, 0.8348, 0.8358, 0.8368,
        0.8378, 0.8388, 0.8398, 0.8408, 0.8418, 0.8428, 0.8438, 0.8448, 0.8458,
        0.8468, 0.8478, 0.8488, 0.8498, 0.8509, 0.8519, 0.8529, 0.8539, 0.8549,
        0.8559, 0.8569, 0.8579, 0.8589, 0.8599, 0.8609, 0.8619, 0.8629, 0.8639,
        0.8649, 0.8659, 0.8669, 0.8679, 0.8689, 0.8699, 0.8709, 0.8719, 0.8729,
        0.8739, 0.8749, 0.8759, 0.8769, 0.8779, 0.8789, 0.8799, 0.8809, 0.8819,
        0.8829, 0.8839, 0.8849, 0.8859, 0.8869, 0.8879, 0.8889, 0.8899, 0.8909,
        0.8919, 0.8929, 0.8939, 0.8949, 0.8959, 0.8969, 0.8979, 0.8989, 0.8999,
        0.9009, 0.9019, 0.9029, 0.9039, 0.9049, 0.9059, 0.9069, 0.9079, 0.9089,
        0.9099, 0.9109, 0.9119, 0.9129, 0.9139, 0.9149, 0.9159, 0.9169, 0.9179,
        0.9189, 0.9199, 0.9209, 0.9219, 0.9229, 0.9239, 0.9249, 0.9259, 0.9269,
        0.9279, 0.9289, 0.9299, 0.9309, 0.9319, 0.9329, 0.9339, 0.9349, 0.9359,
        0.9369, 0.9379, 0.9389, 0.9399, 0.9409, 0.9419, 0.9429, 0.9439, 0.9449,
        0.9459, 0.9469, 0.9479, 0.9489, 0.9499, 0.9510, 0.9520, 0.9530, 0.9540,
        0.9550, 0.9560, 0.9570, 0.9580, 0.9590, 0.9600, 0.9610, 0.9620, 0.9630,
        0.9640, 0.9650, 0.9660, 0.9670, 0.9680, 0.9690, 0.9700, 0.9710, 0.9720,
        0.9730, 0.9740, 0.9750, 0.9760, 0.9770, 0.9780, 0.9790, 0.9800, 0.9810,
        0.9820, 0.9830, 0.9840, 0.9850, 0.9860, 0.9870, 0.9880, 0.9890, 0.9900,
        0.9910, 0.9920, 0.9930, 0.9940, 0.9950, 0.9960, 0.9970, 0.9980, 0.9990,
        1.0000], device='cuda:0') - Variance: tensor([[0.4262],
        [0.3545],
        [0.0113],
        [0.3931],
        [0.3172],
        [0.2320],
        [0.1361],
        [0.2776],
        [0.1872],
        [0.0855],
        [0.5381],
        [0.4797],
        [0.0300],
        [0.0049],
        [0.4462],
        [0.3749],
        [0.2938],
        [0.6413],
        [0.5930],
        [0.5370],
        [0.4720],
        [0.7282],
        [0.6864],
        [0.6361],
        [0.6800],
        [0.6213],
        [0.5470],
        [0.4515],
        [0.6890],
        [0.7657],
        [0.7011],
        [0.8193],
        [0.7605],
        [0.6786],
        [0.5638],
        [0.7138],
        [0.6017],
        [0.4422],
        [0.6203],
        [0.4610],
        [0.5369],
        [0.3383],
        [0.5394],
        [0.3386],
        [0.0499],
        [0.3336],
        [0.4203],
        [0.1646],
        [0.4126],
        [0.1528],
        [0.2615],
        [0.0105],
        [0.2497],
        [0.0111],
        [0.0559],
        [0.3348],
        [0.0400],
        [0.1626],
        [0.0113],
        [0.1484],
        [0.0446],
        [0.0111],
        [0.4129],
        [0.1524],
        [0.0491],
        [0.0109],
        [0.4157],
        [0.3443],
        [0.2642],
        [0.5978],
        [0.4185],
        [0.3474],
        [0.2677],
        [0.5997],
        [0.5506],
        [0.4955],
        [0.7250],
        [0.7606],
        [0.7310],
        [0.6977],
        [0.8363],
        [0.8158],
        [0.7928],
        [0.8886],
        [0.9034],
        [0.8910],
        [0.8772],
        [0.9350],
        [0.9440],
        [0.9365],
        [0.9676],
        [0.9724],
        [0.9684],
        [0.9639],
        [0.9878],
        [0.9856],
        [0.9882],
        [0.9960],
        [0.9972],
        [0.9962],
        [0.9974],
        [1.0011],
        [1.0016],
        [1.0021],
        [1.0037],
        [1.0038],
        [1.0040],
        [1.0036],
        [1.0046],
        [1.0045],
        [1.0043],
        [1.0045],
        [1.0041],
        [1.0035],
        [1.0040],
        [1.0033],
        [1.0022],
        [1.0007],
        [1.0015],
        [0.9979],
        [0.9944],
        [0.9962],
        [0.9919],
        [0.9857],
        [0.9831],
        [0.9867],
        [0.9781],
        [0.9656],
        [0.9719],
        [0.9566],
        [0.9344],
        [0.9252],
        [0.9377],
        [0.9071],
        [0.8626],
        [0.9116],
        [0.8692],
        [0.8076],
        [0.7817],
        [0.8167],
        [0.7314],
        [0.6957],
        [0.7441],
        [0.7100],
        [0.5765],
        [0.5206],
        [0.5964],
        [0.5430],
        [0.3342],
        [0.5644],
        [0.5069],
        [0.2817],
        [0.1873],
        [0.4679],
        [0.2252],
        [0.1234],
        [0.4260],
        [0.1642],
        [0.0545],
        [0.0113],
        [0.2992],
        [0.2071],
        [0.1030],
        [0.2440],
        [0.1447],
        [0.0325],
        [0.0113],
        [0.2828],
        [0.1885],
        [0.0820],
        [0.3987],
        [0.3196],
        [0.2301],
        [0.1290],
        [0.5570],
        [0.4985],
        [0.4324],
        [0.6290],
        [0.5798],
        [0.5243],
        [0.7593],
        [0.7271],
        [0.6908],
        [0.7284],
        [0.8233],
        [0.7994],
        [0.8239],
        [0.8860],
        [0.8701],
        [0.8859],
        [0.9140],
        [0.9013],
        [0.9252],
        [0.9506],
        [0.9493],
        [0.9610],
        [0.9587],
        [0.9668],
        [0.9629],
        [0.9722],
        [0.9747],
        [0.9677],
        [0.9718],
        [0.9612],
        [0.9633],
        [0.9474],
        [0.9477],
        [0.9395],
        [0.9225],
        [0.9205],
        [0.8976],
        [0.8795],
        [0.8449],
        [0.8394],
        [0.8336],
        [0.7859],
        [0.7780],
        [0.7148],
        [0.7042],
        [0.6204],
        [0.6063],
        [0.6402],
        [0.5386],
        [0.5214],
        [0.4595],
        [0.4393],
        [0.4184],
        [0.3433],
        [0.3188],
        [0.2310],
        [0.2968],
        [0.0996],
        [0.1766],
        [0.2470],
        [0.0360],
        [0.1184],
        [0.0113],
        [0.0899],
        [0.0113],
        [0.0604],
        [0.1407],
        [0.0301],
        [0.1129],
        [0.0113],
        [0.0842],
        [0.0885],
        [0.1664],
        [0.2377],
        [0.2412],
        [0.3061],
        [0.2167],
        [0.3685],
        [0.2870],
        [0.4253],
        [0.5370],
        [0.4771],
        [0.5788],
        [0.5808],
        [0.6168],
        [0.6917],
        [0.6931],
        [0.7533],
        [0.7544],
        [0.8028],
        [0.8037],
        [0.8426],
        [0.8740],
        [0.8746],
        [0.9123],
        [0.9127],
        [0.9306],
        [0.9309],
        [0.9524],
        [0.9627],
        [0.9679],
        [0.9752],
        [0.9789],
        [0.9840],
        [0.9867],
        [0.9921],
        [0.9954],
        [0.9961],
        [0.9988],
        [0.9993],
        [1.0011],
        [1.0017],
        [1.0028],
        [1.0036],
        [1.0039],
        [1.0044],
        [1.0046],
        [1.0049],
        [1.0051],
        [1.0052],
        [1.0054],
        [1.0054],
        [1.0055],
        [1.0055],
        [1.0056],
        [1.0056],
        [1.0056],
        [1.0056],
        [1.0056],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0056],
        [1.0056],
        [1.0056],
        [1.0056],
        [1.0056],
        [1.0055],
        [1.0055],
        [1.0054],
        [1.0053],
        [1.0052],
        [1.0050],
        [1.0048],
        [1.0045],
        [1.0042],
        [1.0037],
        [1.0032],
        [1.0025],
        [1.0016],
        [1.0005],
        [0.9991],
        [0.9975],
        [0.9954],
        [0.9930],
        [0.9899],
        [0.9863],
        [0.9822],
        [0.9767],
        [0.9708],
        [0.9639],
        [0.9558],
        [0.9456],
        [0.9351],
        [0.9227],
        [0.9074],
        [0.8915],
        [0.8741],
        [0.8521],
        [0.8307],
        [0.8055],
        [0.7803],
        [0.7478],
        [0.7176],
        [0.6863],
        [0.6459],
        [0.6099],
        [0.5704],
        [0.5229],
        [0.4829],
        [0.4440],
        [0.4022],
        [0.3518],
        [0.3141],
        [0.2742],
        [0.2254],
        [0.1931],
        [0.1595],
        [0.1171],
        [0.0947],
        [0.0717],
        [0.0401],
        [0.0468],
        [0.0137],
        [0.0123],
        [0.0113],
        [0.0249],
        [0.0387],
        [0.0523],
        [0.0803],
        [0.0778],
        [0.1050],
        [0.1450],
        [0.1833],
        [0.2198],
        [0.2663],
        [0.2871],
        [0.3402],
        [0.3796],
        [0.4348],
        [0.4769],
        [0.5236],
        [0.5661],
        [0.5916],
        [0.6340],
        [0.6720],
        [0.7108],
        [0.7409],
        [0.7753],
        [0.7954],
        [0.8227],
        [0.8465],
        [0.8672],
        [0.8870],
        [0.9040],
        [0.9200],
        [0.9310],
        [0.9427],
        [0.9526],
        [0.9623],
        [0.9691],
        [0.9758],
        [0.9812],
        [0.9850],
        [0.9888],
        [0.9923],
        [0.9947],
        [0.9970],
        [0.9988],
        [1.0001],
        [1.0012],
        [1.0023],
        [1.0031],
        [1.0036],
        [1.0041],
        [1.0045],
        [1.0047],
        [1.0050],
        [1.0051],
        [1.0053],
        [1.0054],
        [1.0055],
        [1.0055],
        [1.0056],
        [1.0056],
        [1.0056],
        [1.0056],
        [1.0056],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0056],
        [1.0056],
        [1.0056],
        [1.0056],
        [1.0056],
        [1.0055],
        [1.0055],
        [1.0054],
        [1.0054],
        [1.0052],
        [1.0052],
        [1.0050],
        [1.0045],
        [1.0044],
        [1.0039],
        [1.0038],
        [1.0030],
        [1.0028],
        [1.0010],
        [0.9990],
        [0.9993],
        [0.9966],
        [0.9946],
        [0.9952],
        [0.9882],
        [0.9843],
        [0.9854],
        [0.9718],
        [0.9644],
        [0.9665],
        [0.9685],
        [0.9259],
        [0.9299],
        [0.9338],
        [0.8855],
        [0.8916],
        [0.8974],
        [0.7734],
        [0.7852],
        [0.7964],
        [0.8071],
        [0.6737],
        [0.6906],
        [0.7067],
        [0.5059],
        [0.5314],
        [0.5555],
        [0.5784],
        [0.4495],
        [0.4778],
        [0.5047],
        [0.1683],
        [0.2110],
        [0.2514],
        [0.4482],
        [0.0738],
        [0.1213],
        [0.3520],
        [0.0113],
        [0.0113],
        [0.2392],
        [0.2782],
        [0.0113],
        [0.1069],
        [0.1527],
        [0.0113],
        [0.0113],
        [0.2267],
        [0.2664],
        [0.0433],
        [0.0923],
        [0.3306],
        [0.1269],
        [0.1716],
        [0.3892],
        [0.5500],
        [0.2440],
        [0.4427],
        [0.5895],
        [0.4640],
        [0.4916],
        [0.6257],
        [0.7248],
        [0.6401],
        [0.7354],
        [0.8059],
        [0.7456],
        [0.7589],
        [0.8233],
        [0.8708],
        [0.8302],
        [0.8759],
        [0.9310],
        [0.9085],
        [0.9338],
        [0.9526],
        [0.9664],
        [0.9546],
        [0.9679],
        [0.9778],
        [0.9774],
        [0.9848],
        [0.9902],
        [0.9855],
        [0.9941],
        [0.9971],
        [0.9993],
        [0.9993],
        [1.0009],
        [1.0022],
        [1.0021],
        [1.0030],
        [1.0042],
        [1.0046],
        [1.0045],
        [1.0048],
        [1.0052],
        [1.0050],
        [1.0053],
        [1.0055],
        [1.0055],
        [1.0055],
        [1.0056],
        [1.0056],
        [1.0056],
        [1.0056],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057],
        [1.0057]], device='cuda:0') 
Smoothed_batch_entropy:  tensor([0.0758, 0.0670, 0.0029, 0.0713, 0.0613, 0.0481, 0.0305, 0.0547, 0.0397,
        0.0197, 0.0832, 0.0776, 0.0070, 0.0011, 0.0726, 0.0641, 0.0530, 0.0861,
        0.0821, 0.0768, 0.0698, 0.0866, 0.0832, 0.0786, 0.0793, 0.0735, 0.0657,
        0.0551, 0.0715, 0.0744, 0.0682, 0.0735, 0.0679, 0.0601, 0.0495, 0.0592,
        0.0493, 0.0357, 0.0483, 0.0353, 0.0403, 0.0249, 0.0392, 0.0241, 0.0035,
        0.0231, 0.0290, 0.0111, 0.0279, 0.0101, 0.0172, 0.0007, 0.0163, 0.0007,
        0.0035, 0.0216, 0.0025, 0.0103, 0.0007, 0.0093, 0.0028, 0.0007, 0.0264,
        0.0095, 0.0030, 0.0007, 0.0263, 0.0216, 0.0164, 0.0386, 0.0263, 0.0216,
        0.0165, 0.0384, 0.0350, 0.0312, 0.0472, 0.0498, 0.0475, 0.0450, 0.0555,
        0.0538, 0.0519, 0.0597, 0.0609, 0.0597, 0.0584, 0.0636, 0.0644, 0.0635,
        0.0667, 0.0671, 0.0665, 0.0659, 0.0687, 0.0683, 0.0685, 0.0696, 0.0697,
        0.0694, 0.0695, 0.0701, 0.0701, 0.0700, 0.0703, 0.0702, 0.0701, 0.0699,
        0.0701, 0.0700, 0.0698, 0.0698, 0.0696, 0.0693, 0.0694, 0.0691, 0.0688,
        0.0684, 0.0685, 0.0677, 0.0671, 0.0673, 0.0666, 0.0657, 0.0653, 0.0657,
        0.0646, 0.0631, 0.0637, 0.0621, 0.0599, 0.0590, 0.0601, 0.0573, 0.0537,
        0.0576, 0.0541, 0.0494, 0.0475, 0.0500, 0.0439, 0.0415, 0.0447, 0.0423,
        0.0336, 0.0301, 0.0348, 0.0314, 0.0189, 0.0327, 0.0291, 0.0158, 0.0104,
        0.0266, 0.0125, 0.0068, 0.0240, 0.0090, 0.0030, 0.0006, 0.0166, 0.0114,
        0.0056, 0.0134, 0.0079, 0.0017, 0.0006, 0.0155, 0.0102, 0.0044, 0.0220,
        0.0175, 0.0125, 0.0069, 0.0311, 0.0277, 0.0238, 0.0354, 0.0324, 0.0291,
        0.0435, 0.0414, 0.0391, 0.0414, 0.0477, 0.0461, 0.0477, 0.0521, 0.0510,
        0.0522, 0.0544, 0.0536, 0.0555, 0.0576, 0.0577, 0.0588, 0.0590, 0.0600,
        0.0601, 0.0611, 0.0617, 0.0619, 0.0625, 0.0626, 0.0630, 0.0630, 0.0633,
        0.0634, 0.0632, 0.0633, 0.0628, 0.0624, 0.0613, 0.0612, 0.0610, 0.0591,
        0.0588, 0.0558, 0.0553, 0.0507, 0.0498, 0.0518, 0.0456, 0.0444, 0.0402,
        0.0387, 0.0371, 0.0314, 0.0294, 0.0221, 0.0276, 0.0100, 0.0171, 0.0233,
        0.0037, 0.0117, 0.0012, 0.0090, 0.0012, 0.0060, 0.0137, 0.0030, 0.0110,
        0.0012, 0.0083, 0.0087, 0.0158, 0.0219, 0.0222, 0.0274, 0.0201, 0.0320,
        0.0258, 0.0360, 0.0432, 0.0394, 0.0456, 0.0456, 0.0476, 0.0514, 0.0514,
        0.0541, 0.0540, 0.0559, 0.0558, 0.0571, 0.0579, 0.0579, 0.0587, 0.0586,
        0.0588, 0.0587, 0.0588, 0.0587, 0.0586, 0.0585, 0.0583, 0.0581, 0.0580,
        0.0577, 0.0574, 0.0573, 0.0570, 0.0569, 0.0566, 0.0565, 0.0562, 0.0560,
        0.0559, 0.0557, 0.0556, 0.0554, 0.0552, 0.0551, 0.0550, 0.0548, 0.0547,
        0.0546, 0.0545, 0.0544, 0.0543, 0.0542, 0.0541, 0.0540, 0.0539, 0.0539,
        0.0538, 0.0537, 0.0536, 0.0535, 0.0535, 0.0534, 0.0533, 0.0532, 0.0532,
        0.0531, 0.0530, 0.0529, 0.0529, 0.0528, 0.0527, 0.0527, 0.0526, 0.0525,
        0.0525, 0.0524, 0.0523, 0.0522, 0.0522, 0.0521, 0.0520, 0.0520, 0.0519,
        0.0518, 0.0518, 0.0517, 0.0516, 0.0516, 0.0515, 0.0514, 0.0514, 0.0513,
        0.0512, 0.0512, 0.0511, 0.0510, 0.0510, 0.0509, 0.0508, 0.0508, 0.0507,
        0.0506, 0.0506, 0.0505, 0.0504, 0.0504, 0.0503, 0.0503, 0.0502, 0.0501,
        0.0501, 0.0500, 0.0499, 0.0499, 0.0498, 0.0497, 0.0497, 0.0496, 0.0495,
        0.0495, 0.0494, 0.0494, 0.0493, 0.0492, 0.0492, 0.0491, 0.0490, 0.0490,
        0.0489, 0.0489, 0.0488, 0.0487, 0.0487, 0.0486, 0.0485, 0.0485, 0.0484,
        0.0484, 0.0483, 0.0482, 0.0482, 0.0481, 0.0481, 0.0480, 0.0479, 0.0479,
        0.0478, 0.0478, 0.0477, 0.0476, 0.0476, 0.0475, 0.0475, 0.0474, 0.0473,
        0.0473, 0.0472, 0.0472, 0.0471, 0.0470, 0.0470, 0.0469, 0.0469, 0.0468,
        0.0468, 0.0467, 0.0466, 0.0466, 0.0465, 0.0465, 0.0464, 0.0464, 0.0463,
        0.0462, 0.0462, 0.0461, 0.0461, 0.0460, 0.0460, 0.0459, 0.0458, 0.0458,
        0.0457, 0.0457, 0.0456, 0.0456, 0.0455, 0.0454, 0.0454, 0.0453, 0.0453,
        0.0452, 0.0452, 0.0451, 0.0451, 0.0450, 0.0449, 0.0449, 0.0448, 0.0448,
        0.0447, 0.0447, 0.0446, 0.0446, 0.0445, 0.0445, 0.0444, 0.0443, 0.0443,
        0.0442, 0.0442, 0.0441, 0.0441, 0.0440, 0.0440, 0.0439, 0.0439, 0.0438,
        0.0438, 0.0437, 0.0437, 0.0436, 0.0436, 0.0435, 0.0434, 0.0434, 0.0433,
        0.0433, 0.0432, 0.0432, 0.0431, 0.0431, 0.0430, 0.0430, 0.0429, 0.0429,
        0.0428, 0.0428, 0.0427, 0.0427, 0.0426, 0.0426, 0.0425, 0.0425, 0.0424,
        0.0424, 0.0423, 0.0423, 0.0422, 0.0422, 0.0412, 0.0411, 0.0411, 0.0410,
        0.0409, 0.0408, 0.0407, 0.0405, 0.0404, 0.0403, 0.0401, 0.0400, 0.0398,
        0.0396, 0.0393, 0.0391, 0.0388, 0.0385, 0.0382, 0.0379, 0.0375, 0.0371,
        0.0366, 0.0362, 0.0356, 0.0351, 0.0345, 0.0338, 0.0331, 0.0324, 0.0316,
        0.0308, 0.0299, 0.0289, 0.0280, 0.0270, 0.0259, 0.0249, 0.0238, 0.0226,
        0.0215, 0.0203, 0.0191, 0.0178, 0.0167, 0.0155, 0.0142, 0.0131, 0.0119,
        0.0106, 0.0096, 0.0086, 0.0076, 0.0065, 0.0057, 0.0049, 0.0039, 0.0033,
        0.0027, 0.0020, 0.0016, 0.0012, 0.0006, 0.0008, 0.0002, 0.0002, 0.0002,
        0.0004, 0.0006, 0.0008, 0.0013, 0.0013, 0.0017, 0.0024, 0.0031, 0.0038,
        0.0047, 0.0051, 0.0061, 0.0070, 0.0082, 0.0092, 0.0103, 0.0115, 0.0121,
        0.0134, 0.0145, 0.0158, 0.0168, 0.0181, 0.0189, 0.0201, 0.0212, 0.0222,
        0.0233, 0.0243, 0.0253, 0.0260, 0.0269, 0.0277, 0.0285, 0.0292, 0.0299,
        0.0305, 0.0310, 0.0315, 0.0321, 0.0325, 0.0329, 0.0333, 0.0335, 0.0338,
        0.0341, 0.0344, 0.0346, 0.0347, 0.0349, 0.0350, 0.0351, 0.0352, 0.0353,
        0.0354, 0.0355, 0.0355, 0.0355, 0.0355, 0.0356, 0.0356, 0.0356, 0.0356,
        0.0356, 0.0355, 0.0355, 0.0355, 0.0355, 0.0355, 0.0354, 0.0354, 0.0354,
        0.0353, 0.0353, 0.0353, 0.0352, 0.0352, 0.0352, 0.0351, 0.0351, 0.0351,
        0.0350, 0.0350, 0.0349, 0.0349, 0.0349, 0.0348, 0.0348, 0.0348, 0.0347,
        0.0347, 0.0347, 0.0346, 0.0346, 0.0345, 0.0345, 0.0345, 0.0344, 0.0344,
        0.0344, 0.0343, 0.0343, 0.0342, 0.0342, 0.0342, 0.0341, 0.0341, 0.0341,
        0.0340, 0.0340, 0.0340, 0.0339, 0.0339, 0.0338, 0.0338, 0.0338, 0.0337,
        0.0337, 0.0337, 0.0336, 0.0336, 0.0336, 0.0335, 0.0335, 0.0335, 0.0334,
        0.0334, 0.0333, 0.0333, 0.0333, 0.0332, 0.0332, 0.0332, 0.0331, 0.0331,
        0.0331, 0.0330, 0.0330, 0.0330, 0.0329, 0.0329, 0.0329, 0.0328, 0.0328,
        0.0328, 0.0327, 0.0327, 0.0326, 0.0326, 0.0326, 0.0325, 0.0325, 0.0325,
        0.0324, 0.0324, 0.0324, 0.0323, 0.0323, 0.0323, 0.0322, 0.0322, 0.0322,
        0.0321, 0.0321, 0.0321, 0.0320, 0.0320, 0.0320, 0.0319, 0.0319, 0.0319,
        0.0318, 0.0318, 0.0318, 0.0317, 0.0317, 0.0317, 0.0316, 0.0316, 0.0316,
        0.0315, 0.0315, 0.0315, 0.0315, 0.0314, 0.0314, 0.0314, 0.0313, 0.0313,
        0.0313, 0.0312, 0.0312, 0.0312, 0.0311, 0.0311, 0.0311, 0.0310, 0.0310,
        0.0310, 0.0310, 0.0309, 0.0309, 0.0309, 0.0309, 0.0308, 0.0308, 0.0308,
        0.0308, 0.0308, 0.0307, 0.0307, 0.0307, 0.0307, 0.0307, 0.0307, 0.0307,
        0.0308, 0.0307, 0.0308, 0.0309, 0.0308, 0.0309, 0.0309, 0.0310, 0.0310,
        0.0311, 0.0313, 0.0312, 0.0314, 0.0315, 0.0314, 0.0317, 0.0318, 0.0317,
        0.0320, 0.0321, 0.0320, 0.0319, 0.0322, 0.0321, 0.0321, 0.0319, 0.0319,
        0.0319, 0.0303, 0.0304, 0.0306, 0.0307, 0.0280, 0.0283, 0.0287, 0.0230,
        0.0238, 0.0245, 0.0252, 0.0209, 0.0219, 0.0228, 0.0089, 0.0109, 0.0128,
        0.0207, 0.0041, 0.0065, 0.0170, 0.0006, 0.0006, 0.0121, 0.0138, 0.0006,
        0.0057, 0.0080, 0.0006, 0.0006, 0.0114, 0.0132, 0.0024, 0.0049, 0.0158,
        0.0067, 0.0088, 0.0180, 0.0235, 0.0121, 0.0199, 0.0246, 0.0206, 0.0215,
        0.0255, 0.0278, 0.0258, 0.0280, 0.0292, 0.0281, 0.0283, 0.0294, 0.0299,
        0.0294, 0.0299, 0.0301, 0.0301, 0.0301, 0.0300, 0.0299, 0.0299, 0.0298,
        0.0296, 0.0296, 0.0294, 0.0292, 0.0293, 0.0290, 0.0288, 0.0286, 0.0286,
        0.0285, 0.0283, 0.0283, 0.0282, 0.0280, 0.0279, 0.0279, 0.0278, 0.0277,
        0.0277, 0.0276, 0.0275, 0.0274, 0.0274, 0.0273, 0.0273, 0.0273, 0.0272,
        0.0272, 0.0271, 0.0271, 0.0271, 0.0270, 0.0270, 0.0270, 0.0269, 0.0269,
        0.0269, 0.0269, 0.0268, 0.0268, 0.0268, 0.0268, 0.0267, 0.0267, 0.0267,
        0.0266, 0.0266, 0.0266, 0.0266, 0.0265, 0.0265, 0.0265, 0.0265, 0.0264,
        0.0264, 0.0264, 0.0264, 0.0263, 0.0263, 0.0263, 0.0263, 0.0262, 0.0262,
        0.0262, 0.0262, 0.0261, 0.0261, 0.0261, 0.0261, 0.0260, 0.0260, 0.0260,
        0.0260, 0.0259, 0.0259, 0.0259, 0.0259, 0.0259, 0.0258, 0.0258, 0.0258,
        0.0258, 0.0257, 0.0257, 0.0257, 0.0257, 0.0256, 0.0256, 0.0256, 0.0256,
        0.0255, 0.0255, 0.0255, 0.0255, 0.0254, 0.0254, 0.0254, 0.0254, 0.0254,
        0.0253, 0.0253, 0.0253, 0.0253, 0.0252, 0.0252, 0.0252, 0.0252, 0.0251,
        0.0251, 0.0251, 0.0251, 0.0250], device='cuda:0')
Selected points (indices): {213, 21}
Selected new x values (normalized): tensor([0.2132, 0.0210], device='cuda:0')
Corresponding new x values (unnormalized): tensor([-1147.1472, -1915.9159], device='cuda:0')
Plot saved to /raven/u/dvoss/al_pmssmwithgp/model/plots/Iter2/gp_plot.png
YAML file has been created: /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/data/new_config.yaml
Note: detected 144 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
Note: NumExpr detected 144 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
NumExpr defaulting to 8 threads.

  ___              ____ __  __          _       _   ___            
 | _ \ _  _  _ _  |__ /|  \/  | ___  __| | ___ | | / __| ___  _ _  
 |   /| || || ' \  |_ \| |\/| |/ _ \/ _` |/ -_)| || (_ |/ -_)| ' \ 
 |_|_\ \_,_||_||_||___/|_|  |_|\___/\__,_|\___||_| \___|\___||_||_|
                                                                   

2024-08-28 19:51:00 [info     ] Initialised ModelGenerator with: [Run3ModelGen.modelgen]
2024-08-28 19:51:00 [info     ] 	config_file = /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/data/new_config.yaml [Run3ModelGen.modelgen]
2024-08-28 19:51:00 [info     ] 	scan_dir = /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/scans/scan_2 [Run3ModelGen.modelgen]
2024-08-28 19:51:00 [info     ] 	rawfilen = /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/data/raw.slha [Run3ModelGen.modelgen]
2024-08-28 19:51:00 [info     ] 	seed = 123                    [Run3ModelGen.modelgen]
2024-08-28 19:51:00 [info     ] 	points = {}                   [Run3ModelGen.modelgen]
2024-08-28 19:51:00 [info     ] 	prior = fixed                 [Run3ModelGen.modelgen]
2024-08-28 19:51:00 [info     ] 	num_models = 2                [Run3ModelGen.modelgen]
2024-08-28 19:51:00 [info     ] 	isGMSB = False                [Run3ModelGen.modelgen]
2024-08-28 19:51:00 [info     ] 	parameters =                  [Run3ModelGen.modelgen]
2024-08-28 19:51:00 [info     ] 		tanb: [60, 60]               [Run3ModelGen.modelgen]
2024-08-28 19:51:00 [info     ] 		M_1: [-1147.15, -1915.92]    [Run3ModelGen.modelgen]
2024-08-28 19:51:00 [info     ] 		M_2: [2000, 2000]            [Run3ModelGen.modelgen]
2024-08-28 19:51:00 [info     ] 		M_3: [4000, 4000]            [Run3ModelGen.modelgen]
2024-08-28 19:51:00 [info     ] 		AT: [4000, 4000]             [Run3ModelGen.modelgen]
2024-08-28 19:51:00 [info     ] 		Ab: [2000, 2000]             [Run3ModelGen.modelgen]
2024-08-28 19:51:00 [info     ] 		Atau: [2000, 2000]           [Run3ModelGen.modelgen]
2024-08-28 19:51:00 [info     ] 		mu: [2000, 2000]             [Run3ModelGen.modelgen]
2024-08-28 19:51:00 [info     ] 		mA: [2000, 2000]             [Run3ModelGen.modelgen]
2024-08-28 19:51:00 [info     ] 		meL: [2000, 2000]            [Run3ModelGen.modelgen]
2024-08-28 19:51:00 [info     ] 		mtauL: [2000, 2000]          [Run3ModelGen.modelgen]
2024-08-28 19:51:00 [info     ] 		meR: [2000, 2000]            [Run3ModelGen.modelgen]
2024-08-28 19:51:00 [info     ] 		mtauR: [2000, 2000]          [Run3ModelGen.modelgen]
2024-08-28 19:51:00 [info     ] 		mqL1: [4000, 4000]           [Run3ModelGen.modelgen]
2024-08-28 19:51:00 [info     ] 		mqL3: [4000, 4000]           [Run3ModelGen.modelgen]
2024-08-28 19:51:00 [info     ] 		muR: [4000, 4000]            [Run3ModelGen.modelgen]
2024-08-28 19:51:00 [info     ] 		mtR: [4000, 4000]            [Run3ModelGen.modelgen]
2024-08-28 19:51:00 [info     ] 		mdR: [4000, 4000]            [Run3ModelGen.modelgen]
2024-08-28 19:51:00 [info     ] 		mbR: [4000, 4000]            [Run3ModelGen.modelgen]
2024-08-28 19:51:00 [info     ] 	steps =                       [Run3ModelGen.modelgen]
2024-08-28 19:51:00 [info     ] 		{'name': 'prep_input', 'output_dir': 'input', 'prefix': 'IN'} [Run3ModelGen.modelgen]
2024-08-28 19:51:00 [info     ] 		{'name': 'SPheno', 'input_dir': 'input', 'output_dir': 'SPheno', 'log_dir': 'SPheno_log', 'prefix': 'SP'} [Run3ModelGen.modelgen]
2024-08-28 19:51:00 [info     ] 		{'name': 'softsusy', 'input_dir': 'input', 'output_dir': 'softsusy', 'prefix': 'SS'} [Run3ModelGen.modelgen]
2024-08-28 19:51:00 [info     ] 		{'name': 'micromegas', 'input_dir': 'SPheno', 'output_dir': 'micromegas', 'prefix': 'MO'} [Run3ModelGen.modelgen]
2024-08-28 19:51:00 [info     ] 		{'name': 'superiso', 'input_dir': 'SPheno', 'output_dir': 'superiso', 'prefix': 'SI'} [Run3ModelGen.modelgen]
2024-08-28 19:51:00 [info     ] 		{'name': 'gm2calc', 'input_dir': 'SPheno', 'output_dir': 'gm2calc', 'prefix': 'GM2'} [Run3ModelGen.modelgen]
2024-08-28 19:51:00 [info     ] 		{'name': 'evade', 'input_dir': 'SPheno', 'output_dir': 'evade', 'prefix': 'EV'} [Run3ModelGen.modelgen]
2024-08-28 19:51:00 [info     ] Starting Model Generation...   [Run3ModelGen.modelgen]
2024-08-28 19:51:00 [info     ] Setting up output directories for scan [Run3ModelGen.modelgen]
2024-08-28 19:51:00 [info     ] Generating Model: 0.slha (1/2) [Run3ModelGen.modelgen]
2024-08-28 19:51:00 [info     ] 	running step: prep_input      [Run3ModelGen.modelgen]
2024-08-28 19:51:00 [info     ] 	running step: SPheno          [Run3ModelGen.modelgen]
2024-08-28 19:51:01 [info     ] 	running step: softsusy        [Run3ModelGen.modelgen]
2024-08-28 19:51:01 [info     ] 	running step: micromegas      [Run3ModelGen.modelgen]
2024-08-28 19:51:01 [info     ] 	running step: superiso        [Run3ModelGen.modelgen]
2024-08-28 19:51:01 [info     ] 	running step: gm2calc         [Run3ModelGen.modelgen]
2024-08-28 19:51:01 [info     ] 	running step: evade           [Run3ModelGen.modelgen]
2024-08-28 19:51:01 [info     ] Generating Model: 1.slha (2/2) [Run3ModelGen.modelgen]
2024-08-28 19:51:01 [info     ] 	running step: prep_input      [Run3ModelGen.modelgen]
2024-08-28 19:51:01 [info     ] 	running step: SPheno          [Run3ModelGen.modelgen]
2024-08-28 19:51:01 [info     ] 	running step: softsusy        [Run3ModelGen.modelgen]
2024-08-28 19:51:01 [info     ] 	running step: micromegas      [Run3ModelGen.modelgen]
2024-08-28 19:51:03 [info     ] 	running step: superiso        [Run3ModelGen.modelgen]
2024-08-28 19:51:03 [info     ] 	running step: gm2calc         [Run3ModelGen.modelgen]
2024-08-28 19:51:03 [info     ] 	running step: evade           [Run3ModelGen.modelgen]
2024-08-28 19:51:03 [info     ] Finished generating models! Stats for successful generations (successful/attempted): [Run3ModelGen.modelgen]
2024-08-28 19:51:03 [info     ] 	prep_input: 2/2.              [Run3ModelGen.modelgen]
2024-08-28 19:51:03 [info     ] 	SPheno: 2/2.                  [Run3ModelGen.modelgen]
2024-08-28 19:51:03 [info     ] 	softsusy: 2/2.                [Run3ModelGen.modelgen]
2024-08-28 19:51:03 [info     ] 	micromegas: 2/2.              [Run3ModelGen.modelgen]
2024-08-28 19:51:03 [info     ] 	superiso: 2/2.                [Run3ModelGen.modelgen]
2024-08-28 19:51:03 [info     ] 	gm2calc: 2/2.                 [Run3ModelGen.modelgen]
2024-08-28 19:51:03 [info     ] 	evade: 0/2.                   [Run3ModelGen.modelgen]
2024-08-28 19:51:03 [info     ] Will make NTuple for: /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/scans/scan_2, with 2 models [Run3ModelGen.ntupling]
2024-08-28 19:51:03 [info     ] 	Filling values for model: 0   [Run3ModelGen.ntupling]
2024-08-28 19:51:03 [info     ] Saving NTuple in /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/scans/scan_2/ntuple.0.0.root ... [Run3ModelGen.ntupling]
Completed iteration 2
Starting iteration 3
Initial Training points:  tensor([0.8528, 0.5812, 0.1617, 0.0046], device='cuda:0') torch.Size([4])
Training points after adding:  tensor([0.1617, 0.0046, 0.2132, 0.0561, 0.2392, 0.5812, 0.0210, 0.8528],
       device='cuda:0') torch.Size([8])
These training_points are used in the GP tensor([0.1617, 0.0046, 0.2132, 0.0561, 0.2392, 0.5812, 0.0210, 0.8528],
       device='cuda:0')
Point: tensor([0.0000, 0.0010, 0.0020, 0.0030, 0.0040, 0.0050, 0.0060, 0.0070, 0.0080,
        0.0090, 0.0100, 0.0110, 0.0120, 0.0130, 0.0140, 0.0150, 0.0160, 0.0170,
        0.0180, 0.0190, 0.0200, 0.0210, 0.0220, 0.0230, 0.0240, 0.0250, 0.0260,
        0.0270, 0.0280, 0.0290, 0.0300, 0.0310, 0.0320, 0.0330, 0.0340, 0.0350,
        0.0360, 0.0370, 0.0380, 0.0390, 0.0400, 0.0410, 0.0420, 0.0430, 0.0440,
        0.0450, 0.0460, 0.0470, 0.0480, 0.0490, 0.0501, 0.0511, 0.0521, 0.0531,
        0.0541, 0.0551, 0.0561, 0.0571, 0.0581, 0.0591, 0.0601, 0.0611, 0.0621,
        0.0631, 0.0641, 0.0651, 0.0661, 0.0671, 0.0681, 0.0691, 0.0701, 0.0711,
        0.0721, 0.0731, 0.0741, 0.0751, 0.0761, 0.0771, 0.0781, 0.0791, 0.0801,
        0.0811, 0.0821, 0.0831, 0.0841, 0.0851, 0.0861, 0.0871, 0.0881, 0.0891,
        0.0901, 0.0911, 0.0921, 0.0931, 0.0941, 0.0951, 0.0961, 0.0971, 0.0981,
        0.0991, 0.1001, 0.1011, 0.1021, 0.1031, 0.1041, 0.1051, 0.1061, 0.1071,
        0.1081, 0.1091, 0.1101, 0.1111, 0.1121, 0.1131, 0.1141, 0.1151, 0.1161,
        0.1171, 0.1181, 0.1191, 0.1201, 0.1211, 0.1221, 0.1231, 0.1241, 0.1251,
        0.1261, 0.1271, 0.1281, 0.1291, 0.1301, 0.1311, 0.1321, 0.1331, 0.1341,
        0.1351, 0.1361, 0.1371, 0.1381, 0.1391, 0.1401, 0.1411, 0.1421, 0.1431,
        0.1441, 0.1451, 0.1461, 0.1471, 0.1481, 0.1491, 0.1502, 0.1512, 0.1522,
        0.1532, 0.1542, 0.1552, 0.1562, 0.1572, 0.1582, 0.1592, 0.1602, 0.1612,
        0.1622, 0.1632, 0.1642, 0.1652, 0.1662, 0.1672, 0.1682, 0.1692, 0.1702,
        0.1712, 0.1722, 0.1732, 0.1742, 0.1752, 0.1762, 0.1772, 0.1782, 0.1792,
        0.1802, 0.1812, 0.1822, 0.1832, 0.1842, 0.1852, 0.1862, 0.1872, 0.1882,
        0.1892, 0.1902, 0.1912, 0.1922, 0.1932, 0.1942, 0.1952, 0.1962, 0.1972,
        0.1982, 0.1992, 0.2002, 0.2012, 0.2022, 0.2032, 0.2042, 0.2052, 0.2062,
        0.2072, 0.2082, 0.2092, 0.2102, 0.2112, 0.2122, 0.2132, 0.2142, 0.2152,
        0.2162, 0.2172, 0.2182, 0.2192, 0.2202, 0.2212, 0.2222, 0.2232, 0.2242,
        0.2252, 0.2262, 0.2272, 0.2282, 0.2292, 0.2302, 0.2312, 0.2322, 0.2332,
        0.2342, 0.2352, 0.2362, 0.2372, 0.2382, 0.2392, 0.2402, 0.2412, 0.2422,
        0.2432, 0.2442, 0.2452, 0.2462, 0.2472, 0.2482, 0.2492, 0.2503, 0.2513,
        0.2523, 0.2533, 0.2543, 0.2553, 0.2563, 0.2573, 0.2583, 0.2593, 0.2603,
        0.2613, 0.2623, 0.2633, 0.2643, 0.2653, 0.2663, 0.2673, 0.2683, 0.2693,
        0.2703, 0.2713, 0.2723, 0.2733, 0.2743, 0.2753, 0.2763, 0.2773, 0.2783,
        0.2793, 0.2803, 0.2813, 0.2823, 0.2833, 0.2843, 0.2853, 0.2863, 0.2873,
        0.2883, 0.2893, 0.2903, 0.2913, 0.2923, 0.2933, 0.2943, 0.2953, 0.2963,
        0.2973, 0.2983, 0.2993, 0.3003, 0.3013, 0.3023, 0.3033, 0.3043, 0.3053,
        0.3063, 0.3073, 0.3083, 0.3093, 0.3103, 0.3113, 0.3123, 0.3133, 0.3143,
        0.3153, 0.3163, 0.3173, 0.3183, 0.3193, 0.3203, 0.3213, 0.3223, 0.3233,
        0.3243, 0.3253, 0.3263, 0.3273, 0.3283, 0.3293, 0.3303, 0.3313, 0.3323,
        0.3333, 0.3343, 0.3353, 0.3363, 0.3373, 0.3383, 0.3393, 0.3403, 0.3413,
        0.3423, 0.3433, 0.3443, 0.3453, 0.3463, 0.3473, 0.3483, 0.3493, 0.3504,
        0.3514, 0.3524, 0.3534, 0.3544, 0.3554, 0.3564, 0.3574, 0.3584, 0.3594,
        0.3604, 0.3614, 0.3624, 0.3634, 0.3644, 0.3654, 0.3664, 0.3674, 0.3684,
        0.3694, 0.3704, 0.3714, 0.3724, 0.3734, 0.3744, 0.3754, 0.3764, 0.3774,
        0.3784, 0.3794, 0.3804, 0.3814, 0.3824, 0.3834, 0.3844, 0.3854, 0.3864,
        0.3874, 0.3884, 0.3894, 0.3904, 0.3914, 0.3924, 0.3934, 0.3944, 0.3954,
        0.3964, 0.3974, 0.3984, 0.3994, 0.4004, 0.4014, 0.4024, 0.4034, 0.4044,
        0.4054, 0.4064, 0.4074, 0.4084, 0.4094, 0.4104, 0.4114, 0.4124, 0.4134,
        0.4144, 0.4154, 0.4164, 0.4174, 0.4184, 0.4194, 0.4204, 0.4214, 0.4224,
        0.4234, 0.4244, 0.4254, 0.4264, 0.4274, 0.4284, 0.4294, 0.4304, 0.4314,
        0.4324, 0.4334, 0.4344, 0.4354, 0.4364, 0.4374, 0.4384, 0.4394, 0.4404,
        0.4414, 0.4424, 0.4434, 0.4444, 0.4454, 0.4464, 0.4474, 0.4484, 0.4494,
        0.4505, 0.4515, 0.4525, 0.4535, 0.4545, 0.4555, 0.4565, 0.4575, 0.4585,
        0.4595, 0.4605, 0.4615, 0.4625, 0.4635, 0.4645, 0.4655, 0.4665, 0.4675,
        0.4685, 0.4695, 0.4705, 0.4715, 0.4725, 0.4735, 0.4745, 0.4755, 0.4765,
        0.4775, 0.4785, 0.4795, 0.4805, 0.4815, 0.4825, 0.4835, 0.4845, 0.4855,
        0.4865, 0.4875, 0.4885, 0.4895, 0.4905, 0.4915, 0.4925, 0.4935, 0.4945,
        0.4955, 0.4965, 0.4975, 0.4985, 0.4995, 0.5005, 0.5015, 0.5025, 0.5035,
        0.5045, 0.5055, 0.5065, 0.5075, 0.5085, 0.5095, 0.5105, 0.5115, 0.5125,
        0.5135, 0.5145, 0.5155, 0.5165, 0.5175, 0.5185, 0.5195, 0.5205, 0.5215,
        0.5225, 0.5235, 0.5245, 0.5255, 0.5265, 0.5275, 0.5285, 0.5295, 0.5305,
        0.5315, 0.5325, 0.5335, 0.5345, 0.5355, 0.5365, 0.5375, 0.5385, 0.5395,
        0.5405, 0.5415, 0.5425, 0.5435, 0.5445, 0.5455, 0.5465, 0.5475, 0.5485,
        0.5495, 0.5506, 0.5516, 0.5526, 0.5536, 0.5546, 0.5556, 0.5566, 0.5576,
        0.5586, 0.5596, 0.5606, 0.5616, 0.5626, 0.5636, 0.5646, 0.5656, 0.5666,
        0.5676, 0.5686, 0.5696, 0.5706, 0.5716, 0.5726, 0.5736, 0.5746, 0.5756,
        0.5766, 0.5776, 0.5786, 0.5796, 0.5806, 0.5816, 0.5826, 0.5836, 0.5846,
        0.5856, 0.5866, 0.5876, 0.5886, 0.5896, 0.5906, 0.5916, 0.5926, 0.5936,
        0.5946, 0.5956, 0.5966, 0.5976, 0.5986, 0.5996, 0.6006, 0.6016, 0.6026,
        0.6036, 0.6046, 0.6056, 0.6066, 0.6076, 0.6086, 0.6096, 0.6106, 0.6116,
        0.6126, 0.6136, 0.6146, 0.6156, 0.6166, 0.6176, 0.6186, 0.6196, 0.6206,
        0.6216, 0.6226, 0.6236, 0.6246, 0.6256, 0.6266, 0.6276, 0.6286, 0.6296,
        0.6306, 0.6316, 0.6326, 0.6336, 0.6346, 0.6356, 0.6366, 0.6376, 0.6386,
        0.6396, 0.6406, 0.6416, 0.6426, 0.6436, 0.6446, 0.6456, 0.6466, 0.6476,
        0.6486, 0.6496, 0.6507, 0.6517, 0.6527, 0.6537, 0.6547, 0.6557, 0.6567,
        0.6577, 0.6587, 0.6597, 0.6607, 0.6617, 0.6627, 0.6637, 0.6647, 0.6657,
        0.6667, 0.6677, 0.6687, 0.6697, 0.6707, 0.6717, 0.6727, 0.6737, 0.6747,
        0.6757, 0.6767, 0.6777, 0.6787, 0.6797, 0.6807, 0.6817, 0.6827, 0.6837,
        0.6847, 0.6857, 0.6867, 0.6877, 0.6887, 0.6897, 0.6907, 0.6917, 0.6927,
        0.6937, 0.6947, 0.6957, 0.6967, 0.6977, 0.6987, 0.6997, 0.7007, 0.7017,
        0.7027, 0.7037, 0.7047, 0.7057, 0.7067, 0.7077, 0.7087, 0.7097, 0.7107,
        0.7117, 0.7127, 0.7137, 0.7147, 0.7157, 0.7167, 0.7177, 0.7187, 0.7197,
        0.7207, 0.7217, 0.7227, 0.7237, 0.7247, 0.7257, 0.7267, 0.7277, 0.7287,
        0.7297, 0.7307, 0.7317, 0.7327, 0.7337, 0.7347, 0.7357, 0.7367, 0.7377,
        0.7387, 0.7397, 0.7407, 0.7417, 0.7427, 0.7437, 0.7447, 0.7457, 0.7467,
        0.7477, 0.7487, 0.7497, 0.7508, 0.7518, 0.7528, 0.7538, 0.7548, 0.7558,
        0.7568, 0.7578, 0.7588, 0.7598, 0.7608, 0.7618, 0.7628, 0.7638, 0.7648,
        0.7658, 0.7668, 0.7678, 0.7688, 0.7698, 0.7708, 0.7718, 0.7728, 0.7738,
        0.7748, 0.7758, 0.7768, 0.7778, 0.7788, 0.7798, 0.7808, 0.7818, 0.7828,
        0.7838, 0.7848, 0.7858, 0.7868, 0.7878, 0.7888, 0.7898, 0.7908, 0.7918,
        0.7928, 0.7938, 0.7948, 0.7958, 0.7968, 0.7978, 0.7988, 0.7998, 0.8008,
        0.8018, 0.8028, 0.8038, 0.8048, 0.8058, 0.8068, 0.8078, 0.8088, 0.8098,
        0.8108, 0.8118, 0.8128, 0.8138, 0.8148, 0.8158, 0.8168, 0.8178, 0.8188,
        0.8198, 0.8208, 0.8218, 0.8228, 0.8238, 0.8248, 0.8258, 0.8268, 0.8278,
        0.8288, 0.8298, 0.8308, 0.8318, 0.8328, 0.8338, 0.8348, 0.8358, 0.8368,
        0.8378, 0.8388, 0.8398, 0.8408, 0.8418, 0.8428, 0.8438, 0.8448, 0.8458,
        0.8468, 0.8478, 0.8488, 0.8498, 0.8509, 0.8519, 0.8529, 0.8539, 0.8549,
        0.8559, 0.8569, 0.8579, 0.8589, 0.8599, 0.8609, 0.8619, 0.8629, 0.8639,
        0.8649, 0.8659, 0.8669, 0.8679, 0.8689, 0.8699, 0.8709, 0.8719, 0.8729,
        0.8739, 0.8749, 0.8759, 0.8769, 0.8779, 0.8789, 0.8799, 0.8809, 0.8819,
        0.8829, 0.8839, 0.8849, 0.8859, 0.8869, 0.8879, 0.8889, 0.8899, 0.8909,
        0.8919, 0.8929, 0.8939, 0.8949, 0.8959, 0.8969, 0.8979, 0.8989, 0.8999,
        0.9009, 0.9019, 0.9029, 0.9039, 0.9049, 0.9059, 0.9069, 0.9079, 0.9089,
        0.9099, 0.9109, 0.9119, 0.9129, 0.9139, 0.9149, 0.9159, 0.9169, 0.9179,
        0.9189, 0.9199, 0.9209, 0.9219, 0.9229, 0.9239, 0.9249, 0.9259, 0.9269,
        0.9279, 0.9289, 0.9299, 0.9309, 0.9319, 0.9329, 0.9339, 0.9349, 0.9359,
        0.9369, 0.9379, 0.9389, 0.9399, 0.9409, 0.9419, 0.9429, 0.9439, 0.9449,
        0.9459, 0.9469, 0.9479, 0.9489, 0.9499, 0.9510, 0.9520, 0.9530, 0.9540,
        0.9550, 0.9560, 0.9570, 0.9580, 0.9590, 0.9600, 0.9610, 0.9620, 0.9630,
        0.9640, 0.9650, 0.9660, 0.9670, 0.9680, 0.9690, 0.9700, 0.9710, 0.9720,
        0.9730, 0.9740, 0.9750, 0.9760, 0.9770, 0.9780, 0.9790, 0.9800, 0.9810,
        0.9820, 0.9830, 0.9840, 0.9850, 0.9860, 0.9870, 0.9880, 0.9890, 0.9900,
        0.9910, 0.9920, 0.9930, 0.9940, 0.9950, 0.9960, 0.9970, 0.9980, 0.9990,
        1.0000], device='cuda:0') - Variance: tensor([[1.0000e-06],
        [4.3152e-04],
        [1.0000e-06],
        [1.1023e-02],
        [1.0000e-06],
        [1.0000e-06],
        [1.3245e-03],
        [8.8487e-03],
        [3.2350e-02],
        [1.0000e-06],
        [1.0000e-06],
        [1.0000e-06],
        [1.0000e-06],
        [9.7075e-02],
        [1.0000e-06],
        [1.2636e-02],
        [1.0000e-06],
        [1.0000e-06],
        [5.1406e-02],
        [1.0000e-06],
        [1.5186e-01],
        [1.0000e-06],
        [7.6543e-03],
        [1.0000e-06],
        [1.0000e-06],
        [1.0000e-06],
        [1.0000e-06],
        [1.7533e-02],
        [1.0000e-06],
        [6.1469e-02],
        [1.0000e-06],
        [9.3176e-02],
        [1.0000e-06],
        [1.0000e-06],
        [1.0000e-06],
        [1.0000e-06],
        [1.0000e-06],
        [1.0000e-06],
        [2.0245e-01],
        [1.0000e-06],
        [1.0000e-06],
        [3.9815e-02],
        [1.0000e-06],
        [1.0000e-06],
        [1.0000e-06],
        [1.0000e-06],
        [1.0000e-06],
        [1.0000e-06],
        [1.0000e-06],
        [1.0000e-06],
        [1.0000e-06],
        [1.0000e-06],
        [9.4963e-03],
        [4.0462e-03],
        [1.1125e-01],
        [7.8587e-03],
        [1.0000e-06],
        [6.7129e-03],
        [1.1013e-02],
        [1.0000e-06],
        [1.0000e-06],
        [2.5852e-02],
        [1.0000e-06],
        [1.0000e-06],
        [1.0000e-06],
        [1.0000e-06],
        [4.2511e-02],
        [1.0000e-06],
        [1.4301e-01],
        [1.0000e-06],
        [1.0000e-06],
        [5.6882e-02],
        [1.0000e-06],
        [1.5702e-01],
        [1.0000e-06],
        [2.4740e-01],
        [7.5224e-02],
        [1.0000e-06],
        [1.7577e-01],
        [3.1433e-01],
        [3.5329e-01],
        [2.0618e-01],
        [2.5169e-01],
        [3.7837e-01],
        [4.1447e-01],
        [5.1411e-01],
        [5.4267e-01],
        [5.6970e-01],
        [6.4348e-01],
        [5.6261e-01],
        [6.3772e-01],
        [6.5946e-01],
        [7.1822e-01],
        [7.6701e-01],
        [7.8110e-01],
        [8.1914e-01],
        [7.7743e-01],
        [8.1590e-01],
        [8.4774e-01],
        [8.5652e-01],
        [8.8112e-01],
        [9.0137e-01],
        [9.1800e-01],
        [9.3161e-01],
        [9.1340e-01],
        [9.2666e-01],
        [9.3730e-01],
        [9.4572e-01],
        [9.5225e-01],
        [9.5717e-01],
        [9.6067e-01],
        [9.6294e-01],
        [9.5343e-01],
        [9.5387e-01],
        [9.5314e-01],
        [9.5128e-01],
        [9.4833e-01],
        [9.4431e-01],
        [9.4700e-01],
        [9.2676e-01],
        [9.1879e-01],
        [9.0942e-01],
        [8.9854e-01],
        [9.0007e-01],
        [8.8741e-01],
        [8.7298e-01],
        [8.7409e-01],
        [8.2463e-01],
        [8.0189e-01],
        [8.0314e-01],
        [7.7747e-01],
        [7.7870e-01],
        [7.4981e-01],
        [7.5109e-01],
        [7.1862e-01],
        [6.5643e-01],
        [6.1184e-01],
        [6.1375e-01],
        [6.1564e-01],
        [5.6585e-01],
        [5.6799e-01],
        [5.7014e-01],
        [4.0526e-01],
        [4.0824e-01],
        [4.1123e-01],
        [4.1422e-01],
        [3.3887e-01],
        [3.4229e-01],
        [3.4570e-01],
        [3.4912e-01],
        [2.0709e-01],
        [2.1126e-01],
        [2.1540e-01],
        [2.1951e-01],
        [2.2358e-01],
        [2.2759e-01],
        [2.3151e-01],
        [6.3857e-02],
        [6.8398e-02],
        [7.2744e-02],
        [7.6846e-02],
        [8.0648e-02],
        [8.4083e-02],
        [1.9501e-01],
        [1.9718e-01],
        [1.9787e-02],
        [2.1077e-02],
        [1.3706e-01],
        [1.3635e-01],
        [1.3445e-01],
        [2.3391e-01],
        [2.2953e-01],
        [2.2343e-01],
        [1.5571e-01],
        [1.4497e-01],
        [2.3409e-01],
        [2.1911e-01],
        [2.9527e-01],
        [2.7546e-01],
        [3.4014e-01],
        [1.6848e-01],
        [2.3537e-01],
        [2.9287e-01],
        [2.5379e-01],
        [3.0146e-01],
        [3.4199e-01],
        [3.7629e-01],
        [3.2518e-01],
        [2.2152e-01],
        [2.4950e-01],
        [2.7244e-01],
        [2.9100e-01],
        [2.1253e-01],
        [2.2545e-01],
        [2.3507e-01],
        [9.4361e-02],
        [9.9959e-02],
        [1.0286e-01],
        [1.0339e-01],
        [1.0182e-01],
        [9.8396e-02],
        [9.3292e-02],
        [8.6664e-02],
        [1.0000e-06],
        [1.0000e-06],
        [1.2375e-02],
        [9.1293e-05],
        [1.0000e-06],
        [3.4064e-02],
        [1.8522e-02],
        [6.2304e-02],
        [7.3016e-03],
        [1.0647e-02],
        [1.1213e-02],
        [1.1052e-02],
        [3.5414e-02],
        [9.0873e-03],
        [4.1924e-02],
        [1.0000e-06],
        [1.0000e-06],
        [1.0000e-06],
        [1.0000e-06],
        [6.4297e-02],
        [7.9856e-02],
        [9.1956e-02],
        [1.0061e-01],
        [1.0338e-02],
        [1.3067e-02],
        [1.1999e-02],
        [6.7625e-02],
        [5.9461e-02],
        [1.0573e-01],
        [9.1272e-02],
        [1.2986e-01],
        [2.0302e-02],
        [1.0000e-06],
        [2.9445e-02],
        [5.9952e-02],
        [8.7227e-02],
        [1.1169e-01],
        [1.3373e-01],
        [1.1320e-02],
        [3.2771e-02],
        [5.2359e-02],
        [7.0351e-02],
        [8.6983e-02],
        [1.5718e-01],
        [1.7080e-01],
        [1.8365e-01],
        [1.1918e-01],
        [1.3202e-01],
        [1.4436e-01],
        [2.0774e-01],
        [2.6632e-01],
        [2.7630e-01],
        [3.2970e-01],
        [2.7581e-01],
        [2.8561e-01],
        [3.3835e-01],
        [3.8728e-01],
        [4.3269e-01],
        [4.7485e-01],
        [5.1398e-01],
        [5.5030e-01],
        [5.1436e-01],
        [5.5082e-01],
        [5.8466e-01],
        [6.1607e-01],
        [6.6706e-01],
        [6.9246e-01],
        [7.1602e-01],
        [7.5410e-01],
        [7.3462e-01],
        [7.7035e-01],
        [7.8824e-01],
        [8.1699e-01],
        [8.3141e-01],
        [8.5452e-01],
        [8.7459e-01],
        [8.6460e-01],
        [8.8338e-01],
        [8.9968e-01],
        [9.1383e-01],
        [9.2610e-01],
        [9.3674e-01],
        [9.4598e-01],
        [9.5398e-01],
        [9.5348e-01],
        [9.6049e-01],
        [9.6894e-01],
        [9.7389e-01],
        [9.7902e-01],
        [9.8262e-01],
        [9.8480e-01],
        [9.8817e-01],
        [9.9101e-01],
        [9.9240e-01],
        [9.9455e-01],
        [9.9665e-01],
        [9.9811e-01],
        [9.9904e-01],
        [1.0001e+00],
        [1.0012e+00],
        [1.0020e+00],
        [1.0024e+00],
        [1.0030e+00],
        [1.0035e+00],
        [1.0040e+00],
        [1.0042e+00],
        [1.0045e+00],
        [1.0047e+00],
        [1.0049e+00],
        [1.0050e+00],
        [1.0052e+00],
        [1.0053e+00],
        [1.0053e+00],
        [1.0054e+00],
        [1.0055e+00],
        [1.0055e+00],
        [1.0055e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0055e+00],
        [1.0055e+00],
        [1.0054e+00],
        [1.0054e+00],
        [1.0053e+00],
        [1.0052e+00],
        [1.0051e+00],
        [1.0050e+00],
        [1.0048e+00],
        [1.0046e+00],
        [1.0044e+00],
        [1.0041e+00],
        [1.0038e+00],
        [1.0033e+00],
        [1.0029e+00],
        [1.0023e+00],
        [1.0016e+00],
        [1.0009e+00],
        [9.9993e-01],
        [9.9886e-01],
        [9.9760e-01],
        [9.9615e-01],
        [9.9450e-01],
        [9.9255e-01],
        [9.9032e-01],
        [9.8776e-01],
        [9.8484e-01],
        [9.8151e-01],
        [9.7773e-01],
        [9.7345e-01],
        [9.6863e-01],
        [9.6318e-01],
        [9.5711e-01],
        [9.5033e-01],
        [9.4279e-01],
        [9.3437e-01],
        [9.2562e-01],
        [9.1542e-01],
        [9.0433e-01],
        [8.9210e-01],
        [8.7888e-01],
        [8.6440e-01],
        [8.4889e-01],
        [8.3200e-01],
        [8.1368e-01],
        [7.9425e-01],
        [7.7377e-01],
        [7.5179e-01],
        [7.2882e-01],
        [7.0435e-01],
        [6.7837e-01],
        [6.5365e-01],
        [6.2398e-01],
        [5.9676e-01],
        [5.6574e-01],
        [5.3804e-01],
        [5.0256e-01],
        [4.7504e-01],
        [4.3923e-01],
        [4.1058e-01],
        [3.7289e-01],
        [3.4605e-01],
        [3.0974e-01],
        [2.8305e-01],
        [2.4921e-01],
        [2.2326e-01],
        [1.9958e-01],
        [1.6839e-01],
        [1.4641e-01],
        [1.2011e-01],
        [1.0394e-01],
        [7.9957e-02],
        [7.0387e-02],
        [4.9258e-02],
        [3.9371e-02],
        [2.5232e-02],
        [2.2805e-02],
        [1.6184e-02],
        [2.1454e-02],
        [1.4824e-02],
        [2.7772e-02],
        [2.8846e-02],
        [4.9112e-02],
        [5.7599e-02],
        [7.7269e-02],
        [8.5505e-02],
        [1.1161e-01],
        [1.4373e-01],
        [1.5803e-01],
        [1.8848e-01],
        [2.1450e-01],
        [2.4886e-01],
        [2.7295e-01],
        [3.0477e-01],
        [3.3236e-01],
        [3.7159e-01],
        [3.9656e-01],
        [4.3204e-01],
        [4.6317e-01],
        [4.9875e-01],
        [5.2626e-01],
        [5.5770e-01],
        [5.9358e-01],
        [6.1594e-01],
        [6.4280e-01],
        [6.7586e-01],
        [6.9859e-01],
        [7.2419e-01],
        [7.4359e-01],
        [7.7116e-01],
        [7.9071e-01],
        [8.0863e-01],
        [8.2506e-01],
        [8.4656e-01],
        [8.5983e-01],
        [8.7406e-01],
        [8.8690e-01],
        [9.0104e-01],
        [9.1272e-01],
        [9.2179e-01],
        [9.3115e-01],
        [9.4104e-01],
        [9.4825e-01],
        [9.5545e-01],
        [9.6105e-01],
        [9.6758e-01],
        [9.7235e-01],
        [9.7653e-01],
        [9.8079e-01],
        [9.8425e-01],
        [9.8694e-01],
        [9.8954e-01],
        [9.9212e-01],
        [9.9400e-01],
        [9.9578e-01],
        [9.9716e-01],
        [9.9863e-01],
        [9.9970e-01],
        [1.0006e+00],
        [1.0014e+00],
        [1.0022e+00],
        [1.0027e+00],
        [1.0032e+00],
        [1.0036e+00],
        [1.0040e+00],
        [1.0043e+00],
        [1.0045e+00],
        [1.0048e+00],
        [1.0049e+00],
        [1.0051e+00],
        [1.0052e+00],
        [1.0053e+00],
        [1.0054e+00],
        [1.0054e+00],
        [1.0055e+00],
        [1.0055e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0055e+00],
        [1.0055e+00],
        [1.0054e+00],
        [1.0053e+00],
        [1.0052e+00],
        [1.0051e+00],
        [1.0049e+00],
        [1.0049e+00],
        [1.0047e+00],
        [1.0045e+00],
        [1.0042e+00],
        [1.0038e+00],
        [1.0033e+00],
        [1.0029e+00],
        [1.0028e+00],
        [1.0021e+00],
        [1.0014e+00],
        [1.0003e+00],
        [9.9924e-01],
        [9.9803e-01],
        [9.9600e-01],
        [9.9418e-01],
        [9.9388e-01],
        [9.9166e-01],
        [9.8902e-01],
        [9.8589e-01],
        [9.8216e-01],
        [9.7774e-01],
        [9.7248e-01],
        [9.6624e-01],
        [9.6767e-01],
        [9.6052e-01],
        [9.5203e-01],
        [9.4942e-01],
        [9.3884e-01],
        [9.2627e-01],
        [9.1133e-01],
        [9.1477e-01],
        [9.1036e-01],
        [8.9242e-01],
        [8.7112e-01],
        [8.4580e-01],
        [8.3805e-01],
        [8.0651e-01],
        [7.6905e-01],
        [8.0446e-01],
        [7.6661e-01],
        [7.5501e-01],
        [7.0786e-01],
        [6.9341e-01],
        [6.3467e-01],
        [6.1668e-01],
        [6.3085e-01],
        [6.1267e-01],
        [5.9360e-01],
        [5.1609e-01],
        [4.9234e-01],
        [4.6744e-01],
        [3.6619e-01],
        [3.3517e-01],
        [4.3552e-01],
        [4.0786e-01],
        [2.9541e-01],
        [2.6096e-01],
        [2.2483e-01],
        [1.8696e-01],
        [1.4725e-01],
        [1.0561e-01],
        [2.4031e-01],
        [2.0319e-01],
        [1.6426e-01],
        [1.2345e-01],
        [8.0663e-02],
        [3.5797e-02],
        [1.1341e-02],
        [1.4094e-01],
        [9.8997e-02],
        [5.5020e-02],
        [1.1341e-02],
        [8.3369e-02],
        [3.8634e-02],
        [1.1341e-02],
        [1.1341e-02],
        [2.0788e-01],
        [1.6918e-01],
        [1.2861e-01],
        [1.9413e-01],
        [1.5476e-01],
        [1.1349e-01],
        [1.8014e-01],
        [3.0368e-01],
        [3.5612e-01],
        [3.2462e-01],
        [3.7549e-01],
        [3.4492e-01],
        [3.9428e-01],
        [3.6463e-01],
        [4.1252e-01],
        [5.6055e-01],
        [5.3896e-01],
        [5.7383e-01],
        [6.0609e-01],
        [5.8670e-01],
        [6.1800e-01],
        [6.4696e-01],
        [6.7376e-01],
        [7.5660e-01],
        [7.4452e-01],
        [7.6403e-01],
        [7.8208e-01],
        [7.9878e-01],
        [8.1424e-01],
        [8.2854e-01],
        [8.7275e-01],
        [8.8268e-01],
        [8.9187e-01],
        [9.0037e-01],
        [9.0824e-01],
        [9.1552e-01],
        [9.2225e-01],
        [9.2848e-01],
        [9.5456e-01],
        [9.5838e-01],
        [9.6191e-01],
        [9.6518e-01],
        [9.6821e-01],
        [9.7508e-01],
        [9.7737e-01],
        [9.7948e-01],
        [9.8833e-01],
        [9.8963e-01],
        [9.9258e-01],
        [9.9355e-01],
        [9.9446e-01],
        [9.9652e-01],
        [9.9720e-01],
        [1.0001e+00],
        [1.0005e+00],
        [1.0014e+00],
        [1.0022e+00],
        [1.0025e+00],
        [1.0031e+00],
        [1.0033e+00],
        [1.0037e+00],
        [1.0044e+00],
        [1.0046e+00],
        [1.0047e+00],
        [1.0047e+00],
        [1.0050e+00],
        [1.0052e+00],
        [1.0052e+00],
        [1.0054e+00],
        [1.0054e+00],
        [1.0055e+00],
        [1.0055e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00]], device='cuda:0') 
Smoothed_batch_entropy:  tensor([-3.0621e-02,  5.8818e-05, -2.4495e-03,  2.9328e-03, -1.8702e-02,
        -2.4641e-03,  3.6366e-04,  2.0846e-03,  9.5001e-03, -5.9761e-03,
        -9.6155e-03, -3.5652e-04, -9.1913e-03,  1.9194e-02, -9.3204e-03,
         2.2604e-03, -9.7507e-03, -1.5122e-02,  7.3919e-03, -1.7643e-02,
         1.8414e-02, -1.3942e-02,  8.2242e-04, -1.0508e-03, -5.2157e-02,
        -1.7382e-03, -3.4017e-02,  1.5275e-03, -6.2457e-03,  4.9112e-03,
        -5.8815e-03,  6.9883e-03, -3.1934e-03, -2.4447e-02, -1.6985e-03,
        -1.2739e-02, -1.2193e-03, -1.2211e-02,  1.3262e-02, -1.2798e-02,
        -2.1580e-02,  2.5125e-03, -1.8596e-02, -1.6330e-04, -8.2947e-03,
        -3.9738e-03, -6.8076e-03, -9.9601e-03, -1.6940e-03, -8.3885e-03,
        -1.0329e-03, -2.4852e-03,  6.1295e-04,  2.5651e-04,  7.3699e-03,
         4.9419e-04, -1.2856e-04,  4.5971e-04,  7.2336e-04, -1.3483e-03,
        -4.2531e-04,  1.8709e-03, -2.9520e-03, -1.7910e-03, -5.1132e-03,
        -5.0149e-03,  3.1129e-03, -9.0398e-03,  1.0719e-02, -4.1116e-03,
        -1.1735e-02,  4.3578e-03, -1.2480e-02,  1.2123e-02, -2.8500e-03,
         1.9157e-02,  5.9451e-03, -1.0989e-02,  1.3914e-02,  2.4544e-02,
         2.7542e-02,  1.6367e-02,  1.9953e-02,  2.9645e-02,  3.2393e-02,
         3.9699e-02,  4.1803e-02,  4.3781e-02,  4.8963e-02,  4.3321e-02,
         4.8576e-02,  5.0079e-02,  5.3999e-02,  5.7157e-02,  5.8014e-02,
         6.0364e-02,  5.7660e-02,  5.9997e-02,  6.1856e-02,  6.2274e-02,
         6.3604e-02,  6.4633e-02,  6.5418e-02,  6.6005e-02,  6.4791e-02,
         6.5331e-02,  6.5701e-02,  6.5931e-02,  6.6042e-02,  6.6056e-02,
         6.5983e-02,  6.5835e-02,  6.4971e-02,  6.4697e-02,  6.4352e-02,
         6.3944e-02,  6.3469e-02,  6.2929e-02,  6.2888e-02,  6.1233e-02,
         6.0446e-02,  5.9582e-02,  5.8613e-02,  5.8547e-02,  5.7488e-02,
         5.6318e-02,  5.6247e-02,  5.2707e-02,  5.1034e-02,  5.0984e-02,
         4.9141e-02,  4.9109e-02,  4.7085e-02,  4.7045e-02,  4.4809e-02,
         4.0707e-02,  3.7759e-02,  3.7772e-02,  3.7802e-02,  3.4575e-02,
         3.4595e-02,  3.4651e-02,  2.4406e-02,  2.4504e-02,  2.4598e-02,
         2.4711e-02,  2.0084e-02,  2.0235e-02,  2.0358e-02,  2.0498e-02,
         1.2033e-02,  1.2235e-02,  1.2429e-02,  1.2589e-02,  1.2761e-02,
         1.2922e-02,  1.3068e-02,  3.5807e-03,  3.7897e-03,  4.0030e-03,
         4.2175e-03,  4.3781e-03,  4.5317e-03,  1.0489e-02,  1.0525e-02,
         1.0273e-03,  1.0869e-03,  7.0893e-03,  6.9847e-03,  6.8078e-03,
         1.1866e-02,  1.1529e-02,  1.1074e-02,  7.5945e-03,  6.9815e-03,
         1.1289e-02,  1.0407e-02,  1.4041e-02,  1.2914e-02,  1.5970e-02,
         7.6282e-03,  1.0664e-02,  1.3296e-02,  1.1347e-02,  1.3513e-02,
         1.5358e-02,  1.6917e-02,  1.4419e-02,  9.6082e-03,  1.0858e-02,
         1.1880e-02,  1.2723e-02,  9.1733e-03,  9.7536e-03,  1.0235e-02,
         4.0129e-03,  4.2927e-03,  4.4471e-03,  4.4720e-03,  4.4498e-03,
         4.3026e-03,  4.1432e-03,  3.8617e-03, -1.3416e-03, -1.7823e-03,
         5.5396e-04, -8.9431e-06, -6.3057e-04,  1.6215e-03,  9.1503e-04,
         3.0807e-03,  3.7191e-04,  5.3710e-04,  6.0357e-04,  5.8010e-04,
         1.8752e-03,  4.8594e-04,  2.3413e-03, -9.3060e-04, -1.3449e-03,
        -1.8190e-03, -1.0277e-03,  4.0279e-03,  5.1406e-03,  6.0825e-03,
         6.8277e-03,  7.3514e-04,  9.4840e-04,  9.0411e-04,  5.1498e-03,
         4.6913e-03,  8.4552e-03,  7.5288e-03,  1.0832e-02,  1.8036e-03,
        -4.3735e-04,  2.7184e-03,  5.6268e-03,  8.3051e-03,  1.0746e-02,
         1.2978e-02,  1.1315e-03,  3.4537e-03,  5.5538e-03,  7.5182e-03,
         9.3105e-03,  1.6481e-02,  1.8047e-02,  1.9456e-02,  1.3179e-02,
         1.4608e-02,  1.6052e-02,  2.2459e-02,  2.8058e-02,  2.9058e-02,
         3.3804e-02,  2.9249e-02,  3.0258e-02,  3.4869e-02,  3.8851e-02,
         4.2307e-02,  4.5257e-02,  4.7858e-02,  5.0057e-02,  4.7954e-02,
         5.0186e-02,  5.2070e-02,  5.3674e-02,  5.6008e-02,  5.7033e-02,
         5.7911e-02,  5.9111e-02,  5.8468e-02,  5.9510e-02,  5.9921e-02,
         6.0490e-02,  6.0684e-02,  6.0929e-02,  6.1029e-02,  6.0894e-02,
         6.0931e-02,  6.0869e-02,  6.0728e-02,  6.0528e-02,  6.0284e-02,
         6.0012e-02,  5.9714e-02,  5.9658e-02,  5.9347e-02,  5.8918e-02,
         5.8595e-02,  5.8219e-02,  5.7902e-02,  5.7665e-02,  5.7309e-02,
         5.6966e-02,  5.6747e-02,  5.6425e-02,  5.6078e-02,  5.5788e-02,
         5.5563e-02,  5.5298e-02,  5.5016e-02,  5.4751e-02,  5.4594e-02,
         5.4354e-02,  5.4130e-02,  5.3900e-02,  5.3750e-02,  5.3562e-02,
         5.3371e-02,  5.3206e-02,  5.3070e-02,  5.2921e-02,  5.2771e-02,
         5.2653e-02,  5.2517e-02,  5.2390e-02,  5.2269e-02,  5.2170e-02,
         5.2055e-02,  5.1950e-02,  5.1846e-02,  5.1760e-02,  5.1664e-02,
         5.1572e-02,  5.1483e-02,  5.1404e-02,  5.1320e-02,  5.1240e-02,
         5.1161e-02,  5.1088e-02,  5.1012e-02,  5.0938e-02,  5.0868e-02,
         5.0796e-02,  5.0725e-02,  5.0656e-02,  5.0589e-02,  5.0521e-02,
         5.0454e-02,  5.0388e-02,  5.0323e-02,  5.0257e-02,  5.0193e-02,
         5.0128e-02,  5.0065e-02,  5.0001e-02,  4.9938e-02,  4.9875e-02,
         4.9812e-02,  4.9750e-02,  4.9688e-02,  4.9626e-02,  4.9564e-02,
         4.9502e-02,  4.9441e-02,  4.9380e-02,  4.9318e-02,  4.9257e-02,
         4.9196e-02,  4.9136e-02,  4.9075e-02,  4.9015e-02,  4.8955e-02,
         4.8894e-02,  4.8834e-02,  4.8774e-02,  4.8715e-02,  4.8655e-02,
         4.8595e-02,  4.8536e-02,  4.8476e-02,  4.8417e-02,  4.8358e-02,
         4.8299e-02,  4.8240e-02,  4.8181e-02,  4.8123e-02,  4.8064e-02,
         4.8006e-02,  4.7947e-02,  4.7889e-02,  4.7831e-02,  4.7773e-02,
         4.7715e-02,  4.7657e-02,  4.7599e-02,  4.7541e-02,  4.7484e-02,
         4.7426e-02,  4.7369e-02,  4.7312e-02,  4.7254e-02,  4.7198e-02,
         4.7140e-02,  4.7084e-02,  4.7027e-02,  4.6970e-02,  4.6914e-02,
         4.6857e-02,  4.6801e-02,  4.6745e-02,  4.6689e-02,  4.6633e-02,
         4.6577e-02,  4.6521e-02,  4.6465e-02,  4.6409e-02,  4.6354e-02,
         4.6298e-02,  4.6243e-02,  4.6188e-02,  4.6133e-02,  4.6077e-02,
         4.6023e-02,  4.5968e-02,  4.5913e-02,  4.5858e-02,  4.5804e-02,
         4.5749e-02,  4.5695e-02,  4.5640e-02,  4.5586e-02,  4.5532e-02,
         4.5478e-02,  4.5424e-02,  4.5370e-02,  4.5317e-02,  4.5263e-02,
         4.5210e-02,  4.5156e-02,  4.5103e-02,  4.5050e-02,  4.4996e-02,
         4.4943e-02,  4.4890e-02,  4.4837e-02,  4.4785e-02,  4.4732e-02,
         4.4679e-02,  4.4627e-02,  4.4574e-02,  4.4522e-02,  4.4470e-02,
         4.4417e-02,  4.4365e-02,  4.4313e-02,  4.4262e-02,  4.4210e-02,
         4.4158e-02,  4.4106e-02,  4.4055e-02,  4.4003e-02,  4.3952e-02,
         4.3901e-02,  4.3849e-02,  4.3798e-02,  4.3747e-02,  4.3695e-02,
         4.3645e-02,  4.3594e-02,  4.3543e-02,  4.3492e-02,  4.3441e-02,
         4.3390e-02,  4.3339e-02,  4.3288e-02,  4.3237e-02,  4.3186e-02,
         4.3135e-02,  4.3084e-02,  4.3033e-02,  4.2982e-02,  4.2930e-02,
         4.2878e-02,  4.2826e-02,  4.2774e-02,  4.2722e-02,  4.2669e-02,
         4.2615e-02,  4.2561e-02,  4.2506e-02,  4.2451e-02,  4.2395e-02,
         4.2338e-02,  4.2280e-02,  4.2220e-02,  4.2160e-02,  4.2098e-02,
         4.2034e-02,  4.1968e-02,  4.1901e-02,  4.1831e-02,  4.1758e-02,
         4.1683e-02,  4.1604e-02,  4.1522e-02,  4.1437e-02,  4.1347e-02,
         3.8931e-02,  3.8685e-02,  3.8425e-02,  3.8147e-02,  3.7853e-02,
         3.7540e-02,  3.7209e-02,  3.6857e-02,  3.6485e-02,  3.6092e-02,
         3.5684e-02,  3.5245e-02,  3.4784e-02,  3.4300e-02,  3.3791e-02,
         3.3258e-02,  3.2700e-02,  3.2117e-02,  3.1511e-02,  3.0874e-02,
         3.0216e-02,  2.9536e-02,  2.8832e-02,  2.8098e-02,  2.7385e-02,
         2.6611e-02,  2.5821e-02,  2.5007e-02,  2.4182e-02,  2.3334e-02,
         2.2479e-02,  2.1605e-02,  2.0710e-02,  1.9817e-02,  1.8927e-02,
         1.8022e-02,  1.7123e-02,  1.6226e-02,  1.5315e-02,  1.4481e-02,
         1.3539e-02,  1.2717e-02,  1.1815e-02,  1.1040e-02,  1.0101e-02,
         9.4015e-03,  8.5252e-03,  7.8562e-03,  6.9907e-03,  6.4037e-03,
         5.6360e-03,  5.0859e-03,  4.4145e-03,  3.9068e-03,  3.4521e-03,
         2.8776e-03,  2.4779e-03,  2.0082e-03,  1.7333e-03,  1.3171e-03,
         1.1570e-03,  7.9981e-04,  6.3341e-04,  4.0468e-04,  3.6538e-04,
         2.6305e-04,  3.4121e-04,  2.3931e-04,  4.4251e-04,  4.6556e-04,
         7.9465e-04,  9.3541e-04,  1.2572e-03,  1.3984e-03,  1.8403e-03,
         2.3952e-03,  2.6490e-03,  3.1918e-03,  3.6650e-03,  4.3102e-03,
         4.7786e-03,  5.4055e-03,  5.9614e-03,  6.7852e-03,  7.3151e-03,
         8.0995e-03,  8.8197e-03,  9.6725e-03,  1.0351e-02,  1.1156e-02,
         1.2128e-02,  1.2750e-02,  1.3532e-02,  1.4535e-02,  1.5264e-02,
         1.6116e-02,  1.6796e-02,  1.7813e-02,  1.8568e-02,  1.9298e-02,
         1.9994e-02,  2.0969e-02,  2.1598e-02,  2.2311e-02,  2.2987e-02,
         2.3783e-02,  2.4481e-02,  2.5047e-02,  2.5671e-02,  2.6377e-02,
         2.6919e-02,  2.7498e-02,  2.7973e-02,  2.8573e-02,  2.9038e-02,
         2.9471e-02,  2.9948e-02,  3.0361e-02,  3.0700e-02,  3.1053e-02,
         3.1433e-02,  3.1727e-02,  3.2026e-02,  3.2271e-02,  3.2560e-02,
         3.2784e-02,  3.2986e-02,  3.3169e-02,  3.3380e-02,  3.3525e-02,
         3.3670e-02,  3.3799e-02,  3.3932e-02,  3.4043e-02,  3.4139e-02,
         3.4237e-02,  3.4308e-02,  3.4369e-02,  3.4420e-02,  3.4472e-02,
         3.4507e-02,  3.4545e-02,  3.4565e-02,  3.4594e-02,  3.4610e-02,
         3.4621e-02,  3.4626e-02,  3.4630e-02,  3.4627e-02,  3.4619e-02,
         3.4609e-02,  3.4598e-02,  3.4585e-02,  3.4567e-02,  3.4550e-02,
         3.4529e-02,  3.4507e-02,  3.4483e-02,  3.4458e-02,  3.4432e-02,
         3.4404e-02,  3.4375e-02,  3.4347e-02,  3.4316e-02,  3.4286e-02,
         3.4254e-02,  3.4223e-02,  3.4191e-02,  3.4158e-02,  3.4125e-02,
         3.4092e-02,  3.4059e-02,  3.4025e-02,  3.3992e-02,  3.3958e-02,
         3.3924e-02,  3.3890e-02,  3.3856e-02,  3.3821e-02,  3.3787e-02,
         3.3753e-02,  3.3719e-02,  3.3685e-02,  3.3650e-02,  3.3616e-02,
         3.3582e-02,  3.3547e-02,  3.3513e-02,  3.3479e-02,  3.3445e-02,
         3.3411e-02,  3.3376e-02,  3.3342e-02,  3.3308e-02,  3.3275e-02,
         3.3241e-02,  3.3207e-02,  3.3173e-02,  3.3139e-02,  3.3105e-02,
         3.3072e-02,  3.3038e-02,  3.3004e-02,  3.2971e-02,  3.2937e-02,
         3.2904e-02,  3.2871e-02,  3.2837e-02,  3.2804e-02,  3.2771e-02,
         3.2738e-02,  3.2704e-02,  3.2671e-02,  3.2638e-02,  3.2605e-02,
         3.2572e-02,  3.2540e-02,  3.2507e-02,  3.2474e-02,  3.2441e-02,
         3.2409e-02,  3.2376e-02,  3.2344e-02,  3.2312e-02,  3.2279e-02,
         3.2247e-02,  3.2215e-02,  3.2183e-02,  3.2151e-02,  3.2119e-02,
         3.2088e-02,  3.2056e-02,  3.2025e-02,  3.1994e-02,  3.1964e-02,
         3.1933e-02,  3.1902e-02,  3.1871e-02,  3.1841e-02,  3.1812e-02,
         3.1783e-02,  3.1755e-02,  3.1726e-02,  3.1699e-02,  3.1669e-02,
         3.1642e-02,  3.1617e-02,  3.1592e-02,  3.1568e-02,  3.1546e-02,
         3.1523e-02,  3.1494e-02,  3.1475e-02,  3.1455e-02,  3.1440e-02,
         3.1423e-02,  3.1412e-02,  3.1399e-02,  3.1389e-02,  3.1366e-02,
         3.1359e-02,  3.1354e-02,  3.1353e-02,  3.1365e-02,  3.1372e-02,
         3.1384e-02,  3.1401e-02,  3.1375e-02,  3.1397e-02,  3.1409e-02,
         3.1442e-02,  3.1482e-02,  3.1528e-02,  3.1560e-02,  3.1538e-02,
         3.1599e-02,  3.1642e-02,  3.1719e-02,  3.1772e-02,  3.1831e-02,
         3.1929e-02,  3.1998e-02,  3.1982e-02,  3.2055e-02,  3.2132e-02,
         3.2210e-02,  3.2290e-02,  3.2368e-02,  3.2442e-02,  3.2510e-02,
         3.2455e-02,  3.2515e-02,  3.2560e-02,  3.2543e-02,  3.2558e-02,
         3.2540e-02,  3.2477e-02,  3.2452e-02,  3.2407e-02,  3.2280e-02,
         3.2074e-02,  3.1763e-02,  3.1628e-02,  3.1132e-02,  3.0442e-02,
         3.1030e-02,  3.0330e-02,  3.0070e-02,  2.9015e-02,  2.8649e-02,
         2.7118e-02,  2.6586e-02,  2.6961e-02,  2.6413e-02,  2.5841e-02,
         2.3390e-02,  2.2558e-02,  2.1670e-02,  1.7835e-02,  1.6540e-02,
         2.0439e-02,  1.9383e-02,  1.4799e-02,  1.3283e-02,  1.1596e-02,
         9.8123e-03,  7.8543e-03,  5.7553e-03,  1.2243e-02,  1.0533e-02,
         8.6543e-03,  6.6176e-03,  4.4081e-03,  1.9838e-03,  6.4674e-04,
         7.4487e-03,  5.3048e-03,  3.0046e-03,  6.4347e-04,  4.5139e-03,
         2.1211e-03,  6.4089e-04,  6.3986e-04,  1.0566e-02,  8.7335e-03,
         6.7466e-03,  9.8779e-03,  7.9901e-03,  5.9681e-03,  9.1803e-03,
         1.4627e-02,  1.6725e-02,  1.5455e-02,  1.7438e-02,  1.6231e-02,
         1.8089e-02,  1.6957e-02,  1.8714e-02,  2.3589e-02,  2.2917e-02,
         2.3924e-02,  2.4814e-02,  2.4245e-02,  2.5070e-02,  2.5808e-02,
         2.6436e-02,  2.8203e-02,  2.7927e-02,  2.8280e-02,  2.8578e-02,
         2.8827e-02,  2.9038e-02,  2.9212e-02,  2.9704e-02,  2.9763e-02,
         2.9803e-02,  2.9831e-02,  2.9843e-02,  2.9845e-02,  2.9833e-02,
         2.9814e-02,  2.9729e-02,  2.9674e-02,  2.9616e-02,  2.9556e-02,
         2.9493e-02,  2.9368e-02,  2.9301e-02,  2.9232e-02,  2.8988e-02,
         2.8919e-02,  2.8787e-02,  2.8720e-02,  2.8655e-02,  2.8530e-02,
         2.8467e-02,  2.8264e-02,  2.8206e-02,  2.8104e-02,  2.8006e-02,
         2.7954e-02,  2.7865e-02,  2.7816e-02,  2.7733e-02,  2.7606e-02,
         2.7537e-02,  2.7496e-02,  2.7472e-02,  2.7372e-02,  2.7285e-02,
         2.7261e-02,  2.7185e-02,  2.7161e-02,  2.7094e-02,  2.7069e-02,
         2.7010e-02,  2.6957e-02,  2.6933e-02,  2.6885e-02,  2.6860e-02,
         2.6817e-02,  2.6793e-02,  2.6752e-02,  2.6715e-02,  2.6691e-02,
         2.6656e-02,  2.6632e-02,  2.6599e-02,  2.6575e-02,  2.6544e-02,
         2.6511e-02,  2.6487e-02,  2.6459e-02,  2.6435e-02,  2.6407e-02,
         2.6383e-02,  2.6355e-02,  2.6331e-02,  2.6305e-02,  2.6280e-02,
         2.6255e-02,  2.6230e-02,  2.6206e-02,  2.6181e-02,  2.6157e-02,
         2.6133e-02,  2.6108e-02,  2.6084e-02,  2.6060e-02,  2.6036e-02,
         2.6013e-02,  2.5989e-02,  2.5965e-02,  2.5942e-02,  2.5918e-02,
         2.5894e-02,  2.5871e-02,  2.5848e-02,  2.5824e-02,  2.5801e-02,
         2.5777e-02,  2.5754e-02,  2.5731e-02,  2.5708e-02,  2.5684e-02,
         2.5661e-02,  2.5638e-02,  2.5615e-02,  2.5592e-02,  2.5569e-02,
         2.5546e-02,  2.5523e-02,  2.5500e-02,  2.5477e-02,  2.5454e-02,
         2.5432e-02,  2.5409e-02,  2.5386e-02,  2.5363e-02,  2.5341e-02,
         2.5318e-02,  2.5295e-02,  2.5273e-02,  2.5250e-02,  2.5228e-02],
       device='cuda:0')
Selected points (indices): {337, 109}
Selected new x values (normalized): tensor([0.3373, 0.1091], device='cuda:0')
Corresponding new x values (unnormalized): tensor([ -650.6506, -1563.5636], device='cuda:0')
Plot saved to /raven/u/dvoss/al_pmssmwithgp/model/plots/Iter3/gp_plot.png
YAML file has been created: /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/data/new_config.yaml
Note: detected 144 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
Note: NumExpr detected 144 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
NumExpr defaulting to 8 threads.

  ___              ____ __  __          _       _   ___            
 | _ \ _  _  _ _  |__ /|  \/  | ___  __| | ___ | | / __| ___  _ _  
 |   /| || || ' \  |_ \| |\/| |/ _ \/ _` |/ -_)| || (_ |/ -_)| ' \ 
 |_|_\ \_,_||_||_||___/|_|  |_|\___/\__,_|\___||_| \___|\___||_||_|
                                                                   

2024-08-28 19:51:48 [info     ] Initialised ModelGenerator with: [Run3ModelGen.modelgen]
2024-08-28 19:51:48 [info     ] 	config_file = /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/data/new_config.yaml [Run3ModelGen.modelgen]
2024-08-28 19:51:48 [info     ] 	scan_dir = /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/scans/scan_3 [Run3ModelGen.modelgen]
2024-08-28 19:51:48 [info     ] 	rawfilen = /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/data/raw.slha [Run3ModelGen.modelgen]
2024-08-28 19:51:48 [info     ] 	seed = 123                    [Run3ModelGen.modelgen]
2024-08-28 19:51:48 [info     ] 	points = {}                   [Run3ModelGen.modelgen]
2024-08-28 19:51:48 [info     ] 	prior = fixed                 [Run3ModelGen.modelgen]
2024-08-28 19:51:48 [info     ] 	num_models = 2                [Run3ModelGen.modelgen]
2024-08-28 19:51:48 [info     ] 	isGMSB = False                [Run3ModelGen.modelgen]
2024-08-28 19:51:48 [info     ] 	parameters =                  [Run3ModelGen.modelgen]
2024-08-28 19:51:48 [info     ] 		tanb: [60, 60]               [Run3ModelGen.modelgen]
2024-08-28 19:51:48 [info     ] 		M_1: [-650.65, -1563.56]     [Run3ModelGen.modelgen]
2024-08-28 19:51:48 [info     ] 		M_2: [2000, 2000]            [Run3ModelGen.modelgen]
2024-08-28 19:51:48 [info     ] 		M_3: [4000, 4000]            [Run3ModelGen.modelgen]
2024-08-28 19:51:48 [info     ] 		AT: [4000, 4000]             [Run3ModelGen.modelgen]
2024-08-28 19:51:48 [info     ] 		Ab: [2000, 2000]             [Run3ModelGen.modelgen]
2024-08-28 19:51:48 [info     ] 		Atau: [2000, 2000]           [Run3ModelGen.modelgen]
2024-08-28 19:51:48 [info     ] 		mu: [2000, 2000]             [Run3ModelGen.modelgen]
2024-08-28 19:51:48 [info     ] 		mA: [2000, 2000]             [Run3ModelGen.modelgen]
2024-08-28 19:51:48 [info     ] 		meL: [2000, 2000]            [Run3ModelGen.modelgen]
2024-08-28 19:51:48 [info     ] 		mtauL: [2000, 2000]          [Run3ModelGen.modelgen]
2024-08-28 19:51:48 [info     ] 		meR: [2000, 2000]            [Run3ModelGen.modelgen]
2024-08-28 19:51:48 [info     ] 		mtauR: [2000, 2000]          [Run3ModelGen.modelgen]
2024-08-28 19:51:48 [info     ] 		mqL1: [4000, 4000]           [Run3ModelGen.modelgen]
2024-08-28 19:51:48 [info     ] 		mqL3: [4000, 4000]           [Run3ModelGen.modelgen]
2024-08-28 19:51:48 [info     ] 		muR: [4000, 4000]            [Run3ModelGen.modelgen]
2024-08-28 19:51:48 [info     ] 		mtR: [4000, 4000]            [Run3ModelGen.modelgen]
2024-08-28 19:51:48 [info     ] 		mdR: [4000, 4000]            [Run3ModelGen.modelgen]
2024-08-28 19:51:48 [info     ] 		mbR: [4000, 4000]            [Run3ModelGen.modelgen]
2024-08-28 19:51:48 [info     ] 	steps =                       [Run3ModelGen.modelgen]
2024-08-28 19:51:48 [info     ] 		{'name': 'prep_input', 'output_dir': 'input', 'prefix': 'IN'} [Run3ModelGen.modelgen]
2024-08-28 19:51:48 [info     ] 		{'name': 'SPheno', 'input_dir': 'input', 'output_dir': 'SPheno', 'log_dir': 'SPheno_log', 'prefix': 'SP'} [Run3ModelGen.modelgen]
2024-08-28 19:51:48 [info     ] 		{'name': 'softsusy', 'input_dir': 'input', 'output_dir': 'softsusy', 'prefix': 'SS'} [Run3ModelGen.modelgen]
2024-08-28 19:51:48 [info     ] 		{'name': 'micromegas', 'input_dir': 'SPheno', 'output_dir': 'micromegas', 'prefix': 'MO'} [Run3ModelGen.modelgen]
2024-08-28 19:51:48 [info     ] 		{'name': 'superiso', 'input_dir': 'SPheno', 'output_dir': 'superiso', 'prefix': 'SI'} [Run3ModelGen.modelgen]
2024-08-28 19:51:48 [info     ] 		{'name': 'gm2calc', 'input_dir': 'SPheno', 'output_dir': 'gm2calc', 'prefix': 'GM2'} [Run3ModelGen.modelgen]
2024-08-28 19:51:48 [info     ] 		{'name': 'evade', 'input_dir': 'SPheno', 'output_dir': 'evade', 'prefix': 'EV'} [Run3ModelGen.modelgen]
2024-08-28 19:51:48 [info     ] Starting Model Generation...   [Run3ModelGen.modelgen]
2024-08-28 19:51:48 [info     ] Setting up output directories for scan [Run3ModelGen.modelgen]
2024-08-28 19:51:48 [info     ] Generating Model: 0.slha (1/2) [Run3ModelGen.modelgen]
2024-08-28 19:51:48 [info     ] 	running step: prep_input      [Run3ModelGen.modelgen]
2024-08-28 19:51:48 [info     ] 	running step: SPheno          [Run3ModelGen.modelgen]
2024-08-28 19:51:48 [info     ] 	running step: softsusy        [Run3ModelGen.modelgen]
2024-08-28 19:51:48 [info     ] 	running step: micromegas      [Run3ModelGen.modelgen]
2024-08-28 19:51:48 [info     ] 	running step: superiso        [Run3ModelGen.modelgen]
2024-08-28 19:51:49 [info     ] 	running step: gm2calc         [Run3ModelGen.modelgen]
2024-08-28 19:51:49 [info     ] 	running step: evade           [Run3ModelGen.modelgen]
2024-08-28 19:51:49 [info     ] Generating Model: 1.slha (2/2) [Run3ModelGen.modelgen]
2024-08-28 19:51:49 [info     ] 	running step: prep_input      [Run3ModelGen.modelgen]
2024-08-28 19:51:49 [info     ] 	running step: SPheno          [Run3ModelGen.modelgen]
2024-08-28 19:51:49 [info     ] 	running step: softsusy        [Run3ModelGen.modelgen]
2024-08-28 19:51:49 [info     ] 	running step: micromegas      [Run3ModelGen.modelgen]
2024-08-28 19:51:49 [info     ] 	running step: superiso        [Run3ModelGen.modelgen]
2024-08-28 19:51:50 [info     ] 	running step: gm2calc         [Run3ModelGen.modelgen]
2024-08-28 19:51:50 [info     ] 	running step: evade           [Run3ModelGen.modelgen]
2024-08-28 19:51:50 [info     ] Finished generating models! Stats for successful generations (successful/attempted): [Run3ModelGen.modelgen]
2024-08-28 19:51:50 [info     ] 	prep_input: 2/2.              [Run3ModelGen.modelgen]
2024-08-28 19:51:50 [info     ] 	SPheno: 2/2.                  [Run3ModelGen.modelgen]
2024-08-28 19:51:50 [info     ] 	softsusy: 2/2.                [Run3ModelGen.modelgen]
2024-08-28 19:51:50 [info     ] 	micromegas: 2/2.              [Run3ModelGen.modelgen]
2024-08-28 19:51:50 [info     ] 	superiso: 2/2.                [Run3ModelGen.modelgen]
2024-08-28 19:51:50 [info     ] 	gm2calc: 2/2.                 [Run3ModelGen.modelgen]
2024-08-28 19:51:50 [info     ] 	evade: 0/2.                   [Run3ModelGen.modelgen]
2024-08-28 19:51:50 [info     ] Will make NTuple for: /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/scans/scan_3, with 2 models [Run3ModelGen.ntupling]
2024-08-28 19:51:50 [info     ] 	Filling values for model: 0   [Run3ModelGen.ntupling]
2024-08-28 19:51:50 [info     ] Saving NTuple in /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/scans/scan_3/ntuple.0.0.root ... [Run3ModelGen.ntupling]
Completed iteration 3
Starting iteration 4
Initial Training points:  tensor([0.8528, 0.5812, 0.1617, 0.0046], device='cuda:0') torch.Size([4])
Training points after adding:  tensor([0.1617, 0.1091, 0.0046, 0.2132, 0.0561, 0.3373, 0.2392, 0.5812, 0.0210,
        0.8528], device='cuda:0') torch.Size([10])
These training_points are used in the GP tensor([0.1617, 0.1091, 0.0046, 0.2132, 0.0561, 0.3373, 0.2392, 0.5812, 0.0210,
        0.8528], device='cuda:0')
Point: tensor([0.0000, 0.0010, 0.0020, 0.0030, 0.0040, 0.0050, 0.0060, 0.0070, 0.0080,
        0.0090, 0.0100, 0.0110, 0.0120, 0.0130, 0.0140, 0.0150, 0.0160, 0.0170,
        0.0180, 0.0190, 0.0200, 0.0210, 0.0220, 0.0230, 0.0240, 0.0250, 0.0260,
        0.0270, 0.0280, 0.0290, 0.0300, 0.0310, 0.0320, 0.0330, 0.0340, 0.0350,
        0.0360, 0.0370, 0.0380, 0.0390, 0.0400, 0.0410, 0.0420, 0.0430, 0.0440,
        0.0450, 0.0460, 0.0470, 0.0480, 0.0490, 0.0501, 0.0511, 0.0521, 0.0531,
        0.0541, 0.0551, 0.0561, 0.0571, 0.0581, 0.0591, 0.0601, 0.0611, 0.0621,
        0.0631, 0.0641, 0.0651, 0.0661, 0.0671, 0.0681, 0.0691, 0.0701, 0.0711,
        0.0721, 0.0731, 0.0741, 0.0751, 0.0761, 0.0771, 0.0781, 0.0791, 0.0801,
        0.0811, 0.0821, 0.0831, 0.0841, 0.0851, 0.0861, 0.0871, 0.0881, 0.0891,
        0.0901, 0.0911, 0.0921, 0.0931, 0.0941, 0.0951, 0.0961, 0.0971, 0.0981,
        0.0991, 0.1001, 0.1011, 0.1021, 0.1031, 0.1041, 0.1051, 0.1061, 0.1071,
        0.1081, 0.1091, 0.1101, 0.1111, 0.1121, 0.1131, 0.1141, 0.1151, 0.1161,
        0.1171, 0.1181, 0.1191, 0.1201, 0.1211, 0.1221, 0.1231, 0.1241, 0.1251,
        0.1261, 0.1271, 0.1281, 0.1291, 0.1301, 0.1311, 0.1321, 0.1331, 0.1341,
        0.1351, 0.1361, 0.1371, 0.1381, 0.1391, 0.1401, 0.1411, 0.1421, 0.1431,
        0.1441, 0.1451, 0.1461, 0.1471, 0.1481, 0.1491, 0.1502, 0.1512, 0.1522,
        0.1532, 0.1542, 0.1552, 0.1562, 0.1572, 0.1582, 0.1592, 0.1602, 0.1612,
        0.1622, 0.1632, 0.1642, 0.1652, 0.1662, 0.1672, 0.1682, 0.1692, 0.1702,
        0.1712, 0.1722, 0.1732, 0.1742, 0.1752, 0.1762, 0.1772, 0.1782, 0.1792,
        0.1802, 0.1812, 0.1822, 0.1832, 0.1842, 0.1852, 0.1862, 0.1872, 0.1882,
        0.1892, 0.1902, 0.1912, 0.1922, 0.1932, 0.1942, 0.1952, 0.1962, 0.1972,
        0.1982, 0.1992, 0.2002, 0.2012, 0.2022, 0.2032, 0.2042, 0.2052, 0.2062,
        0.2072, 0.2082, 0.2092, 0.2102, 0.2112, 0.2122, 0.2132, 0.2142, 0.2152,
        0.2162, 0.2172, 0.2182, 0.2192, 0.2202, 0.2212, 0.2222, 0.2232, 0.2242,
        0.2252, 0.2262, 0.2272, 0.2282, 0.2292, 0.2302, 0.2312, 0.2322, 0.2332,
        0.2342, 0.2352, 0.2362, 0.2372, 0.2382, 0.2392, 0.2402, 0.2412, 0.2422,
        0.2432, 0.2442, 0.2452, 0.2462, 0.2472, 0.2482, 0.2492, 0.2503, 0.2513,
        0.2523, 0.2533, 0.2543, 0.2553, 0.2563, 0.2573, 0.2583, 0.2593, 0.2603,
        0.2613, 0.2623, 0.2633, 0.2643, 0.2653, 0.2663, 0.2673, 0.2683, 0.2693,
        0.2703, 0.2713, 0.2723, 0.2733, 0.2743, 0.2753, 0.2763, 0.2773, 0.2783,
        0.2793, 0.2803, 0.2813, 0.2823, 0.2833, 0.2843, 0.2853, 0.2863, 0.2873,
        0.2883, 0.2893, 0.2903, 0.2913, 0.2923, 0.2933, 0.2943, 0.2953, 0.2963,
        0.2973, 0.2983, 0.2993, 0.3003, 0.3013, 0.3023, 0.3033, 0.3043, 0.3053,
        0.3063, 0.3073, 0.3083, 0.3093, 0.3103, 0.3113, 0.3123, 0.3133, 0.3143,
        0.3153, 0.3163, 0.3173, 0.3183, 0.3193, 0.3203, 0.3213, 0.3223, 0.3233,
        0.3243, 0.3253, 0.3263, 0.3273, 0.3283, 0.3293, 0.3303, 0.3313, 0.3323,
        0.3333, 0.3343, 0.3353, 0.3363, 0.3373, 0.3383, 0.3393, 0.3403, 0.3413,
        0.3423, 0.3433, 0.3443, 0.3453, 0.3463, 0.3473, 0.3483, 0.3493, 0.3504,
        0.3514, 0.3524, 0.3534, 0.3544, 0.3554, 0.3564, 0.3574, 0.3584, 0.3594,
        0.3604, 0.3614, 0.3624, 0.3634, 0.3644, 0.3654, 0.3664, 0.3674, 0.3684,
        0.3694, 0.3704, 0.3714, 0.3724, 0.3734, 0.3744, 0.3754, 0.3764, 0.3774,
        0.3784, 0.3794, 0.3804, 0.3814, 0.3824, 0.3834, 0.3844, 0.3854, 0.3864,
        0.3874, 0.3884, 0.3894, 0.3904, 0.3914, 0.3924, 0.3934, 0.3944, 0.3954,
        0.3964, 0.3974, 0.3984, 0.3994, 0.4004, 0.4014, 0.4024, 0.4034, 0.4044,
        0.4054, 0.4064, 0.4074, 0.4084, 0.4094, 0.4104, 0.4114, 0.4124, 0.4134,
        0.4144, 0.4154, 0.4164, 0.4174, 0.4184, 0.4194, 0.4204, 0.4214, 0.4224,
        0.4234, 0.4244, 0.4254, 0.4264, 0.4274, 0.4284, 0.4294, 0.4304, 0.4314,
        0.4324, 0.4334, 0.4344, 0.4354, 0.4364, 0.4374, 0.4384, 0.4394, 0.4404,
        0.4414, 0.4424, 0.4434, 0.4444, 0.4454, 0.4464, 0.4474, 0.4484, 0.4494,
        0.4505, 0.4515, 0.4525, 0.4535, 0.4545, 0.4555, 0.4565, 0.4575, 0.4585,
        0.4595, 0.4605, 0.4615, 0.4625, 0.4635, 0.4645, 0.4655, 0.4665, 0.4675,
        0.4685, 0.4695, 0.4705, 0.4715, 0.4725, 0.4735, 0.4745, 0.4755, 0.4765,
        0.4775, 0.4785, 0.4795, 0.4805, 0.4815, 0.4825, 0.4835, 0.4845, 0.4855,
        0.4865, 0.4875, 0.4885, 0.4895, 0.4905, 0.4915, 0.4925, 0.4935, 0.4945,
        0.4955, 0.4965, 0.4975, 0.4985, 0.4995, 0.5005, 0.5015, 0.5025, 0.5035,
        0.5045, 0.5055, 0.5065, 0.5075, 0.5085, 0.5095, 0.5105, 0.5115, 0.5125,
        0.5135, 0.5145, 0.5155, 0.5165, 0.5175, 0.5185, 0.5195, 0.5205, 0.5215,
        0.5225, 0.5235, 0.5245, 0.5255, 0.5265, 0.5275, 0.5285, 0.5295, 0.5305,
        0.5315, 0.5325, 0.5335, 0.5345, 0.5355, 0.5365, 0.5375, 0.5385, 0.5395,
        0.5405, 0.5415, 0.5425, 0.5435, 0.5445, 0.5455, 0.5465, 0.5475, 0.5485,
        0.5495, 0.5506, 0.5516, 0.5526, 0.5536, 0.5546, 0.5556, 0.5566, 0.5576,
        0.5586, 0.5596, 0.5606, 0.5616, 0.5626, 0.5636, 0.5646, 0.5656, 0.5666,
        0.5676, 0.5686, 0.5696, 0.5706, 0.5716, 0.5726, 0.5736, 0.5746, 0.5756,
        0.5766, 0.5776, 0.5786, 0.5796, 0.5806, 0.5816, 0.5826, 0.5836, 0.5846,
        0.5856, 0.5866, 0.5876, 0.5886, 0.5896, 0.5906, 0.5916, 0.5926, 0.5936,
        0.5946, 0.5956, 0.5966, 0.5976, 0.5986, 0.5996, 0.6006, 0.6016, 0.6026,
        0.6036, 0.6046, 0.6056, 0.6066, 0.6076, 0.6086, 0.6096, 0.6106, 0.6116,
        0.6126, 0.6136, 0.6146, 0.6156, 0.6166, 0.6176, 0.6186, 0.6196, 0.6206,
        0.6216, 0.6226, 0.6236, 0.6246, 0.6256, 0.6266, 0.6276, 0.6286, 0.6296,
        0.6306, 0.6316, 0.6326, 0.6336, 0.6346, 0.6356, 0.6366, 0.6376, 0.6386,
        0.6396, 0.6406, 0.6416, 0.6426, 0.6436, 0.6446, 0.6456, 0.6466, 0.6476,
        0.6486, 0.6496, 0.6507, 0.6517, 0.6527, 0.6537, 0.6547, 0.6557, 0.6567,
        0.6577, 0.6587, 0.6597, 0.6607, 0.6617, 0.6627, 0.6637, 0.6647, 0.6657,
        0.6667, 0.6677, 0.6687, 0.6697, 0.6707, 0.6717, 0.6727, 0.6737, 0.6747,
        0.6757, 0.6767, 0.6777, 0.6787, 0.6797, 0.6807, 0.6817, 0.6827, 0.6837,
        0.6847, 0.6857, 0.6867, 0.6877, 0.6887, 0.6897, 0.6907, 0.6917, 0.6927,
        0.6937, 0.6947, 0.6957, 0.6967, 0.6977, 0.6987, 0.6997, 0.7007, 0.7017,
        0.7027, 0.7037, 0.7047, 0.7057, 0.7067, 0.7077, 0.7087, 0.7097, 0.7107,
        0.7117, 0.7127, 0.7137, 0.7147, 0.7157, 0.7167, 0.7177, 0.7187, 0.7197,
        0.7207, 0.7217, 0.7227, 0.7237, 0.7247, 0.7257, 0.7267, 0.7277, 0.7287,
        0.7297, 0.7307, 0.7317, 0.7327, 0.7337, 0.7347, 0.7357, 0.7367, 0.7377,
        0.7387, 0.7397, 0.7407, 0.7417, 0.7427, 0.7437, 0.7447, 0.7457, 0.7467,
        0.7477, 0.7487, 0.7497, 0.7508, 0.7518, 0.7528, 0.7538, 0.7548, 0.7558,
        0.7568, 0.7578, 0.7588, 0.7598, 0.7608, 0.7618, 0.7628, 0.7638, 0.7648,
        0.7658, 0.7668, 0.7678, 0.7688, 0.7698, 0.7708, 0.7718, 0.7728, 0.7738,
        0.7748, 0.7758, 0.7768, 0.7778, 0.7788, 0.7798, 0.7808, 0.7818, 0.7828,
        0.7838, 0.7848, 0.7858, 0.7868, 0.7878, 0.7888, 0.7898, 0.7908, 0.7918,
        0.7928, 0.7938, 0.7948, 0.7958, 0.7968, 0.7978, 0.7988, 0.7998, 0.8008,
        0.8018, 0.8028, 0.8038, 0.8048, 0.8058, 0.8068, 0.8078, 0.8088, 0.8098,
        0.8108, 0.8118, 0.8128, 0.8138, 0.8148, 0.8158, 0.8168, 0.8178, 0.8188,
        0.8198, 0.8208, 0.8218, 0.8228, 0.8238, 0.8248, 0.8258, 0.8268, 0.8278,
        0.8288, 0.8298, 0.8308, 0.8318, 0.8328, 0.8338, 0.8348, 0.8358, 0.8368,
        0.8378, 0.8388, 0.8398, 0.8408, 0.8418, 0.8428, 0.8438, 0.8448, 0.8458,
        0.8468, 0.8478, 0.8488, 0.8498, 0.8509, 0.8519, 0.8529, 0.8539, 0.8549,
        0.8559, 0.8569, 0.8579, 0.8589, 0.8599, 0.8609, 0.8619, 0.8629, 0.8639,
        0.8649, 0.8659, 0.8669, 0.8679, 0.8689, 0.8699, 0.8709, 0.8719, 0.8729,
        0.8739, 0.8749, 0.8759, 0.8769, 0.8779, 0.8789, 0.8799, 0.8809, 0.8819,
        0.8829, 0.8839, 0.8849, 0.8859, 0.8869, 0.8879, 0.8889, 0.8899, 0.8909,
        0.8919, 0.8929, 0.8939, 0.8949, 0.8959, 0.8969, 0.8979, 0.8989, 0.8999,
        0.9009, 0.9019, 0.9029, 0.9039, 0.9049, 0.9059, 0.9069, 0.9079, 0.9089,
        0.9099, 0.9109, 0.9119, 0.9129, 0.9139, 0.9149, 0.9159, 0.9169, 0.9179,
        0.9189, 0.9199, 0.9209, 0.9219, 0.9229, 0.9239, 0.9249, 0.9259, 0.9269,
        0.9279, 0.9289, 0.9299, 0.9309, 0.9319, 0.9329, 0.9339, 0.9349, 0.9359,
        0.9369, 0.9379, 0.9389, 0.9399, 0.9409, 0.9419, 0.9429, 0.9439, 0.9449,
        0.9459, 0.9469, 0.9479, 0.9489, 0.9499, 0.9510, 0.9520, 0.9530, 0.9540,
        0.9550, 0.9560, 0.9570, 0.9580, 0.9590, 0.9600, 0.9610, 0.9620, 0.9630,
        0.9640, 0.9650, 0.9660, 0.9670, 0.9680, 0.9690, 0.9700, 0.9710, 0.9720,
        0.9730, 0.9740, 0.9750, 0.9760, 0.9770, 0.9780, 0.9790, 0.9800, 0.9810,
        0.9820, 0.9830, 0.9840, 0.9850, 0.9860, 0.9870, 0.9880, 0.9890, 0.9900,
        0.9910, 0.9920, 0.9930, 0.9940, 0.9950, 0.9960, 0.9970, 0.9980, 0.9990,
        1.0000], device='cuda:0') - Variance: tensor([[1.0000e-06],
        [1.0000e-06],
        [1.0000e-06],
        [1.0851e-02],
        [8.5892e-03],
        [1.0000e-06],
        [1.9979e-03],
        [1.0000e-06],
        [1.0000e-06],
        [1.0282e-02],
        [1.0000e-06],
        [1.0000e-06],
        [3.2035e-02],
        [1.0000e-06],
        [1.0000e-06],
        [1.0000e-06],
        [1.0000e-06],
        [1.3895e-01],
        [1.0000e-06],
        [1.0000e-06],
        [1.0000e-06],
        [8.3942e-02],
        [1.6802e-01],
        [1.0000e-06],
        [2.1994e-01],
        [1.0000e-06],
        [4.4252e-02],
        [3.0826e-01],
        [7.4239e-02],
        [3.2307e-01],
        [8.7669e-02],
        [1.3263e-01],
        [3.5480e-01],
        [1.2014e-01],
        [3.3810e-01],
        [9.0548e-02],
        [3.0789e-01],
        [4.1800e-02],
        [2.6222e-01],
        [4.2828e-01],
        [1.9832e-01],
        [3.7157e-01],
        [1.1249e-01],
        [2.9660e-01],
        [4.3932e-01],
        [1.9927e-01],
        [3.5550e-01],
        [7.4419e-02],
        [2.4856e-01],
        [3.8738e-01],
        [1.1303e-01],
        [2.7195e-01],
        [1.0000e-06],
        [1.2675e-01],
        [4.3839e-01],
        [1.7934e-01],
        [3.1815e-01],
        [9.2095e-03],
        [1.6773e-01],
        [5.0907e-02],
        [2.0649e-01],
        [3.3569e-01],
        [2.2753e-02],
        [3.6229e-01],
        [6.0143e-02],
        [2.0880e-01],
        [3.3281e-01],
        [2.3293e-01],
        [3.5071e-01],
        [3.7084e-02],
        [3.6314e-01],
        [4.5636e-01],
        [3.6883e-01],
        [4.5705e-01],
        [1.8423e-01],
        [4.4911e-01],
        [5.1806e-01],
        [4.2992e-01],
        [4.9433e-01],
        [3.9594e-01],
        [4.5557e-01],
        [3.4245e-01],
        [3.9697e-01],
        [5.6595e-01],
        [3.1212e-01],
        [4.9591e-01],
        [1.9256e-01],
        [3.9834e-01],
        [5.4805e-01],
        [2.6435e-01],
        [4.3996e-01],
        [8.2207e-02],
        [2.9369e-01],
        [4.5396e-01],
        [9.6915e-02],
        [2.9671e-01],
        [9.2674e-02],
        [8.6539e-02],
        [2.8338e-01],
        [7.1659e-02],
        [2.6953e-01],
        [1.0000e-06],
        [3.8785e-02],
        [3.7598e-03],
        [1.4057e-02],
        [9.0782e-03],
        [1.0403e-01],
        [9.1178e-02],
        [1.1269e-02],
        [1.1162e-02],
        [1.0717e-02],
        [9.8966e-03],
        [1.6505e-02],
        [7.2018e-03],
        [5.0933e-03],
        [2.3207e-03],
        [1.0000e-06],
        [1.6259e-01],
        [1.4579e-01],
        [1.2764e-01],
        [1.0788e-01],
        [8.6183e-02],
        [6.2150e-02],
        [2.4546e-01],
        [2.2132e-01],
        [2.8926e-01],
        [2.6152e-01],
        [2.2953e-01],
        [3.6597e-01],
        [3.3119e-01],
        [3.7444e-01],
        [3.3204e-01],
        [2.8193e-01],
        [3.1457e-01],
        [4.0821e-01],
        [4.2566e-01],
        [3.6275e-01],
        [3.7199e-01],
        [2.9272e-01],
        [4.3597e-01],
        [3.5506e-01],
        [3.4596e-01],
        [2.4345e-01],
        [2.2457e-01],
        [2.0161e-01],
        [2.4845e-01],
        [2.1984e-01],
        [1.8774e-01],
        [1.5217e-01],
        [1.0000e-06],
        [1.5144e-01],
        [1.0905e-01],
        [6.3347e-02],
        [1.4262e-02],
        [1.0000e-06],
        [1.0755e-03],
        [6.9363e-02],
        [1.7736e-02],
        [8.5639e-03],
        [9.7525e-03],
        [1.0608e-02],
        [1.3059e-02],
        [1.1248e-02],
        [1.0891e-02],
        [9.8942e-03],
        [8.9136e-03],
        [6.7942e-02],
        [1.2091e-02],
        [1.6163e-03],
        [1.9125e-02],
        [1.0000e-06],
        [1.0000e-06],
        [1.5875e-01],
        [1.0205e-01],
        [1.5318e-01],
        [9.2266e-02],
        [1.3973e-01],
        [2.4909e-01],
        [2.8323e-01],
        [2.2087e-01],
        [2.4996e-01],
        [2.7440e-01],
        [1.9952e-01],
        [3.6218e-01],
        [3.7133e-01],
        [2.9246e-01],
        [2.9322e-01],
        [2.8909e-01],
        [4.0952e-01],
        [3.9701e-01],
        [2.9686e-01],
        [2.7264e-01],
        [2.4309e-01],
        [2.0796e-01],
        [3.1169e-01],
        [2.7201e-01],
        [2.2689e-01],
        [1.7598e-01],
        [1.1887e-01],
        [2.1629e-01],
        [2.5673e-01],
        [1.9873e-01],
        [1.3455e-01],
        [6.3658e-02],
        [1.0000e-06],
        [1.9287e-01],
        [1.2266e-01],
        [4.5141e-02],
        [8.2406e-02],
        [1.0000e-06],
        [8.9208e-02],
        [1.2089e-01],
        [3.6244e-02],
        [6.6718e-02],
        [8.7003e-03],
        [6.3578e-03],
        [7.9733e-02],
        [1.0077e-01],
        [9.1670e-04],
        [1.8319e-02],
        [3.2444e-02],
        [1.4721e-01],
        [9.8217e-02],
        [1.0127e-01],
        [4.2579e-02],
        [3.8493e-02],
        [1.0000e-06],
        [1.2120e-01],
        [1.0625e-01],
        [2.8305e-02],
        [4.2182e-03],
        [1.0000e-06],
        [1.0762e-01],
        [7.5369e-02],
        [3.9059e-02],
        [1.0000e-06],
        [1.0000e-06],
        [5.3848e-03],
        [8.5141e-02],
        [3.7614e-02],
        [4.7740e-02],
        [8.3698e-03],
        [7.1922e-03],
        [1.0562e-01],
        [5.2933e-02],
        [5.7545e-02],
        [6.1536e-02],
        [4.3564e-03],
        [1.5784e-01],
        [1.6030e-01],
        [1.6255e-01],
        [1.6466e-01],
        [1.6668e-01],
        [1.6866e-01],
        [2.9620e-01],
        [2.9790e-01],
        [2.9965e-01],
        [3.4413e-01],
        [3.4592e-01],
        [4.4678e-01],
        [4.8223e-01],
        [4.8390e-01],
        [5.1716e-01],
        [5.1889e-01],
        [5.5008e-01],
        [6.4358e-01],
        [6.4506e-01],
        [6.6834e-01],
        [6.9015e-01],
        [7.1058e-01],
        [7.7130e-01],
        [7.8648e-01],
        [8.0066e-01],
        [8.1387e-01],
        [8.2617e-01],
        [8.3758e-01],
        [8.7180e-01],
        [8.8762e-01],
        [8.9466e-01],
        [9.0102e-01],
        [9.1267e-01],
        [9.3033e-01],
        [9.3792e-01],
        [9.4424e-01],
        [9.4577e-01],
        [9.5007e-01],
        [9.5325e-01],
        [9.6204e-01],
        [9.5993e-01],
        [9.5958e-01],
        [9.5813e-01],
        [9.5550e-01],
        [9.5768e-01],
        [9.5594e-01],
        [9.4992e-01],
        [9.4233e-01],
        [9.3293e-01],
        [9.2652e-01],
        [9.2269e-01],
        [9.1424e-01],
        [8.9805e-01],
        [8.8635e-01],
        [8.7168e-01],
        [8.5656e-01],
        [8.3428e-01],
        [8.2418e-01],
        [7.9676e-01],
        [7.8419e-01],
        [7.5838e-01],
        [7.2950e-01],
        [7.1265e-01],
        [6.7834e-01],
        [6.3997e-01],
        [6.2949e-01],
        [5.8534e-01],
        [5.7326e-01],
        [5.3735e-01],
        [4.8235e-01],
        [4.6728e-01],
        [4.2255e-01],
        [3.7410e-01],
        [3.7588e-01],
        [3.2355e-01],
        [2.6686e-01],
        [2.6894e-01],
        [2.3226e-01],
        [2.0995e-01],
        [1.7034e-01],
        [1.2876e-01],
        [1.3122e-01],
        [8.7684e-02],
        [7.1634e-02],
        [7.4253e-02],
        [2.7884e-02],
        [1.1274e-02],
        [4.4100e-02],
        [2.7288e-02],
        [3.0031e-02],
        [1.2971e-02],
        [2.6690e-02],
        [5.9470e-02],
        [4.2926e-02],
        [2.6092e-02],
        [8.8021e-02],
        [1.0070e-01],
        [8.4879e-02],
        [1.4309e-01],
        [1.5501e-01],
        [2.0879e-01],
        [2.1980e-01],
        [2.5451e-01],
        [3.0199e-01],
        [3.1172e-01],
        [3.4236e-01],
        [4.0341e-01],
        [4.1174e-01],
        [4.6640e-01],
        [4.9022e-01],
        [5.1298e-01],
        [5.7209e-01],
        [5.9124e-01],
        [6.1568e-01],
        [6.5706e-01],
        [6.7762e-01],
        [7.0175e-01],
        [7.3400e-01],
        [7.5399e-01],
        [7.8418e-01],
        [8.0048e-01],
        [8.1852e-01],
        [8.4352e-01],
        [8.5778e-01],
        [8.7078e-01],
        [8.8880e-01],
        [9.0073e-01],
        [9.0996e-01],
        [9.2403e-01],
        [9.3350e-01],
        [9.4411e-01],
        [9.5125e-01],
        [9.5680e-01],
        [9.6463e-01],
        [9.6995e-01],
        [9.7409e-01],
        [9.7956e-01],
        [9.8295e-01],
        [9.8589e-01],
        [9.8932e-01],
        [9.9165e-01],
        [9.9408e-01],
        [9.9574e-01],
        [9.9716e-01],
        [9.9885e-01],
        [9.9982e-01],
        [1.0007e+00],
        [1.0017e+00],
        [1.0023e+00],
        [1.0030e+00],
        [1.0034e+00],
        [1.0038e+00],
        [1.0042e+00],
        [1.0044e+00],
        [1.0047e+00],
        [1.0049e+00],
        [1.0050e+00],
        [1.0051e+00],
        [1.0053e+00],
        [1.0053e+00],
        [1.0054e+00],
        [1.0055e+00],
        [1.0055e+00],
        [1.0055e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0055e+00],
        [1.0055e+00],
        [1.0055e+00],
        [1.0054e+00],
        [1.0054e+00],
        [1.0053e+00],
        [1.0052e+00],
        [1.0051e+00],
        [1.0050e+00],
        [1.0048e+00],
        [1.0046e+00],
        [1.0044e+00],
        [1.0041e+00],
        [1.0037e+00],
        [1.0033e+00],
        [1.0027e+00],
        [1.0021e+00],
        [1.0014e+00],
        [1.0005e+00],
        [9.9948e-01],
        [9.9822e-01],
        [9.9681e-01],
        [9.9509e-01],
        [9.9315e-01],
        [9.9083e-01],
        [9.8823e-01],
        [9.8510e-01],
        [9.8166e-01],
        [9.7773e-01],
        [9.7307e-01],
        [9.6796e-01],
        [9.6193e-01],
        [9.5545e-01],
        [9.4788e-01],
        [9.3971e-01],
        [9.3020e-01],
        [9.2004e-01],
        [9.0826e-01],
        [8.9601e-01],
        [8.8190e-01],
        [8.6714e-01],
        [8.5122e-01],
        [8.3304e-01],
        [8.1396e-01],
        [7.9264e-01],
        [7.6988e-01],
        [7.4903e-01],
        [7.2271e-01],
        [6.9613e-01],
        [6.6705e-01],
        [6.4138e-01],
        [6.1026e-01],
        [5.7648e-01],
        [5.4344e-01],
        [5.1419e-01],
        [4.7842e-01],
        [4.4444e-01],
        [4.1358e-01],
        [3.7788e-01],
        [3.4263e-01],
        [3.0539e-01],
        [2.7834e-01],
        [2.4347e-01],
        [2.1004e-01],
        [1.7839e-01],
        [1.5975e-01],
        [1.2952e-01],
        [1.0528e-01],
        [9.2156e-02],
        [6.6876e-02],
        [4.8404e-02],
        [2.9569e-02],
        [3.0700e-02],
        [1.9253e-02],
        [1.5438e-02],
        [1.1608e-02],
        [2.0486e-02],
        [2.4372e-02],
        [2.8242e-02],
        [3.9674e-02],
        [6.3144e-02],
        [8.1416e-02],
        [9.9334e-02],
        [1.2824e-01],
        [1.5194e-01],
        [1.7501e-01],
        [2.0374e-01],
        [2.4135e-01],
        [2.6779e-01],
        [2.9885e-01],
        [3.2861e-01],
        [3.7037e-01],
        [4.0185e-01],
        [4.4350e-01],
        [4.6867e-01],
        [5.0571e-01],
        [5.4019e-01],
        [5.6793e-01],
        [6.0444e-01],
        [6.2834e-01],
        [6.5982e-01],
        [6.9358e-01],
        [7.1217e-01],
        [7.4082e-01],
        [7.6668e-01],
        [7.8783e-01],
        [8.0909e-01],
        [8.2649e-01],
        [8.4649e-01],
        [8.6425e-01],
        [8.7677e-01],
        [8.9293e-01],
        [9.0551e-01],
        [9.1579e-01],
        [9.2828e-01],
        [9.3798e-01],
        [9.4587e-01],
        [9.5336e-01],
        [9.5946e-01],
        [9.6649e-01],
        [9.7193e-01],
        [9.7633e-01],
        [9.8079e-01],
        [9.8458e-01],
        [9.8732e-01],
        [9.9011e-01],
        [9.9269e-01],
        [9.9455e-01],
        [9.9639e-01],
        [9.9784e-01],
        [9.9913e-01],
        [1.0003e+00],
        [1.0011e+00],
        [1.0019e+00],
        [1.0027e+00],
        [1.0031e+00],
        [1.0036e+00],
        [1.0040e+00],
        [1.0043e+00],
        [1.0046e+00],
        [1.0047e+00],
        [1.0049e+00],
        [1.0051e+00],
        [1.0052e+00],
        [1.0053e+00],
        [1.0054e+00],
        [1.0054e+00],
        [1.0055e+00],
        [1.0055e+00],
        [1.0055e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0055e+00],
        [1.0055e+00],
        [1.0054e+00],
        [1.0054e+00],
        [1.0054e+00],
        [1.0053e+00],
        [1.0051e+00],
        [1.0050e+00],
        [1.0049e+00],
        [1.0047e+00],
        [1.0046e+00],
        [1.0043e+00],
        [1.0037e+00],
        [1.0035e+00],
        [1.0029e+00],
        [1.0026e+00],
        [1.0022e+00],
        [1.0002e+00],
        [9.9953e-01],
        [9.9878e-01],
        [9.9794e-01],
        [9.9700e-01],
        [9.9465e-01],
        [9.9022e-01],
        [9.8834e-01],
        [9.8623e-01],
        [9.8386e-01],
        [9.8120e-01],
        [9.7136e-01],
        [9.6718e-01],
        [9.6249e-01],
        [9.5722e-01],
        [9.5770e-01],
        [9.5185e-01],
        [9.3020e-01],
        [9.2099e-01],
        [9.1067e-01],
        [9.1160e-01],
        [9.0013e-01],
        [8.5767e-01],
        [8.5913e-01],
        [8.4125e-01],
        [8.2119e-01],
        [8.2301e-01],
        [8.0073e-01],
        [7.5205e-01],
        [7.2111e-01],
        [7.2391e-01],
        [6.8953e-01],
        [6.9264e-01],
        [5.6671e-01],
        [5.7103e-01],
        [5.7530e-01],
        [5.2280e-01],
        [5.2755e-01],
        [5.3226e-01],
        [4.1980e-01],
        [3.4833e-01],
        [3.5480e-01],
        [3.6120e-01],
        [3.6754e-01],
        [2.1596e-01],
        [2.2373e-01],
        [2.3142e-01],
        [2.3904e-01],
        [2.4658e-01],
        [6.6260e-02],
        [7.5503e-02],
        [8.4654e-02],
        [9.3716e-02],
        [1.0269e-01],
        [1.1157e-01],
        [1.1274e-02],
        [3.8804e-02],
        [4.8317e-02],
        [5.7736e-02],
        [6.7062e-02],
        [1.1274e-02],
        [1.1274e-02],
        [1.1274e-02],
        [1.2747e-01],
        [1.3611e-01],
        [2.4584e-01],
        [6.5342e-02],
        [1.8400e-01],
        [1.9208e-01],
        [2.9474e-01],
        [3.0174e-01],
        [2.3688e-01],
        [2.4444e-01],
        [3.4050e-01],
        [4.2443e-01],
        [4.3015e-01],
        [5.0277e-01],
        [4.5644e-01],
        [5.2574e-01],
        [5.8630e-01],
        [5.9042e-01],
        [6.4282e-01],
        [6.0939e-01],
        [6.5939e-01],
        [7.0309e-01],
        [7.4127e-01],
        [7.7463e-01],
        [8.0378e-01],
        [7.8518e-01],
        [8.1300e-01],
        [8.3731e-01],
        [8.5855e-01],
        [8.7712e-01],
        [8.8177e-01],
        [8.9740e-01],
        [9.1106e-01],
        [9.2300e-01],
        [9.4191e-01],
        [9.4996e-01],
        [9.4482e-01],
        [9.5875e-01],
        [9.5442e-01],
        [9.6984e-01],
        [9.6654e-01],
        [9.8435e-01],
        [9.8239e-01],
        [9.8939e-01],
        [9.8789e-01],
        [9.9055e-01],
        [9.9509e-01],
        [9.9412e-01],
        [9.9937e-01],
        [9.9879e-01],
        [9.9816e-01],
        [1.0016e+00],
        [1.0012e+00],
        [1.0032e+00],
        [1.0030e+00],
        [1.0028e+00],
        [1.0041e+00],
        [1.0039e+00],
        [1.0047e+00],
        [1.0046e+00],
        [1.0048e+00],
        [1.0050e+00],
        [1.0051e+00],
        [1.0053e+00],
        [1.0053e+00],
        [1.0054e+00],
        [1.0055e+00],
        [1.0055e+00],
        [1.0055e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00]], device='cuda:0') 
Smoothed_batch_entropy:  tensor([-2.7118e-02, -2.8141e-03, -6.6816e-03,  2.8400e-03,  2.2972e-03,
        -1.4658e-02,  4.8628e-04, -5.7167e-04, -1.1852e-02,  2.5672e-03,
        -2.3589e-02, -4.6643e-03,  7.8203e-03, -2.2167e-02, -6.6850e-04,
        -2.1849e-02, -1.3149e-02,  2.6218e-02, -2.2194e-02, -1.0964e-04,
        -2.3180e-02,  1.4368e-02,  2.5701e-02, -1.6683e-02,  3.0055e-02,
        -5.0673e-03,  6.2524e-03,  3.5543e-02,  9.4796e-03,  3.4554e-02,
         1.0205e-02,  1.4424e-02,  3.3849e-02,  1.2224e-02,  3.0586e-02,
         8.6985e-03,  2.6579e-02,  3.7838e-03,  2.1745e-02,  3.3199e-02,
         1.5882e-02,  2.8003e-02,  8.7290e-03,  2.1838e-02,  3.0989e-02,
         1.4372e-02,  2.4695e-02,  5.2524e-03,  1.7037e-02,  2.5831e-02,
         7.6754e-03,  1.8019e-02, -5.8147e-04,  8.3454e-03,  2.8007e-02,
         1.1603e-02,  2.0263e-02,  5.7975e-04,  1.0654e-02,  3.2315e-03,
         1.2976e-02,  2.0871e-02,  1.4205e-03,  2.2329e-02,  3.7413e-03,
         1.2894e-02,  2.0353e-02,  1.4247e-02,  2.1264e-02,  2.2485e-03,
         2.1856e-02,  2.7271e-02,  2.2034e-02,  2.7096e-02,  1.0983e-02,
         2.6446e-02,  3.0323e-02,  2.5095e-02,  2.8704e-02,  2.2922e-02,
         2.6229e-02,  1.9663e-02,  2.2676e-02,  3.2192e-02,  1.7655e-02,
         2.7965e-02,  1.0836e-02,  2.2280e-02,  3.0607e-02,  1.4677e-02,
         2.4382e-02,  4.5466e-03,  1.6142e-02,  2.4954e-02,  5.3086e-03,
         1.6190e-02,  5.0290e-03,  4.6885e-03,  1.5361e-02,  3.8561e-03,
         1.4546e-02, -1.7938e-04,  2.0997e-03,  2.0629e-04,  7.4959e-04,
         4.8301e-04,  5.5539e-03,  4.8440e-03,  5.8913e-04,  6.0254e-04,
         5.6342e-04,  5.2137e-04,  8.8158e-04,  3.7587e-04,  2.6442e-04,
         1.3415e-04, -7.5758e-05,  8.5752e-03,  7.6684e-03,  6.7105e-03,
         5.6642e-03,  4.5338e-03,  3.2757e-03,  1.2935e-02,  1.1652e-02,
         1.5251e-02,  1.3779e-02,  1.2104e-02,  1.9351e-02,  1.7521e-02,
         1.9833e-02,  1.7614e-02,  1.4990e-02,  1.6743e-02,  2.1793e-02,
         2.2783e-02,  1.9449e-02,  1.9985e-02,  1.5784e-02,  2.3523e-02,
         1.9228e-02,  1.8769e-02,  1.3285e-02,  1.2278e-02,  1.1049e-02,
         1.3627e-02,  1.2091e-02,  1.0339e-02,  8.3986e-03, -3.0131e-04,
         8.3455e-03,  6.0385e-03,  3.5092e-03,  8.0445e-04, -8.9345e-05,
         7.0857e-05,  3.8307e-03,  9.7557e-04,  4.7390e-04,  5.2549e-04,
         5.7305e-04,  7.0392e-04,  6.1277e-04,  5.8603e-04,  5.3676e-04,
         4.8319e-04,  3.5938e-03,  6.4605e-04,  8.2380e-05,  1.0001e-03,
        -3.6701e-04, -7.8149e-04,  8.0461e-03,  5.1328e-03,  7.6193e-03,
         4.5569e-03,  6.8472e-03,  1.2096e-02,  1.3642e-02,  1.0507e-02,
         1.1815e-02,  1.2858e-02,  9.2319e-03,  1.6746e-02,  1.7053e-02,
         1.3253e-02,  1.3211e-02,  1.2949e-02,  1.8390e-02,  1.7728e-02,
         1.3100e-02,  1.1960e-02,  1.0635e-02,  9.0644e-03,  1.3698e-02,
         1.1933e-02,  9.9356e-03,  7.7062e-03,  5.1929e-03,  9.5786e-03,
         1.1467e-02,  8.8949e-03,  6.0468e-03,  2.8829e-03, -6.5818e-04,
         8.9658e-03,  5.7262e-03,  2.1198e-03,  3.9450e-03, -4.2308e-05,
         4.4512e-03,  6.0982e-03,  1.8660e-03,  3.5070e-03,  4.8043e-04,
         3.5119e-04,  4.4459e-03,  5.7487e-03,  5.7270e-05,  1.1150e-03,
         2.0146e-03,  9.2457e-03,  6.3940e-03,  6.7873e-03,  2.9817e-03,
         2.7295e-03, -2.4469e-03,  9.0385e-03,  8.1658e-03,  2.2770e-03,
         3.4259e-04, -2.0758e-03,  9.1431e-03,  6.6538e-03,  3.5956e-03,
        -1.1377e-04, -1.5126e-04,  5.0571e-04,  8.2319e-03,  3.7616e-03,
         4.8909e-03,  8.6430e-04,  7.6059e-04,  1.0832e-02,  5.5935e-03,
         6.1489e-03,  6.6186e-03,  4.8800e-04,  1.6216e-02,  1.6500e-02,
         1.6757e-02,  1.6983e-02,  1.7290e-02,  1.7491e-02,  2.8322e-02,
         2.8512e-02,  2.8627e-02,  3.1939e-02,  3.2054e-02,  3.8639e-02,
         4.0640e-02,  4.0662e-02,  4.2396e-02,  4.2367e-02,  4.3834e-02,
         4.7740e-02,  4.7629e-02,  4.8304e-02,  4.8863e-02,  4.9306e-02,
         5.0684e-02,  5.0767e-02,  5.0782e-02,  5.0711e-02,  5.0597e-02,
         5.0412e-02,  5.0421e-02,  5.0160e-02,  4.9756e-02,  4.9304e-02,
         4.8867e-02,  4.8347e-02,  4.7798e-02,  4.7211e-02,  4.6518e-02,
         4.5847e-02,  4.5138e-02,  4.4497e-02,  4.3615e-02,  4.2801e-02,
         4.1941e-02,  4.1034e-02,  4.0379e-02,  3.9649e-02,  3.8649e-02,
         3.7580e-02,  3.6440e-02,  3.5580e-02,  3.4907e-02,  3.4018e-02,
         3.2656e-02,  3.1676e-02,  3.0551e-02,  2.9482e-02,  2.8073e-02,
         2.7387e-02,  2.5855e-02,  2.5130e-02,  2.3814e-02,  2.2438e-02,
         2.1632e-02,  2.0149e-02,  1.8573e-02,  1.8142e-02,  1.6473e-02,
         1.6011e-02,  1.4746e-02,  1.2900e-02,  1.2404e-02,  1.1007e-02,
         9.5500e-03,  9.5835e-03,  8.0878e-03,  6.5427e-03,  6.5807e-03,
         5.6131e-03,  5.0280e-03,  4.0227e-03,  3.0095e-03,  3.0514e-03,
         2.0104e-03,  1.6337e-03,  1.6982e-03,  6.3135e-04,  2.5823e-04,
         9.9165e-04,  6.1862e-04,  6.7271e-04,  2.8988e-04,  5.9515e-04,
         1.3366e-03,  9.6456e-04,  5.8268e-04,  2.0085e-03,  2.2905e-03,
         1.9185e-03,  3.2991e-03,  3.5797e-03,  4.9055e-03,  5.1742e-03,
         6.0522e-03,  7.2897e-03,  7.5445e-03,  8.3664e-03,  1.0069e-02,
         1.0300e-02,  1.1911e-02,  1.2638e-02,  1.3334e-02,  1.5251e-02,
         1.5886e-02,  1.6724e-02,  1.8207e-02,  1.8971e-02,  1.9895e-02,
         2.1195e-02,  2.2017e-02,  2.3341e-02,  2.4075e-02,  2.4924e-02,
         2.6172e-02,  2.6910e-02,  2.7614e-02,  2.8644e-02,  2.9354e-02,
         2.9922e-02,  3.0855e-02,  3.1514e-02,  3.2300e-02,  3.2854e-02,
         3.3299e-02,  3.3983e-02,  3.4470e-02,  3.4865e-02,  3.5437e-02,
         3.5804e-02,  3.6141e-02,  3.6570e-02,  3.6877e-02,  3.7223e-02,
         3.7468e-02,  3.7690e-02,  3.7988e-02,  3.8160e-02,  3.8333e-02,
         3.8543e-02,  3.8676e-02,  3.8837e-02,  3.8948e-02,  3.9045e-02,
         3.9161e-02,  3.9229e-02,  3.9304e-02,  3.9354e-02,  3.9409e-02,
         3.9453e-02,  3.9490e-02,  3.9510e-02,  3.9530e-02,  3.9545e-02,
         3.9553e-02,  3.9551e-02,  3.9549e-02,  3.9543e-02,  3.9529e-02,
         3.9515e-02,  3.9499e-02,  3.9478e-02,  3.9454e-02,  3.9429e-02,
         3.9402e-02,  3.9373e-02,  3.9342e-02,  3.9310e-02,  3.9276e-02,
         3.9242e-02,  3.9206e-02,  3.9170e-02,  3.9133e-02,  3.9095e-02,
         3.9056e-02,  3.9018e-02,  3.8978e-02,  3.8939e-02,  3.8899e-02,
         3.8859e-02,  3.8819e-02,  3.8778e-02,  3.8737e-02,  3.8696e-02,
         3.8655e-02,  3.8614e-02,  3.8573e-02,  3.8532e-02,  3.8491e-02,
         3.8450e-02,  3.8409e-02,  3.8368e-02,  3.8327e-02,  3.8286e-02,
         3.8245e-02,  3.8204e-02,  3.8163e-02,  3.8122e-02,  3.8082e-02,
         3.8041e-02,  3.8000e-02,  3.7959e-02,  3.7919e-02,  3.7878e-02,
         3.7838e-02,  3.7797e-02,  3.7757e-02,  3.7716e-02,  3.7676e-02,
         3.7636e-02,  3.7595e-02,  3.7555e-02,  3.7515e-02,  3.7475e-02,
         3.7435e-02,  3.7395e-02,  3.7355e-02,  3.7314e-02,  3.7274e-02,
         3.7234e-02,  3.7194e-02,  3.7154e-02,  3.7113e-02,  3.7073e-02,
         3.7032e-02,  3.6991e-02,  3.6951e-02,  3.6910e-02,  3.6868e-02,
         3.6826e-02,  3.6784e-02,  3.6742e-02,  3.6699e-02,  3.6655e-02,
         3.6611e-02,  3.6566e-02,  3.6520e-02,  3.6473e-02,  3.6425e-02,
         3.6376e-02,  3.6325e-02,  3.6273e-02,  3.6219e-02,  3.6162e-02,
         3.4727e-02,  3.4580e-02,  3.4422e-02,  3.4254e-02,  3.4074e-02,
         3.3880e-02,  3.3673e-02,  3.3452e-02,  3.3215e-02,  3.2962e-02,
         3.2697e-02,  3.2403e-02,  3.2102e-02,  3.1769e-02,  3.1426e-02,
         3.1050e-02,  3.0665e-02,  3.0240e-02,  2.9809e-02,  2.9354e-02,
         2.8857e-02,  2.8350e-02,  2.7797e-02,  2.7244e-02,  2.6642e-02,
         2.6036e-02,  2.5380e-02,  2.4721e-02,  2.4006e-02,  2.3310e-02,
         2.2554e-02,  2.1809e-02,  2.1052e-02,  2.0234e-02,  1.9426e-02,
         1.8568e-02,  1.7705e-02,  1.6946e-02,  1.6044e-02,  1.5168e-02,
         1.4265e-02,  1.3490e-02,  1.2610e-02,  1.1688e-02,  1.0827e-02,
         1.0083e-02,  9.2217e-03,  8.4256e-03,  7.7385e-03,  6.9582e-03,
         6.2146e-03,  5.4509e-03,  4.9104e-03,  4.2420e-03,  3.6103e-03,
         3.0297e-03,  2.6990e-03,  2.1629e-03,  1.7404e-03,  1.5172e-03,
         1.0895e-03,  7.8931e-04,  4.7459e-04,  4.9780e-04,  3.0940e-04,
         2.4671e-04,  1.8411e-04,  3.3227e-04,  3.9436e-04,  4.5679e-04,
         6.3591e-04,  1.0184e-03,  1.3305e-03,  1.6275e-03,  2.1130e-03,
         2.5205e-03,  2.9282e-03,  3.4391e-03,  4.1251e-03,  4.6214e-03,
         5.2069e-03,  5.7922e-03,  6.6274e-03,  7.2793e-03,  8.1735e-03,
         8.7236e-03,  9.5712e-03,  1.0388e-02,  1.1062e-02,  1.1987e-02,
         1.2618e-02,  1.3479e-02,  1.4453e-02,  1.5003e-02,  1.5895e-02,
         1.6741e-02,  1.7467e-02,  1.8229e-02,  1.8879e-02,  1.9669e-02,
         2.0411e-02,  2.0957e-02,  2.1704e-02,  2.2315e-02,  2.2842e-02,
         2.3522e-02,  2.4083e-02,  2.4565e-02,  2.5050e-02,  2.5464e-02,
         2.5980e-02,  2.6405e-02,  2.6768e-02,  2.7165e-02,  2.7525e-02,
         2.7800e-02,  2.8102e-02,  2.8402e-02,  2.8631e-02,  2.8876e-02,
         2.9084e-02,  2.9282e-02,  2.9476e-02,  2.9623e-02,  2.9778e-02,
         2.9926e-02,  3.0015e-02,  3.0133e-02,  3.0235e-02,  3.0309e-02,
         3.0401e-02,  3.0454e-02,  3.0510e-02,  3.0567e-02,  3.0606e-02,
         3.0637e-02,  3.0668e-02,  3.0687e-02,  3.0706e-02,  3.0715e-02,
         3.0728e-02,  3.0731e-02,  3.0728e-02,  3.0728e-02,  3.0724e-02,
         3.0714e-02,  3.0702e-02,  3.0690e-02,  3.0675e-02,  3.0659e-02,
         3.0639e-02,  3.0620e-02,  3.0600e-02,  3.0578e-02,  3.0555e-02,
         3.0532e-02,  3.0507e-02,  3.0483e-02,  3.0457e-02,  3.0431e-02,
         3.0405e-02,  3.0378e-02,  3.0351e-02,  3.0324e-02,  3.0296e-02,
         3.0269e-02,  3.0241e-02,  3.0213e-02,  3.0185e-02,  3.0156e-02,
         3.0128e-02,  3.0100e-02,  3.0072e-02,  3.0043e-02,  3.0015e-02,
         2.9986e-02,  2.9958e-02,  2.9930e-02,  2.9901e-02,  2.9873e-02,
         2.9844e-02,  2.9816e-02,  2.9788e-02,  2.9760e-02,  2.9731e-02,
         2.9703e-02,  2.9675e-02,  2.9647e-02,  2.9618e-02,  2.9590e-02,
         2.9562e-02,  2.9534e-02,  2.9506e-02,  2.9478e-02,  2.9450e-02,
         2.9422e-02,  2.9394e-02,  2.9367e-02,  2.9339e-02,  2.9311e-02,
         2.9283e-02,  2.9256e-02,  2.9228e-02,  2.9200e-02,  2.9173e-02,
         2.9146e-02,  2.9118e-02,  2.9091e-02,  2.9063e-02,  2.9036e-02,
         2.9008e-02,  2.8981e-02,  2.8954e-02,  2.8927e-02,  2.8900e-02,
         2.8873e-02,  2.8846e-02,  2.8818e-02,  2.8792e-02,  2.8765e-02,
         2.8738e-02,  2.8711e-02,  2.8684e-02,  2.8657e-02,  2.8630e-02,
         2.8604e-02,  2.8577e-02,  2.8551e-02,  2.8524e-02,  2.8498e-02,
         2.8471e-02,  2.8445e-02,  2.8419e-02,  2.8393e-02,  2.8367e-02,
         2.8341e-02,  2.8315e-02,  2.8290e-02,  2.8265e-02,  2.8239e-02,
         2.8214e-02,  2.8189e-02,  2.8166e-02,  2.8141e-02,  2.8116e-02,
         2.8093e-02,  2.8069e-02,  2.8048e-02,  2.8026e-02,  2.8003e-02,
         2.7982e-02,  2.7960e-02,  2.7939e-02,  2.7925e-02,  2.7906e-02,
         2.7889e-02,  2.7870e-02,  2.7854e-02,  2.7850e-02,  2.7835e-02,
         2.7824e-02,  2.7811e-02,  2.7804e-02,  2.7793e-02,  2.7807e-02,
         2.7809e-02,  2.7814e-02,  2.7804e-02,  2.7815e-02,  2.7848e-02,
         2.7870e-02,  2.7869e-02,  2.7900e-02,  2.7903e-02,  2.7943e-02,
         2.8023e-02,  2.8037e-02,  2.8101e-02,  2.8121e-02,  2.8143e-02,
         2.8337e-02,  2.8371e-02,  2.8408e-02,  2.8448e-02,  2.8490e-02,
         2.8616e-02,  2.8823e-02,  2.8880e-02,  2.8940e-02,  2.9000e-02,
         2.9062e-02,  2.9300e-02,  2.9362e-02,  2.9421e-02,  2.9477e-02,
         2.9441e-02,  2.9492e-02,  2.9654e-02,  2.9668e-02,  2.9661e-02,
         2.9629e-02,  2.9607e-02,  2.9424e-02,  2.9404e-02,  2.9245e-02,
         2.9029e-02,  2.9016e-02,  2.8742e-02,  2.8039e-02,  2.7480e-02,
         2.7502e-02,  2.6823e-02,  2.6852e-02,  2.3870e-02,  2.3967e-02,
         2.4049e-02,  2.2561e-02,  2.2673e-02,  2.2783e-02,  1.9195e-02,
         1.6567e-02,  1.6786e-02,  1.7019e-02,  1.7232e-02,  1.0993e-02,
         1.1328e-02,  1.1657e-02,  1.1981e-02,  1.2300e-02,  3.6271e-03,
         4.0934e-03,  4.5766e-03,  5.0516e-03,  5.4693e-03,  5.9286e-03,
         6.4347e-04,  2.1501e-03,  2.6607e-03,  3.1365e-03,  3.6042e-03,
         6.3935e-04,  6.3849e-04,  6.3763e-04,  6.6261e-03,  7.0603e-03,
         1.2006e-02,  3.4985e-03,  9.2533e-03,  9.6219e-03,  1.3934e-02,
         1.4212e-02,  1.1518e-02,  1.1829e-02,  1.5626e-02,  1.8567e-02,
         1.8743e-02,  2.0983e-02,  1.9549e-02,  2.1578e-02,  2.3162e-02,
         2.3251e-02,  2.4449e-02,  2.3648e-02,  2.4742e-02,  2.5590e-02,
         2.6227e-02,  2.6703e-02,  2.7047e-02,  2.6787e-02,  2.7096e-02,
         2.7312e-02,  2.7450e-02,  2.7530e-02,  2.7521e-02,  2.7544e-02,
         2.7531e-02,  2.7488e-02,  2.7376e-02,  2.7285e-02,  2.7303e-02,
         2.7139e-02,  2.7164e-02,  2.6920e-02,  2.6952e-02,  2.6530e-02,
         2.6563e-02,  2.6310e-02,  2.6342e-02,  2.6218e-02,  2.5987e-02,
         2.6013e-02,  2.5676e-02,  2.5695e-02,  2.5715e-02,  2.5425e-02,
         2.5437e-02,  2.5197e-02,  2.5203e-02,  2.5210e-02,  2.5007e-02,
         2.5009e-02,  2.4843e-02,  2.4840e-02,  2.4785e-02,  2.4699e-02,
         2.4651e-02,  2.4579e-02,  2.4537e-02,  2.4476e-02,  2.4439e-02,
         2.4426e-02,  2.4354e-02,  2.4322e-02,  2.4279e-02,  2.4249e-02,
         2.4233e-02,  2.4183e-02,  2.4156e-02,  2.4123e-02,  2.4098e-02,
         2.4073e-02,  2.4038e-02,  2.4019e-02,  2.3988e-02,  2.3965e-02,
         2.3943e-02,  2.3918e-02,  2.3896e-02,  2.3870e-02,  2.3849e-02,
         2.3825e-02,  2.3805e-02,  2.3784e-02,  2.3761e-02,  2.3740e-02,
         2.3718e-02,  2.3698e-02,  2.3677e-02,  2.3656e-02,  2.3636e-02,
         2.3615e-02,  2.3594e-02,  2.3574e-02,  2.3554e-02,  2.3534e-02,
         2.3513e-02,  2.3493e-02,  2.3473e-02,  2.3453e-02,  2.3433e-02,
         2.3413e-02,  2.3394e-02,  2.3374e-02,  2.3354e-02,  2.3334e-02,
         2.3314e-02,  2.3294e-02,  2.3275e-02,  2.3255e-02,  2.3235e-02,
         2.3216e-02,  2.3196e-02,  2.3177e-02,  2.3157e-02,  2.3138e-02,
         2.3118e-02,  2.3098e-02,  2.3079e-02,  2.3060e-02,  2.3040e-02,
         2.3021e-02,  2.3001e-02,  2.2982e-02,  2.2963e-02,  2.2944e-02,
         2.2925e-02,  2.2905e-02,  2.2886e-02,  2.2867e-02,  2.2848e-02,
         2.2828e-02,  2.2809e-02,  2.2790e-02,  2.2771e-02,  2.2752e-02],
       device='cuda:0')
Selected points (indices): {272, 432}
Selected new x values (normalized): tensor([0.2723, 0.4324], device='cuda:0')
Corresponding new x values (unnormalized): tensor([-910.9110, -270.2704], device='cuda:0')
Plot saved to /raven/u/dvoss/al_pmssmwithgp/model/plots/Iter4/gp_plot.png
YAML file has been created: /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/data/new_config.yaml
Note: detected 144 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
Note: NumExpr detected 144 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
NumExpr defaulting to 8 threads.

  ___              ____ __  __          _       _   ___            
 | _ \ _  _  _ _  |__ /|  \/  | ___  __| | ___ | | / __| ___  _ _  
 |   /| || || ' \  |_ \| |\/| |/ _ \/ _` |/ -_)| || (_ |/ -_)| ' \ 
 |_|_\ \_,_||_||_||___/|_|  |_|\___/\__,_|\___||_| \___|\___||_||_|
                                                                   

2024-08-28 19:52:34 [info     ] Initialised ModelGenerator with: [Run3ModelGen.modelgen]
2024-08-28 19:52:34 [info     ] 	config_file = /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/data/new_config.yaml [Run3ModelGen.modelgen]
2024-08-28 19:52:34 [info     ] 	scan_dir = /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/scans/scan_4 [Run3ModelGen.modelgen]
2024-08-28 19:52:34 [info     ] 	rawfilen = /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/data/raw.slha [Run3ModelGen.modelgen]
2024-08-28 19:52:34 [info     ] 	seed = 123                    [Run3ModelGen.modelgen]
2024-08-28 19:52:34 [info     ] 	points = {}                   [Run3ModelGen.modelgen]
2024-08-28 19:52:34 [info     ] 	prior = fixed                 [Run3ModelGen.modelgen]
2024-08-28 19:52:34 [info     ] 	num_models = 2                [Run3ModelGen.modelgen]
2024-08-28 19:52:34 [info     ] 	isGMSB = False                [Run3ModelGen.modelgen]
2024-08-28 19:52:34 [info     ] 	parameters =                  [Run3ModelGen.modelgen]
2024-08-28 19:52:34 [info     ] 		tanb: [60, 60]               [Run3ModelGen.modelgen]
2024-08-28 19:52:34 [info     ] 		M_1: [-910.91, -270.27]      [Run3ModelGen.modelgen]
2024-08-28 19:52:34 [info     ] 		M_2: [2000, 2000]            [Run3ModelGen.modelgen]
2024-08-28 19:52:34 [info     ] 		M_3: [4000, 4000]            [Run3ModelGen.modelgen]
2024-08-28 19:52:34 [info     ] 		AT: [4000, 4000]             [Run3ModelGen.modelgen]
2024-08-28 19:52:34 [info     ] 		Ab: [2000, 2000]             [Run3ModelGen.modelgen]
2024-08-28 19:52:34 [info     ] 		Atau: [2000, 2000]           [Run3ModelGen.modelgen]
2024-08-28 19:52:34 [info     ] 		mu: [2000, 2000]             [Run3ModelGen.modelgen]
2024-08-28 19:52:34 [info     ] 		mA: [2000, 2000]             [Run3ModelGen.modelgen]
2024-08-28 19:52:34 [info     ] 		meL: [2000, 2000]            [Run3ModelGen.modelgen]
2024-08-28 19:52:34 [info     ] 		mtauL: [2000, 2000]          [Run3ModelGen.modelgen]
2024-08-28 19:52:34 [info     ] 		meR: [2000, 2000]            [Run3ModelGen.modelgen]
2024-08-28 19:52:34 [info     ] 		mtauR: [2000, 2000]          [Run3ModelGen.modelgen]
2024-08-28 19:52:34 [info     ] 		mqL1: [4000, 4000]           [Run3ModelGen.modelgen]
2024-08-28 19:52:34 [info     ] 		mqL3: [4000, 4000]           [Run3ModelGen.modelgen]
2024-08-28 19:52:34 [info     ] 		muR: [4000, 4000]            [Run3ModelGen.modelgen]
2024-08-28 19:52:34 [info     ] 		mtR: [4000, 4000]            [Run3ModelGen.modelgen]
2024-08-28 19:52:34 [info     ] 		mdR: [4000, 4000]            [Run3ModelGen.modelgen]
2024-08-28 19:52:34 [info     ] 		mbR: [4000, 4000]            [Run3ModelGen.modelgen]
2024-08-28 19:52:34 [info     ] 	steps =                       [Run3ModelGen.modelgen]
2024-08-28 19:52:34 [info     ] 		{'name': 'prep_input', 'output_dir': 'input', 'prefix': 'IN'} [Run3ModelGen.modelgen]
2024-08-28 19:52:34 [info     ] 		{'name': 'SPheno', 'input_dir': 'input', 'output_dir': 'SPheno', 'log_dir': 'SPheno_log', 'prefix': 'SP'} [Run3ModelGen.modelgen]
2024-08-28 19:52:34 [info     ] 		{'name': 'softsusy', 'input_dir': 'input', 'output_dir': 'softsusy', 'prefix': 'SS'} [Run3ModelGen.modelgen]
2024-08-28 19:52:34 [info     ] 		{'name': 'micromegas', 'input_dir': 'SPheno', 'output_dir': 'micromegas', 'prefix': 'MO'} [Run3ModelGen.modelgen]
2024-08-28 19:52:34 [info     ] 		{'name': 'superiso', 'input_dir': 'SPheno', 'output_dir': 'superiso', 'prefix': 'SI'} [Run3ModelGen.modelgen]
2024-08-28 19:52:34 [info     ] 		{'name': 'gm2calc', 'input_dir': 'SPheno', 'output_dir': 'gm2calc', 'prefix': 'GM2'} [Run3ModelGen.modelgen]
2024-08-28 19:52:34 [info     ] 		{'name': 'evade', 'input_dir': 'SPheno', 'output_dir': 'evade', 'prefix': 'EV'} [Run3ModelGen.modelgen]
2024-08-28 19:52:34 [info     ] Starting Model Generation...   [Run3ModelGen.modelgen]
2024-08-28 19:52:34 [info     ] Setting up output directories for scan [Run3ModelGen.modelgen]
2024-08-28 19:52:35 [info     ] Generating Model: 0.slha (1/2) [Run3ModelGen.modelgen]
2024-08-28 19:52:35 [info     ] 	running step: prep_input      [Run3ModelGen.modelgen]
2024-08-28 19:52:35 [info     ] 	running step: SPheno          [Run3ModelGen.modelgen]
2024-08-28 19:52:35 [info     ] 	running step: softsusy        [Run3ModelGen.modelgen]
2024-08-28 19:52:35 [info     ] 	running step: micromegas      [Run3ModelGen.modelgen]
2024-08-28 19:52:35 [info     ] 	running step: superiso        [Run3ModelGen.modelgen]
2024-08-28 19:52:35 [info     ] 	running step: gm2calc         [Run3ModelGen.modelgen]
2024-08-28 19:52:35 [info     ] 	running step: evade           [Run3ModelGen.modelgen]
2024-08-28 19:52:35 [info     ] Generating Model: 1.slha (2/2) [Run3ModelGen.modelgen]
2024-08-28 19:52:35 [info     ] 	running step: prep_input      [Run3ModelGen.modelgen]
2024-08-28 19:52:35 [info     ] 	running step: SPheno          [Run3ModelGen.modelgen]
2024-08-28 19:52:36 [info     ] 	running step: softsusy        [Run3ModelGen.modelgen]
2024-08-28 19:52:36 [info     ] 	running step: micromegas      [Run3ModelGen.modelgen]
2024-08-28 19:52:36 [info     ] 	running step: superiso        [Run3ModelGen.modelgen]
2024-08-28 19:52:36 [info     ] 	running step: gm2calc         [Run3ModelGen.modelgen]
2024-08-28 19:52:36 [info     ] 	running step: evade           [Run3ModelGen.modelgen]
2024-08-28 19:52:36 [info     ] Finished generating models! Stats for successful generations (successful/attempted): [Run3ModelGen.modelgen]
2024-08-28 19:52:36 [info     ] 	prep_input: 2/2.              [Run3ModelGen.modelgen]
2024-08-28 19:52:36 [info     ] 	SPheno: 2/2.                  [Run3ModelGen.modelgen]
2024-08-28 19:52:36 [info     ] 	softsusy: 2/2.                [Run3ModelGen.modelgen]
2024-08-28 19:52:36 [info     ] 	micromegas: 2/2.              [Run3ModelGen.modelgen]
2024-08-28 19:52:36 [info     ] 	superiso: 2/2.                [Run3ModelGen.modelgen]
2024-08-28 19:52:36 [info     ] 	gm2calc: 2/2.                 [Run3ModelGen.modelgen]
2024-08-28 19:52:36 [info     ] 	evade: 0/2.                   [Run3ModelGen.modelgen]
2024-08-28 19:52:36 [info     ] Will make NTuple for: /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/scans/scan_4, with 2 models [Run3ModelGen.ntupling]
2024-08-28 19:52:36 [info     ] 	Filling values for model: 0   [Run3ModelGen.ntupling]
2024-08-28 19:52:36 [info     ] Saving NTuple in /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/scans/scan_4/ntuple.0.0.root ... [Run3ModelGen.ntupling]
Completed iteration 4
Starting iteration 5
Initial Training points:  tensor([0.8528, 0.5812, 0.1617, 0.0046], device='cuda:0') torch.Size([4])
Training points after adding:  tensor([0.4324, 0.1617, 0.1091, 0.0046, 0.2132, 0.0561, 0.3373, 0.2723, 0.2392,
        0.5812, 0.0210, 0.8528], device='cuda:0') torch.Size([12])
These training_points are used in the GP tensor([0.4324, 0.1617, 0.1091, 0.0046, 0.2132, 0.0561, 0.3373, 0.2723, 0.2392,
        0.5812, 0.0210, 0.8528], device='cuda:0')
Point: tensor([0.0000, 0.0010, 0.0020, 0.0030, 0.0040, 0.0050, 0.0060, 0.0070, 0.0080,
        0.0090, 0.0100, 0.0110, 0.0120, 0.0130, 0.0140, 0.0150, 0.0160, 0.0170,
        0.0180, 0.0190, 0.0200, 0.0210, 0.0220, 0.0230, 0.0240, 0.0250, 0.0260,
        0.0270, 0.0280, 0.0290, 0.0300, 0.0310, 0.0320, 0.0330, 0.0340, 0.0350,
        0.0360, 0.0370, 0.0380, 0.0390, 0.0400, 0.0410, 0.0420, 0.0430, 0.0440,
        0.0450, 0.0460, 0.0470, 0.0480, 0.0490, 0.0501, 0.0511, 0.0521, 0.0531,
        0.0541, 0.0551, 0.0561, 0.0571, 0.0581, 0.0591, 0.0601, 0.0611, 0.0621,
        0.0631, 0.0641, 0.0651, 0.0661, 0.0671, 0.0681, 0.0691, 0.0701, 0.0711,
        0.0721, 0.0731, 0.0741, 0.0751, 0.0761, 0.0771, 0.0781, 0.0791, 0.0801,
        0.0811, 0.0821, 0.0831, 0.0841, 0.0851, 0.0861, 0.0871, 0.0881, 0.0891,
        0.0901, 0.0911, 0.0921, 0.0931, 0.0941, 0.0951, 0.0961, 0.0971, 0.0981,
        0.0991, 0.1001, 0.1011, 0.1021, 0.1031, 0.1041, 0.1051, 0.1061, 0.1071,
        0.1081, 0.1091, 0.1101, 0.1111, 0.1121, 0.1131, 0.1141, 0.1151, 0.1161,
        0.1171, 0.1181, 0.1191, 0.1201, 0.1211, 0.1221, 0.1231, 0.1241, 0.1251,
        0.1261, 0.1271, 0.1281, 0.1291, 0.1301, 0.1311, 0.1321, 0.1331, 0.1341,
        0.1351, 0.1361, 0.1371, 0.1381, 0.1391, 0.1401, 0.1411, 0.1421, 0.1431,
        0.1441, 0.1451, 0.1461, 0.1471, 0.1481, 0.1491, 0.1502, 0.1512, 0.1522,
        0.1532, 0.1542, 0.1552, 0.1562, 0.1572, 0.1582, 0.1592, 0.1602, 0.1612,
        0.1622, 0.1632, 0.1642, 0.1652, 0.1662, 0.1672, 0.1682, 0.1692, 0.1702,
        0.1712, 0.1722, 0.1732, 0.1742, 0.1752, 0.1762, 0.1772, 0.1782, 0.1792,
        0.1802, 0.1812, 0.1822, 0.1832, 0.1842, 0.1852, 0.1862, 0.1872, 0.1882,
        0.1892, 0.1902, 0.1912, 0.1922, 0.1932, 0.1942, 0.1952, 0.1962, 0.1972,
        0.1982, 0.1992, 0.2002, 0.2012, 0.2022, 0.2032, 0.2042, 0.2052, 0.2062,
        0.2072, 0.2082, 0.2092, 0.2102, 0.2112, 0.2122, 0.2132, 0.2142, 0.2152,
        0.2162, 0.2172, 0.2182, 0.2192, 0.2202, 0.2212, 0.2222, 0.2232, 0.2242,
        0.2252, 0.2262, 0.2272, 0.2282, 0.2292, 0.2302, 0.2312, 0.2322, 0.2332,
        0.2342, 0.2352, 0.2362, 0.2372, 0.2382, 0.2392, 0.2402, 0.2412, 0.2422,
        0.2432, 0.2442, 0.2452, 0.2462, 0.2472, 0.2482, 0.2492, 0.2503, 0.2513,
        0.2523, 0.2533, 0.2543, 0.2553, 0.2563, 0.2573, 0.2583, 0.2593, 0.2603,
        0.2613, 0.2623, 0.2633, 0.2643, 0.2653, 0.2663, 0.2673, 0.2683, 0.2693,
        0.2703, 0.2713, 0.2723, 0.2733, 0.2743, 0.2753, 0.2763, 0.2773, 0.2783,
        0.2793, 0.2803, 0.2813, 0.2823, 0.2833, 0.2843, 0.2853, 0.2863, 0.2873,
        0.2883, 0.2893, 0.2903, 0.2913, 0.2923, 0.2933, 0.2943, 0.2953, 0.2963,
        0.2973, 0.2983, 0.2993, 0.3003, 0.3013, 0.3023, 0.3033, 0.3043, 0.3053,
        0.3063, 0.3073, 0.3083, 0.3093, 0.3103, 0.3113, 0.3123, 0.3133, 0.3143,
        0.3153, 0.3163, 0.3173, 0.3183, 0.3193, 0.3203, 0.3213, 0.3223, 0.3233,
        0.3243, 0.3253, 0.3263, 0.3273, 0.3283, 0.3293, 0.3303, 0.3313, 0.3323,
        0.3333, 0.3343, 0.3353, 0.3363, 0.3373, 0.3383, 0.3393, 0.3403, 0.3413,
        0.3423, 0.3433, 0.3443, 0.3453, 0.3463, 0.3473, 0.3483, 0.3493, 0.3504,
        0.3514, 0.3524, 0.3534, 0.3544, 0.3554, 0.3564, 0.3574, 0.3584, 0.3594,
        0.3604, 0.3614, 0.3624, 0.3634, 0.3644, 0.3654, 0.3664, 0.3674, 0.3684,
        0.3694, 0.3704, 0.3714, 0.3724, 0.3734, 0.3744, 0.3754, 0.3764, 0.3774,
        0.3784, 0.3794, 0.3804, 0.3814, 0.3824, 0.3834, 0.3844, 0.3854, 0.3864,
        0.3874, 0.3884, 0.3894, 0.3904, 0.3914, 0.3924, 0.3934, 0.3944, 0.3954,
        0.3964, 0.3974, 0.3984, 0.3994, 0.4004, 0.4014, 0.4024, 0.4034, 0.4044,
        0.4054, 0.4064, 0.4074, 0.4084, 0.4094, 0.4104, 0.4114, 0.4124, 0.4134,
        0.4144, 0.4154, 0.4164, 0.4174, 0.4184, 0.4194, 0.4204, 0.4214, 0.4224,
        0.4234, 0.4244, 0.4254, 0.4264, 0.4274, 0.4284, 0.4294, 0.4304, 0.4314,
        0.4324, 0.4334, 0.4344, 0.4354, 0.4364, 0.4374, 0.4384, 0.4394, 0.4404,
        0.4414, 0.4424, 0.4434, 0.4444, 0.4454, 0.4464, 0.4474, 0.4484, 0.4494,
        0.4505, 0.4515, 0.4525, 0.4535, 0.4545, 0.4555, 0.4565, 0.4575, 0.4585,
        0.4595, 0.4605, 0.4615, 0.4625, 0.4635, 0.4645, 0.4655, 0.4665, 0.4675,
        0.4685, 0.4695, 0.4705, 0.4715, 0.4725, 0.4735, 0.4745, 0.4755, 0.4765,
        0.4775, 0.4785, 0.4795, 0.4805, 0.4815, 0.4825, 0.4835, 0.4845, 0.4855,
        0.4865, 0.4875, 0.4885, 0.4895, 0.4905, 0.4915, 0.4925, 0.4935, 0.4945,
        0.4955, 0.4965, 0.4975, 0.4985, 0.4995, 0.5005, 0.5015, 0.5025, 0.5035,
        0.5045, 0.5055, 0.5065, 0.5075, 0.5085, 0.5095, 0.5105, 0.5115, 0.5125,
        0.5135, 0.5145, 0.5155, 0.5165, 0.5175, 0.5185, 0.5195, 0.5205, 0.5215,
        0.5225, 0.5235, 0.5245, 0.5255, 0.5265, 0.5275, 0.5285, 0.5295, 0.5305,
        0.5315, 0.5325, 0.5335, 0.5345, 0.5355, 0.5365, 0.5375, 0.5385, 0.5395,
        0.5405, 0.5415, 0.5425, 0.5435, 0.5445, 0.5455, 0.5465, 0.5475, 0.5485,
        0.5495, 0.5506, 0.5516, 0.5526, 0.5536, 0.5546, 0.5556, 0.5566, 0.5576,
        0.5586, 0.5596, 0.5606, 0.5616, 0.5626, 0.5636, 0.5646, 0.5656, 0.5666,
        0.5676, 0.5686, 0.5696, 0.5706, 0.5716, 0.5726, 0.5736, 0.5746, 0.5756,
        0.5766, 0.5776, 0.5786, 0.5796, 0.5806, 0.5816, 0.5826, 0.5836, 0.5846,
        0.5856, 0.5866, 0.5876, 0.5886, 0.5896, 0.5906, 0.5916, 0.5926, 0.5936,
        0.5946, 0.5956, 0.5966, 0.5976, 0.5986, 0.5996, 0.6006, 0.6016, 0.6026,
        0.6036, 0.6046, 0.6056, 0.6066, 0.6076, 0.6086, 0.6096, 0.6106, 0.6116,
        0.6126, 0.6136, 0.6146, 0.6156, 0.6166, 0.6176, 0.6186, 0.6196, 0.6206,
        0.6216, 0.6226, 0.6236, 0.6246, 0.6256, 0.6266, 0.6276, 0.6286, 0.6296,
        0.6306, 0.6316, 0.6326, 0.6336, 0.6346, 0.6356, 0.6366, 0.6376, 0.6386,
        0.6396, 0.6406, 0.6416, 0.6426, 0.6436, 0.6446, 0.6456, 0.6466, 0.6476,
        0.6486, 0.6496, 0.6507, 0.6517, 0.6527, 0.6537, 0.6547, 0.6557, 0.6567,
        0.6577, 0.6587, 0.6597, 0.6607, 0.6617, 0.6627, 0.6637, 0.6647, 0.6657,
        0.6667, 0.6677, 0.6687, 0.6697, 0.6707, 0.6717, 0.6727, 0.6737, 0.6747,
        0.6757, 0.6767, 0.6777, 0.6787, 0.6797, 0.6807, 0.6817, 0.6827, 0.6837,
        0.6847, 0.6857, 0.6867, 0.6877, 0.6887, 0.6897, 0.6907, 0.6917, 0.6927,
        0.6937, 0.6947, 0.6957, 0.6967, 0.6977, 0.6987, 0.6997, 0.7007, 0.7017,
        0.7027, 0.7037, 0.7047, 0.7057, 0.7067, 0.7077, 0.7087, 0.7097, 0.7107,
        0.7117, 0.7127, 0.7137, 0.7147, 0.7157, 0.7167, 0.7177, 0.7187, 0.7197,
        0.7207, 0.7217, 0.7227, 0.7237, 0.7247, 0.7257, 0.7267, 0.7277, 0.7287,
        0.7297, 0.7307, 0.7317, 0.7327, 0.7337, 0.7347, 0.7357, 0.7367, 0.7377,
        0.7387, 0.7397, 0.7407, 0.7417, 0.7427, 0.7437, 0.7447, 0.7457, 0.7467,
        0.7477, 0.7487, 0.7497, 0.7508, 0.7518, 0.7528, 0.7538, 0.7548, 0.7558,
        0.7568, 0.7578, 0.7588, 0.7598, 0.7608, 0.7618, 0.7628, 0.7638, 0.7648,
        0.7658, 0.7668, 0.7678, 0.7688, 0.7698, 0.7708, 0.7718, 0.7728, 0.7738,
        0.7748, 0.7758, 0.7768, 0.7778, 0.7788, 0.7798, 0.7808, 0.7818, 0.7828,
        0.7838, 0.7848, 0.7858, 0.7868, 0.7878, 0.7888, 0.7898, 0.7908, 0.7918,
        0.7928, 0.7938, 0.7948, 0.7958, 0.7968, 0.7978, 0.7988, 0.7998, 0.8008,
        0.8018, 0.8028, 0.8038, 0.8048, 0.8058, 0.8068, 0.8078, 0.8088, 0.8098,
        0.8108, 0.8118, 0.8128, 0.8138, 0.8148, 0.8158, 0.8168, 0.8178, 0.8188,
        0.8198, 0.8208, 0.8218, 0.8228, 0.8238, 0.8248, 0.8258, 0.8268, 0.8278,
        0.8288, 0.8298, 0.8308, 0.8318, 0.8328, 0.8338, 0.8348, 0.8358, 0.8368,
        0.8378, 0.8388, 0.8398, 0.8408, 0.8418, 0.8428, 0.8438, 0.8448, 0.8458,
        0.8468, 0.8478, 0.8488, 0.8498, 0.8509, 0.8519, 0.8529, 0.8539, 0.8549,
        0.8559, 0.8569, 0.8579, 0.8589, 0.8599, 0.8609, 0.8619, 0.8629, 0.8639,
        0.8649, 0.8659, 0.8669, 0.8679, 0.8689, 0.8699, 0.8709, 0.8719, 0.8729,
        0.8739, 0.8749, 0.8759, 0.8769, 0.8779, 0.8789, 0.8799, 0.8809, 0.8819,
        0.8829, 0.8839, 0.8849, 0.8859, 0.8869, 0.8879, 0.8889, 0.8899, 0.8909,
        0.8919, 0.8929, 0.8939, 0.8949, 0.8959, 0.8969, 0.8979, 0.8989, 0.8999,
        0.9009, 0.9019, 0.9029, 0.9039, 0.9049, 0.9059, 0.9069, 0.9079, 0.9089,
        0.9099, 0.9109, 0.9119, 0.9129, 0.9139, 0.9149, 0.9159, 0.9169, 0.9179,
        0.9189, 0.9199, 0.9209, 0.9219, 0.9229, 0.9239, 0.9249, 0.9259, 0.9269,
        0.9279, 0.9289, 0.9299, 0.9309, 0.9319, 0.9329, 0.9339, 0.9349, 0.9359,
        0.9369, 0.9379, 0.9389, 0.9399, 0.9409, 0.9419, 0.9429, 0.9439, 0.9449,
        0.9459, 0.9469, 0.9479, 0.9489, 0.9499, 0.9510, 0.9520, 0.9530, 0.9540,
        0.9550, 0.9560, 0.9570, 0.9580, 0.9590, 0.9600, 0.9610, 0.9620, 0.9630,
        0.9640, 0.9650, 0.9660, 0.9670, 0.9680, 0.9690, 0.9700, 0.9710, 0.9720,
        0.9730, 0.9740, 0.9750, 0.9760, 0.9770, 0.9780, 0.9790, 0.9800, 0.9810,
        0.9820, 0.9830, 0.9840, 0.9850, 0.9860, 0.9870, 0.9880, 0.9890, 0.9900,
        0.9910, 0.9920, 0.9930, 0.9940, 0.9950, 0.9960, 0.9970, 0.9980, 0.9990,
        1.0000], device='cuda:0') - Variance: tensor([[1.0000e-06],
        [1.0000e-06],
        [9.2069e-03],
        [1.0000e-06],
        [1.0000e-06],
        [3.5374e-03],
        [1.0000e-06],
        [1.0000e-06],
        [1.0000e-06],
        [1.0000e-06],
        [1.0000e-06],
        [1.0000e-06],
        [1.0000e-06],
        [1.0000e-06],
        [1.0000e-06],
        [1.0000e-06],
        [1.0000e-06],
        [1.0000e-06],
        [1.0000e-06],
        [1.0000e-06],
        [1.0000e-06],
        [1.0000e-06],
        [1.0000e-06],
        [7.3409e-05],
        [1.0000e-06],
        [1.0000e-06],
        [1.0000e-06],
        [1.0000e-06],
        [1.0000e-06],
        [1.0000e-06],
        [1.0000e-06],
        [1.0000e-06],
        [1.0000e-06],
        [1.0000e-06],
        [1.0000e-06],
        [1.0000e-06],
        [1.0000e-06],
        [1.0000e-06],
        [1.0000e-06],
        [1.0000e-06],
        [1.0000e-06],
        [1.0000e-06],
        [1.0000e-06],
        [1.0000e-06],
        [1.0000e-06],
        [1.0000e-06],
        [1.0000e-06],
        [1.0000e-06],
        [1.8746e-01],
        [1.0000e-06],
        [5.0333e-02],
        [1.0000e-06],
        [1.2129e-01],
        [1.0000e-06],
        [1.0000e-06],
        [1.0000e-06],
        [1.0000e-06],
        [1.4277e-01],
        [1.3914e-02],
        [1.6295e-01],
        [2.6945e-03],
        [1.7402e-01],
        [1.0000e-06],
        [1.7768e-01],
        [2.9224e-01],
        [1.0000e-06],
        [2.8676e-01],
        [1.0000e-06],
        [2.7594e-01],
        [1.0000e-06],
        [2.5900e-01],
        [1.0000e-06],
        [2.3438e-01],
        [3.2506e-01],
        [1.9951e-01],
        [2.8825e-01],
        [1.5054e-01],
        [4.0679e-01],
        [8.1986e-02],
        [3.5059e-01],
        [1.0000e-06],
        [2.7205e-01],
        [4.7283e-01],
        [1.6248e-01],
        [3.8323e-01],
        [2.3022e-01],
        [4.2392e-01],
        [6.5196e-02],
        [2.8935e-01],
        [9.5164e-02],
        [1.0314e-01],
        [3.0403e-01],
        [1.0259e-01],
        [2.9656e-01],
        [8.7553e-02],
        [7.2666e-02],
        [1.0000e-06],
        [3.9505e-02],
        [2.3429e-01],
        [1.0000e-06],
        [1.0000e-06],
        [1.0000e-06],
        [4.2066e-02],
        [1.5849e-02],
        [6.7820e-03],
        [8.4716e-03],
        [9.7719e-03],
        [1.0700e-02],
        [1.0906e-02],
        [1.1095e-02],
        [1.0782e-02],
        [2.6539e-02],
        [9.6828e-03],
        [8.2475e-03],
        [6.2094e-03],
        [3.4222e-03],
        [1.0000e-06],
        [1.0000e-06],
        [1.0000e-06],
        [1.2519e-01],
        [9.2270e-02],
        [5.6741e-02],
        [1.3419e-01],
        [9.6906e-02],
        [1.6742e-01],
        [1.2709e-01],
        [8.1903e-02],
        [1.4543e-01],
        [2.8716e-01],
        [3.3004e-01],
        [2.8197e-01],
        [3.1763e-01],
        [2.5980e-01],
        [2.8735e-01],
        [3.0903e-01],
        [2.3448e-01],
        [4.0099e-01],
        [4.0517e-01],
        [4.0490e-01],
        [3.1969e-01],
        [3.0969e-01],
        [2.9495e-01],
        [2.7553e-01],
        [2.5142e-01],
        [2.2261e-01],
        [2.6016e-01],
        [2.2453e-01],
        [1.8439e-01],
        [1.3963e-01],
        [9.0092e-02],
        [3.5588e-02],
        [1.0000e-06],
        [3.9482e-02],
        [1.7383e-01],
        [1.1887e-01],
        [5.9397e-02],
        [2.4572e-03],
        [4.7693e-03],
        [7.3685e-03],
        [8.9011e-03],
        [1.0042e-02],
        [1.1338e-01],
        [4.9826e-02],
        [1.1353e-02],
        [3.6483e-02],
        [9.5635e-03],
        [2.1005e-02],
        [5.2405e-03],
        [2.9298e-03],
        [1.0000e-06],
        [1.7467e-01],
        [1.0948e-01],
        [1.5149e-01],
        [1.9011e-01],
        [1.2148e-01],
        [1.5774e-01],
        [1.9030e-01],
        [1.1444e-01],
        [3.0363e-01],
        [3.2284e-01],
        [3.3854e-01],
        [3.5066e-01],
        [2.7301e-01],
        [2.7830e-01],
        [2.7915e-01],
        [2.7538e-01],
        [2.6677e-01],
        [3.8676e-01],
        [3.7025e-01],
        [3.4904e-01],
        [3.2281e-01],
        [2.9124e-01],
        [2.5392e-01],
        [3.0390e-01],
        [2.5966e-01],
        [3.4456e-01],
        [2.9634e-01],
        [2.4189e-01],
        [2.7761e-01],
        [2.1701e-01],
        [1.4923e-01],
        [1.8307e-01],
        [1.0857e-01],
        [2.5535e-02],
        [2.1643e-01],
        [1.4032e-01],
        [1.6713e-01],
        [8.3573e-02],
        [1.0954e-01],
        [1.7340e-02],
        [4.2249e-02],
        [6.4987e-02],
        [1.3413e-01],
        [1.5125e-01],
        [1.6631e-01],
        [6.9194e-02],
        [8.1396e-02],
        [9.1063e-02],
        [3.9565e-02],
        [4.4158e-02],
        [1.0000e-06],
        [1.4582e-01],
        [1.4085e-01],
        [1.3275e-01],
        [1.2138e-01],
        [1.0657e-01],
        [8.8134e-02],
        [6.5888e-02],
        [3.9616e-02],
        [1.6353e-01],
        [1.3334e-01],
        [9.9082e-02],
        [1.1777e-01],
        [7.7186e-02],
        [3.1952e-02],
        [4.3763e-02],
        [6.5780e-05],
        [6.3144e-03],
        [9.9448e-02],
        [1.0124e-01],
        [1.0102e-01],
        [4.0303e-02],
        [3.5797e-02],
        [2.9096e-02],
        [2.0101e-02],
        [8.6724e-03],
        [1.4360e-01],
        [1.2872e-01],
        [1.1109e-01],
        [9.0436e-02],
        [6.6382e-02],
        [9.7121e-02],
        [6.6917e-02],
        [3.2172e-02],
        [5.3706e-02],
        [1.5215e-01],
        [1.6367e-01],
        [1.1780e-01],
        [1.2251e-01],
        [1.2356e-01],
        [6.3933e-02],
        [5.7418e-02],
        [4.7157e-02],
        [1.6587e-01],
        [1.5005e-01],
        [1.3107e-01],
        [1.0895e-01],
        [8.3688e-02],
        [5.5285e-02],
        [2.3729e-02],
        [6.4739e-03],
        [1.4986e-02],
        [1.1276e-01],
        [7.6324e-02],
        [9.5885e-02],
        [5.6564e-02],
        [7.4672e-02],
        [3.2744e-02],
        [4.9870e-02],
        [6.6123e-02],
        [2.0295e-01],
        [1.6477e-01],
        [1.7779e-01],
        [1.9025e-01],
        [2.0217e-01],
        [2.1353e-01],
        [2.2431e-01],
        [2.3447e-01],
        [3.4336e-01],
        [3.9044e-01],
        [3.9661e-01],
        [4.0187e-01],
        [4.0606e-01],
        [4.4513e-01],
        [4.4645e-01],
        [4.7999e-01],
        [4.7773e-01],
        [5.6874e-01],
        [5.8917e-01],
        [5.8052e-01],
        [5.9524e-01],
        [6.0643e-01],
        [6.1407e-01],
        [6.4176e-01],
        [6.3010e-01],
        [6.3788e-01],
        [6.3025e-01],
        [6.4068e-01],
        [6.2516e-01],
        [6.1754e-01],
        [5.9356e-01],
        [5.7810e-01],
        [5.8253e-01],
        [5.6025e-01],
        [5.3412e-01],
        [5.0401e-01],
        [4.9718e-01],
        [4.6031e-01],
        [4.1905e-01],
        [3.9257e-01],
        [3.7647e-01],
        [3.4550e-01],
        [3.1207e-01],
        [2.7612e-01],
        [2.7585e-01],
        [2.3671e-01],
        [1.9499e-01],
        [1.5060e-01],
        [1.3120e-01],
        [1.2844e-01],
        [1.0802e-01],
        [8.6959e-02],
        [3.5404e-02],
        [6.1548e-02],
        [3.9036e-02],
        [1.5919e-02],
        [2.3380e-02],
        [4.9574e-02],
        [2.6584e-02],
        [3.3872e-02],
        [4.1091e-02],
        [6.6732e-02],
        [7.3690e-02],
        [8.0594e-02],
        [8.7445e-02],
        [9.4246e-02],
        [1.7222e-01],
        [1.7840e-01],
        [1.8453e-01],
        [2.1570e-01],
        [2.8329e-01],
        [2.8865e-01],
        [3.1587e-01],
        [3.4205e-01],
        [3.9884e-01],
        [4.2187e-01],
        [4.6130e-01],
        [4.8195e-01],
        [5.2674e-01],
        [5.5907e-01],
        [5.8264e-01],
        [6.1116e-01],
        [6.5594e-01],
        [6.7946e-01],
        [7.0137e-01],
        [7.2175e-01],
        [7.4482e-01],
        [7.7418e-01],
        [7.9282e-01],
        [8.1288e-01],
        [8.2816e-01],
        [8.5250e-01],
        [8.6427e-01],
        [8.7691e-01],
        [8.8995e-01],
        [9.0476e-01],
        [9.1422e-01],
        [9.2231e-01],
        [9.2911e-01],
        [9.3788e-01],
        [9.4199e-01],
        [9.4592e-01],
        [9.4866e-01],
        [9.5022e-01],
        [9.5260e-01],
        [9.5251e-01],
        [9.5040e-01],
        [9.4790e-01],
        [9.4586e-01],
        [9.4076e-01],
        [9.3534e-01],
        [9.2857e-01],
        [9.2236e-01],
        [9.1273e-01],
        [9.0139e-01],
        [8.8815e-01],
        [8.7770e-01],
        [8.6282e-01],
        [8.4593e-01],
        [8.2956e-01],
        [8.1034e-01],
        [7.8992e-01],
        [7.6790e-01],
        [7.4482e-01],
        [7.2021e-01],
        [6.9477e-01],
        [6.6794e-01],
        [6.3774e-01],
        [6.0900e-01],
        [5.7682e-01],
        [5.4687e-01],
        [5.1347e-01],
        [4.8315e-01],
        [4.4508e-01],
        [4.1517e-01],
        [3.7706e-01],
        [3.4866e-01],
        [3.1169e-01],
        [2.7264e-01],
        [2.4548e-01],
        [2.0895e-01],
        [1.8585e-01],
        [1.5314e-01],
        [1.3525e-01],
        [1.0756e-01],
        [8.8712e-02],
        [6.6907e-02],
        [5.4666e-02],
        [3.2052e-02],
        [2.7032e-02],
        [1.5435e-02],
        [1.8076e-02],
        [1.1513e-02],
        [1.6794e-02],
        [1.6734e-02],
        [3.0863e-02],
        [3.4605e-02],
        [4.5860e-02],
        [6.6937e-02],
        [8.1436e-02],
        [1.0877e-01],
        [1.2949e-01],
        [1.5872e-01],
        [1.8473e-01],
        [2.1829e-01],
        [2.4545e-01],
        [2.8220e-01],
        [3.0988e-01],
        [3.4867e-01],
        [3.7872e-01],
        [4.1287e-01],
        [4.4734e-01],
        [4.8083e-01],
        [5.1424e-01],
        [5.4732e-01],
        [5.7900e-01],
        [6.1005e-01],
        [6.4026e-01],
        [6.6883e-01],
        [6.9638e-01],
        [7.2222e-01],
        [7.4693e-01],
        [7.7040e-01],
        [7.9145e-01],
        [8.1202e-01],
        [8.3113e-01],
        [8.4882e-01],
        [8.6526e-01],
        [8.8048e-01],
        [8.9426e-01],
        [9.0691e-01],
        [9.1839e-01],
        [9.2876e-01],
        [9.3816e-01],
        [9.4659e-01],
        [9.5409e-01],
        [9.6082e-01],
        [9.6670e-01],
        [9.7201e-01],
        [9.7667e-01],
        [9.8078e-01],
        [9.8438e-01],
        [9.8752e-01],
        [9.9024e-01],
        [9.9260e-01],
        [9.9464e-01],
        [9.9637e-01],
        [9.9789e-01],
        [9.9917e-01],
        [1.0003e+00],
        [1.0012e+00],
        [1.0020e+00],
        [1.0026e+00],
        [1.0032e+00],
        [1.0036e+00],
        [1.0040e+00],
        [1.0043e+00],
        [1.0046e+00],
        [1.0048e+00],
        [1.0050e+00],
        [1.0051e+00],
        [1.0052e+00],
        [1.0053e+00],
        [1.0054e+00],
        [1.0054e+00],
        [1.0055e+00],
        [1.0055e+00],
        [1.0055e+00],
        [1.0055e+00],
        [1.0054e+00],
        [1.0054e+00],
        [1.0053e+00],
        [1.0052e+00],
        [1.0051e+00],
        [1.0049e+00],
        [1.0047e+00],
        [1.0045e+00],
        [1.0042e+00],
        [1.0039e+00],
        [1.0035e+00],
        [1.0030e+00],
        [1.0024e+00],
        [1.0017e+00],
        [1.0009e+00],
        [9.9992e-01],
        [9.9877e-01],
        [9.9742e-01],
        [9.9584e-01],
        [9.9399e-01],
        [9.9185e-01],
        [9.8937e-01],
        [9.8652e-01],
        [9.8322e-01],
        [9.7946e-01],
        [9.7519e-01],
        [9.7033e-01],
        [9.6482e-01],
        [9.5884e-01],
        [9.5185e-01],
        [9.4407e-01],
        [9.3545e-01],
        [9.2577e-01],
        [9.1512e-01],
        [9.0324e-01],
        [8.9027e-01],
        [8.7616e-01],
        [8.6061e-01],
        [8.4383e-01],
        [8.2581e-01],
        [8.0618e-01],
        [7.8527e-01],
        [7.6465e-01],
        [7.3874e-01],
        [7.1605e-01],
        [6.8742e-01],
        [6.6170e-01],
        [6.3064e-01],
        [6.0193e-01],
        [5.6889e-01],
        [5.3911e-01],
        [5.0291e-01],
        [4.7281e-01],
        [4.3593e-01],
        [4.0418e-01],
        [3.7559e-01],
        [3.3723e-01],
        [3.0820e-01],
        [2.7150e-01],
        [2.4557e-01],
        [2.0870e-01],
        [1.8377e-01],
        [1.5060e-01],
        [1.3413e-01],
        [1.0602e-01],
        [8.8692e-02],
        [6.6479e-02],
        [5.5842e-02],
        [3.2832e-02],
        [2.9471e-02],
        [1.3605e-02],
        [1.7925e-02],
        [1.1513e-02],
        [2.1739e-02],
        [3.3648e-02],
        [3.3166e-02],
        [5.2415e-02],
        [5.9365e-02],
        [8.5314e-02],
        [9.9135e-02],
        [1.2399e-01],
        [1.4399e-01],
        [1.7414e-01],
        [1.9933e-01],
        [2.3360e-01],
        [2.6282e-01],
        [2.9439e-01],
        [3.2663e-01],
        [3.6055e-01],
        [3.9459e-01],
        [4.2963e-01],
        [4.6427e-01],
        [4.9532e-01],
        [5.3205e-01],
        [5.6614e-01],
        [5.8730e-01],
        [6.2343e-01],
        [6.5645e-01],
        [6.8661e-01],
        [7.0668e-01],
        [7.3251e-01],
        [7.5610e-01],
        [7.8120e-01],
        [7.9858e-01],
        [8.1941e-01],
        [8.4074e-01],
        [8.5733e-01],
        [8.7094e-01],
        [8.8637e-01],
        [9.0004e-01],
        [9.1214e-01],
        [9.2414e-01],
        [9.3278e-01],
        [9.4214e-01],
        [9.5029e-01],
        [9.5741e-01],
        [9.6319e-01],
        [9.6865e-01],
        [9.7391e-01],
        [9.7885e-01],
        [9.8207e-01],
        [9.8575e-01],
        [9.8858e-01],
        [9.9125e-01],
        [9.9338e-01],
        [9.9530e-01],
        [9.9706e-01],
        [9.9853e-01],
        [9.9974e-01],
        [1.0006e+00],
        [1.0015e+00],
        [1.0023e+00],
        [1.0029e+00],
        [1.0034e+00],
        [1.0038e+00],
        [1.0042e+00],
        [1.0045e+00],
        [1.0047e+00],
        [1.0049e+00],
        [1.0051e+00],
        [1.0052e+00],
        [1.0053e+00],
        [1.0054e+00],
        [1.0055e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0056e+00],
        [1.0055e+00],
        [1.0055e+00],
        [1.0054e+00],
        [1.0054e+00],
        [1.0053e+00],
        [1.0051e+00],
        [1.0051e+00],
        [1.0049e+00],
        [1.0047e+00],
        [1.0043e+00],
        [1.0039e+00],
        [1.0034e+00],
        [1.0032e+00],
        [1.0025e+00],
        [1.0021e+00],
        [1.0012e+00],
        [1.0007e+00],
        [9.9790e-01],
        [9.9707e-01],
        [9.9614e-01],
        [9.9370e-01],
        [9.9241e-01],
        [9.9099e-01],
        [9.8724e-01],
        [9.8527e-01],
        [9.8309e-01],
        [9.7449e-01],
        [9.7117e-01],
        [9.6239e-01],
        [9.5777e-01],
        [9.5267e-01],
        [9.4702e-01],
        [9.4077e-01],
        [9.3385e-01],
        [9.0660e-01],
        [8.9606e-01],
        [8.8439e-01],
        [8.7148e-01],
        [8.5719e-01],
        [8.6071e-01],
        [8.4528e-01],
        [8.2821e-01],
        [8.0932e-01],
        [7.3489e-01],
        [7.4129e-01],
        [7.1316e-01],
        [6.8204e-01],
        [6.8969e-01],
        [6.5607e-01],
        [6.1888e-01],
        [6.2803e-01],
        [4.8490e-01],
        [4.9721e-01],
        [4.4312e-01],
        [4.5642e-01],
        [3.9799e-01],
        [4.1236e-01],
        [3.4925e-01],
        [3.6477e-01],
        [3.7992e-01],
        [1.4279e-01],
        [1.6319e-01],
        [1.8311e-01],
        [9.5614e-02],
        [1.1713e-01],
        [1.3814e-01],
        [1.5865e-01],
        [1.7868e-01],
        [1.1513e-02],
        [1.1513e-02],
        [1.1513e-02],
        [1.1513e-02],
        [1.1513e-02],
        [1.1513e-02],
        [1.7805e-02],
        [4.1162e-02],
        [1.1513e-02],
        [1.1513e-02],
        [1.8287e-02],
        [4.1633e-02],
        [6.4427e-02],
        [8.6682e-02],
        [2.1385e-01],
        [2.3258e-01],
        [2.5086e-01],
        [8.7131e-02],
        [2.1424e-01],
        [2.3295e-01],
        [2.5122e-01],
        [3.5563e-01],
        [3.7100e-01],
        [4.5883e-01],
        [4.7176e-01],
        [4.3230e-01],
        [4.4586e-01],
        [5.2333e-01],
        [5.9009e-01],
        [5.9992e-01],
        [6.5607e-01],
        [6.6434e-01],
        [7.1158e-01],
        [7.5229e-01],
        [7.3356e-01],
        [7.4000e-01],
        [7.7677e-01],
        [8.0846e-01],
        [8.3576e-01],
        [8.5928e-01],
        [8.7955e-01],
        [8.9702e-01],
        [8.7343e-01],
        [8.9174e-01],
        [9.0752e-01],
        [9.3106e-01],
        [9.4140e-01],
        [9.5031e-01],
        [9.5798e-01],
        [9.6459e-01],
        [9.7029e-01],
        [9.6767e-01],
        [9.7294e-01],
        [9.8081e-01],
        [9.8852e-01],
        [9.8724e-01],
        [9.8587e-01],
        [9.9201e-01],
        [9.9427e-01],
        [9.9782e-01],
        [9.9723e-01],
        [9.9987e-01],
        [9.9943e-01],
        [1.0005e+00],
        [1.0021e+00],
        [1.0018e+00],
        [1.0037e+00],
        [1.0035e+00],
        [1.0042e+00],
        [1.0041e+00],
        [1.0049e+00],
        [1.0048e+00],
        [1.0047e+00],
        [1.0052e+00],
        [1.0052e+00],
        [1.0054e+00],
        [1.0054e+00],
        [1.0055e+00],
        [1.0056e+00],
        [1.0056e+00],
        [1.0057e+00],
        [1.0056e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0057e+00],
        [1.0058e+00],
        [1.0057e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00],
        [1.0058e+00]], device='cuda:0') 
Smoothed_batch_entropy:  tensor([-3.4157e-02, -1.3877e-02,  2.6128e-03, -1.4045e-02, -8.9656e-03,
         1.1113e-03, -9.0243e-03, -4.6877e-03, -1.1679e-02, -4.8048e-03,
        -1.0212e-02, -1.0545e-02, -9.7942e-03, -9.9587e-03, -9.8219e-03,
        -9.8876e-03, -9.6757e-03, -1.0791e-02, -3.4078e-03, -1.2666e-02,
        -1.3789e-02, -2.2517e-02, -1.6516e-02,  1.9606e-05, -7.9207e-03,
        -1.8042e-03, -5.0679e-03, -4.2753e-03, -5.8708e-03, -7.1618e-03,
        -5.0494e-03, -1.0581e-02, -8.7815e-03, -1.4622e-02, -1.2737e-02,
        -1.9524e-02, -1.7000e-02, -2.5667e-02, -2.1888e-02, -4.9183e-03,
        -2.7791e-02, -1.0034e-02, -1.5712e-02, -1.1164e-03, -2.3173e-02,
        -7.4686e-03, -2.6382e-02, -1.5404e-02,  1.0002e-02, -9.0502e-03,
         2.7541e-03, -1.0333e-02,  6.7594e-03, -5.7611e-03, -1.8179e-03,
        -2.6104e-03, -1.3939e-04,  8.4965e-03,  8.5518e-04,  9.9461e-03,
         1.6631e-04,  1.0843e-02, -2.4336e-05,  1.1323e-02,  1.8463e-02,
        -2.6521e-03,  1.8410e-02, -5.0160e-03,  1.8005e-02, -6.4799e-03,
         1.7123e-02, -8.7071e-03,  1.5714e-02,  2.1417e-02,  1.3546e-02,
         1.9179e-02,  1.0301e-02,  2.6247e-02,  5.6695e-03,  2.2768e-02,
        -9.4264e-04,  1.7756e-02,  2.9319e-02,  1.0667e-02,  2.3855e-02,
         1.4647e-02,  2.5713e-02,  4.1720e-03,  1.7609e-02,  5.9153e-03,
         6.3278e-03,  1.7877e-02,  6.1332e-03,  1.7100e-02,  5.1432e-03,
         4.2184e-03, -3.4902e-03,  2.2635e-03,  1.2999e-02, -1.6519e-04,
        -7.5380e-04, -4.4294e-04,  2.3188e-03,  8.8416e-04,  3.6658e-04,
         4.6539e-04,  5.3332e-04,  5.7184e-04,  5.9480e-04,  6.0048e-04,
         5.6857e-04,  1.3811e-03,  5.2033e-04,  4.2789e-04,  3.1886e-04,
         1.8462e-04, -1.0319e-05, -2.7939e-04, -4.2058e-04,  6.4684e-03,
         4.7658e-03,  2.9528e-03,  6.9453e-03,  5.0326e-03,  8.6508e-03,
         6.6189e-03,  4.2697e-03,  7.5893e-03,  1.4894e-02,  1.7090e-02,
         1.4721e-02,  1.6573e-02,  1.3651e-02,  1.5130e-02,  1.6294e-02,
         1.2507e-02,  2.1077e-02,  2.1370e-02,  2.1397e-02,  1.7110e-02,
         1.6626e-02,  1.5897e-02,  1.4911e-02,  1.3678e-02,  1.2170e-02,
         1.4145e-02,  1.2274e-02,  1.0142e-02,  7.7060e-03,  5.0070e-03,
         1.9812e-03, -1.3588e-03,  2.1799e-03,  9.4966e-03,  6.5283e-03,
         3.2636e-03,  1.3604e-04,  2.5651e-04,  4.1551e-04,  4.9505e-04,
         5.3779e-04,  6.0786e-03,  2.6768e-03,  6.0375e-04,  1.9688e-03,
         5.0314e-04,  1.1163e-03,  2.7147e-04,  1.6029e-04, -1.4094e-04,
         8.8433e-03,  5.5572e-03,  7.5852e-03,  9.3964e-03,  5.9841e-03,
         7.6981e-03,  9.1717e-03,  5.5040e-03,  1.4275e-02,  1.5023e-02,
         1.5607e-02,  1.6029e-02,  1.2398e-02,  1.2548e-02,  1.2475e-02,
         1.2208e-02,  1.1760e-02,  1.6861e-02,  1.6069e-02,  1.5053e-02,
         1.3866e-02,  1.2484e-02,  1.0849e-02,  1.2960e-02,  1.1069e-02,
         1.4675e-02,  1.2648e-02,  1.0344e-02,  1.1908e-02,  9.3636e-03,
         6.4802e-03,  7.9999e-03,  4.8023e-03,  1.1233e-03,  9.7112e-03,
         6.3724e-03,  7.7034e-03,  3.9006e-03,  5.2150e-03,  8.3721e-04,
         2.0813e-03,  3.2649e-03,  6.8504e-03,  7.8749e-03,  8.7920e-03,
         3.8212e-03,  4.5706e-03,  5.2660e-03,  2.3862e-03,  2.7131e-03,
        -1.0518e-03,  9.3017e-03,  9.2368e-03,  8.9658e-03,  8.4905e-03,
         7.6978e-03,  6.6031e-03,  5.1045e-03,  3.1833e-03,  1.2886e-02,
         1.0900e-02,  8.4327e-03,  1.0138e-02,  6.9107e-03,  2.9946e-03,
         4.1757e-03,  2.9925e-05,  6.1759e-04,  9.5433e-03,  9.7825e-03,
         9.8412e-03,  4.1345e-03,  3.6607e-03,  2.9646e-03,  2.0820e-03,
         8.5364e-04,  1.3605e-02,  1.2216e-02,  1.0549e-02,  8.5993e-03,
         6.3428e-03,  8.9511e-03,  6.1623e-03,  2.9441e-03,  4.7663e-03,
         1.2474e-02,  1.3021e-02,  9.3781e-03,  9.4380e-03,  9.2713e-03,
         4.7533e-03,  4.1584e-03,  3.3229e-03,  1.0846e-02,  9.5742e-03,
         8.1693e-03,  6.6684e-03,  4.9933e-03,  3.2421e-03,  1.3563e-03,
         3.5067e-04,  8.2139e-04,  5.8281e-03,  3.8713e-03,  4.7587e-03,
         2.7712e-03,  3.5595e-03,  1.5124e-03,  2.2778e-03,  2.9555e-03,
         8.7766e-03,  6.9981e-03,  7.4405e-03,  7.8078e-03,  8.1528e-03,
         8.4510e-03,  8.7450e-03,  8.9745e-03,  1.2894e-02,  1.4438e-02,
         1.4444e-02,  1.4403e-02,  1.4326e-02,  1.5480e-02,  1.5270e-02,
         1.6205e-02,  1.5845e-02,  1.8728e-02,  1.9151e-02,  1.8540e-02,
         1.8760e-02,  1.8847e-02,  1.8829e-02,  1.9506e-02,  1.8820e-02,
         1.8842e-02,  1.8343e-02,  1.8483e-02,  1.7759e-02,  1.7332e-02,
         1.6387e-02,  1.5760e-02,  1.5805e-02,  1.4995e-02,  1.4102e-02,
         1.3128e-02,  1.2877e-02,  1.1749e-02,  1.0546e-02,  9.7778e-03,
         9.3238e-03,  8.4726e-03,  7.5703e-03,  6.6286e-03,  6.6115e-03,
         5.6188e-03,  4.5826e-03,  3.4923e-03,  3.0351e-03,  2.9560e-03,
         2.4863e-03,  1.9801e-03,  7.9293e-04,  1.3940e-03,  8.7823e-04,
         3.6056e-04,  5.1810e-04,  1.1206e-03,  6.0048e-04,  7.5423e-04,
         9.2862e-04,  1.5030e-03,  1.6639e-03,  1.8243e-03,  1.9736e-03,
         2.1325e-03,  3.9487e-03,  4.0926e-03,  4.2356e-03,  4.9909e-03,
         6.6538e-03,  6.7785e-03,  7.4634e-03,  8.1225e-03,  9.6044e-03,
         1.0205e-02,  1.1264e-02,  1.1818e-02,  1.3076e-02,  1.3993e-02,
         1.4670e-02,  1.5508e-02,  1.6885e-02,  1.7592e-02,  1.8275e-02,
         1.8900e-02,  1.9632e-02,  2.0615e-02,  2.1219e-02,  2.1894e-02,
         2.2384e-02,  2.3271e-02,  2.3633e-02,  2.4040e-02,  2.4481e-02,
         2.5019e-02,  2.5309e-02,  2.5535e-02,  2.5696e-02,  2.5988e-02,
         2.6012e-02,  2.6043e-02,  2.6011e-02,  2.5913e-02,  2.5899e-02,
         2.5737e-02,  2.5442e-02,  2.5155e-02,  2.4938e-02,  2.4524e-02,
         2.4126e-02,  2.3670e-02,  2.3291e-02,  2.2721e-02,  2.2093e-02,
         2.1405e-02,  2.0910e-02,  2.0219e-02,  1.9473e-02,  1.8795e-02,
         1.8035e-02,  1.7270e-02,  1.6485e-02,  1.5705e-02,  1.4907e-02,
         1.4122e-02,  1.3333e-02,  1.2486e-02,  1.1711e-02,  1.0882e-02,
         1.0145e-02,  9.3492e-03,  8.6602e-03,  7.8261e-03,  7.1995e-03,
         6.4166e-03,  5.8602e-03,  5.1549e-03,  4.4330e-03,  3.9418e-03,
         3.3106e-03,  2.9148e-03,  2.3661e-03,  2.0844e-03,  1.6362e-03,
         1.3363e-03,  1.0013e-03,  8.1245e-04,  4.7269e-04,  4.0055e-04,
         2.2796e-04,  2.6356e-04,  1.7043e-04,  2.4172e-04,  2.4155e-04,
         4.5610e-04,  5.1277e-04,  6.7735e-04,  9.9234e-04,  1.2145e-03,
         1.6387e-03,  1.9559e-03,  2.4325e-03,  2.8528e-03,  3.4065e-03,
         3.8666e-03,  4.5121e-03,  5.0050e-03,  5.7229e-03,  6.2940e-03,
         6.9586e-03,  7.6560e-03,  8.3565e-03,  9.0823e-03,  9.8275e-03,
         1.0574e-02,  1.1333e-02,  1.2103e-02,  1.2871e-02,  1.3638e-02,
         1.4391e-02,  1.5146e-02,  1.5902e-02,  1.6609e-02,  1.7339e-02,
         1.8051e-02,  1.8749e-02,  1.9431e-02,  2.0099e-02,  2.0738e-02,
         2.1361e-02,  2.1960e-02,  2.2533e-02,  2.3084e-02,  2.3610e-02,
         2.4107e-02,  2.4583e-02,  2.5026e-02,  2.5451e-02,  2.5852e-02,
         2.6228e-02,  2.6581e-02,  2.6910e-02,  2.7217e-02,  2.7501e-02,
         2.7765e-02,  2.8003e-02,  2.8228e-02,  2.8433e-02,  2.8617e-02,
         2.8787e-02,  2.8940e-02,  2.9077e-02,  2.9198e-02,  2.9307e-02,
         2.9402e-02,  2.9485e-02,  2.9556e-02,  2.9615e-02,  2.9664e-02,
         2.9272e-02,  2.9179e-02,  2.9077e-02,  2.8966e-02,  2.8845e-02,
         2.8714e-02,  2.8573e-02,  2.8420e-02,  2.8257e-02,  2.8080e-02,
         2.7890e-02,  2.7685e-02,  2.7466e-02,  2.7232e-02,  2.6981e-02,
         2.6713e-02,  2.6428e-02,  2.6121e-02,  2.5797e-02,  2.5454e-02,
         2.5090e-02,  2.4703e-02,  2.4309e-02,  2.3877e-02,  2.3424e-02,
         2.2951e-02,  2.2449e-02,  2.1926e-02,  2.1376e-02,  2.0804e-02,
         2.0213e-02,  1.9593e-02,  1.8955e-02,  1.8301e-02,  1.7622e-02,
         1.6928e-02,  1.6270e-02,  1.5485e-02,  1.4814e-02,  1.4009e-02,
         1.3308e-02,  1.2496e-02,  1.1772e-02,  1.0963e-02,  1.0263e-02,
         9.4402e-03,  8.7697e-03,  7.9744e-03,  7.3090e-03,  6.7279e-03,
         5.9606e-03,  5.3946e-03,  4.6961e-03,  4.2083e-03,  3.5403e-03,
         3.0993e-03,  2.5178e-03,  2.2186e-03,  1.7471e-03,  1.4484e-03,
         1.0797e-03,  9.0669e-04,  5.3057e-04,  4.7545e-04,  2.1747e-04,
         2.8730e-04,  1.8566e-04,  3.4912e-04,  5.4364e-04,  5.2756e-04,
         8.4555e-04,  9.5356e-04,  1.3789e-03,  1.6098e-03,  2.0179e-03,
         2.3553e-03,  2.8764e-03,  3.3116e-03,  3.9142e-03,  4.4376e-03,
         5.0127e-03,  5.6016e-03,  6.2470e-03,  6.9048e-03,  7.5970e-03,
         8.2997e-03,  8.9459e-03,  9.7232e-03,  1.0477e-02,  1.0950e-02,
         1.1789e-02,  1.2583e-02,  1.3334e-02,  1.3845e-02,  1.4521e-02,
         1.5168e-02,  1.5879e-02,  1.6385e-02,  1.7018e-02,  1.7693e-02,
         1.8238e-02,  1.8702e-02,  1.9253e-02,  1.9759e-02,  2.0228e-02,
         2.0717e-02,  2.1082e-02,  2.1498e-02,  2.1877e-02,  2.2225e-02,
         2.2519e-02,  2.2810e-02,  2.3109e-02,  2.3406e-02,  2.3604e-02,
         2.3849e-02,  2.4047e-02,  2.4245e-02,  2.4410e-02,  2.4570e-02,
         2.4726e-02,  2.4863e-02,  2.4985e-02,  2.5075e-02,  2.5170e-02,
         2.5262e-02,  2.5334e-02,  2.5399e-02,  2.5465e-02,  2.5510e-02,
         2.5558e-02,  2.5591e-02,  2.5624e-02,  2.5651e-02,  2.5679e-02,
         2.5690e-02,  2.5701e-02,  2.5713e-02,  2.5721e-02,  2.5718e-02,
         2.5718e-02,  2.5716e-02,  2.5710e-02,  2.5702e-02,  2.5692e-02,
         2.5680e-02,  2.5667e-02,  2.5653e-02,  2.5637e-02,  2.5619e-02,
         2.5601e-02,  2.5583e-02,  2.5562e-02,  2.5542e-02,  2.5521e-02,
         2.5499e-02,  2.5477e-02,  2.5454e-02,  2.5432e-02,  2.5408e-02,
         2.5385e-02,  2.5361e-02,  2.5337e-02,  2.5312e-02,  2.5288e-02,
         2.5264e-02,  2.5239e-02,  2.5215e-02,  2.5190e-02,  2.5165e-02,
         2.5140e-02,  2.5115e-02,  2.5091e-02,  2.5065e-02,  2.5041e-02,
         2.5016e-02,  2.4991e-02,  2.4966e-02,  2.4941e-02,  2.4916e-02,
         2.4891e-02,  2.4866e-02,  2.4842e-02,  2.4817e-02,  2.4792e-02,
         2.4768e-02,  2.4743e-02,  2.4718e-02,  2.4694e-02,  2.4669e-02,
         2.4644e-02,  2.4620e-02,  2.4595e-02,  2.4571e-02,  2.4547e-02,
         2.4522e-02,  2.4498e-02,  2.4473e-02,  2.4449e-02,  2.4425e-02,
         2.4401e-02,  2.4377e-02,  2.4353e-02,  2.4328e-02,  2.4304e-02,
         2.4280e-02,  2.4256e-02,  2.4232e-02,  2.4208e-02,  2.4185e-02,
         2.4161e-02,  2.4137e-02,  2.4113e-02,  2.4089e-02,  2.4066e-02,
         2.4042e-02,  2.4018e-02,  2.3995e-02,  2.3971e-02,  2.3948e-02,
         2.3924e-02,  2.3901e-02,  2.3878e-02,  2.3854e-02,  2.3831e-02,
         2.3808e-02,  2.3785e-02,  2.3762e-02,  2.3739e-02,  2.3716e-02,
         2.3693e-02,  2.3670e-02,  2.3648e-02,  2.3625e-02,  2.3602e-02,
         2.3580e-02,  2.3558e-02,  2.3536e-02,  2.3514e-02,  2.3493e-02,
         2.3471e-02,  2.3450e-02,  2.3429e-02,  2.3408e-02,  2.3387e-02,
         2.3367e-02,  2.3348e-02,  2.3331e-02,  2.3313e-02,  2.3294e-02,
         2.3278e-02,  2.3260e-02,  2.3243e-02,  2.3229e-02,  2.3214e-02,
         2.3200e-02,  2.3197e-02,  2.3186e-02,  2.3176e-02,  2.3168e-02,
         2.3162e-02,  2.3157e-02,  2.3155e-02,  2.3155e-02,  2.3184e-02,
         2.3184e-02,  2.3195e-02,  2.3199e-02,  2.3217e-02,  2.3239e-02,
         2.3238e-02,  2.3268e-02,  2.3303e-02,  2.3374e-02,  2.3427e-02,
         2.3487e-02,  2.3504e-02,  2.3577e-02,  2.3600e-02,  2.3687e-02,
         2.3717e-02,  2.3957e-02,  2.4000e-02,  2.4045e-02,  2.4185e-02,
         2.4239e-02,  2.4296e-02,  2.4462e-02,  2.4526e-02,  2.4594e-02,
         2.4876e-02,  2.4952e-02,  2.5157e-02,  2.5234e-02,  2.5312e-02,
         2.5388e-02,  2.5462e-02,  2.5530e-02,  2.5770e-02,  2.5806e-02,
         2.5830e-02,  2.5835e-02,  2.5820e-02,  2.5786e-02,  2.5751e-02,
         2.5683e-02,  2.5577e-02,  2.4950e-02,  2.4994e-02,  2.4638e-02,
         2.4191e-02,  2.4273e-02,  2.3733e-02,  2.3069e-02,  2.3214e-02,
         2.0031e-02,  2.0321e-02,  1.8824e-02,  1.9181e-02,  1.7440e-02,
         1.7858e-02,  1.5799e-02,  1.6307e-02,  1.6762e-02,  7.4227e-03,
         8.3486e-03,  9.2352e-03,  5.1243e-03,  6.1628e-03,  7.1565e-03,
         8.0848e-03,  8.9741e-03,  6.5242e-04,  6.5121e-04,  6.5018e-04,
         6.4932e-04,  6.4829e-04,  6.4743e-04,  9.6663e-04,  2.2364e-03,
         6.4450e-04,  6.4330e-04,  1.0138e-03,  2.2482e-03,  3.4280e-03,
         4.5309e-03,  1.0265e-02,  1.1001e-02,  1.1707e-02,  4.5275e-03,
         1.0226e-02,  1.0958e-02,  1.1660e-02,  1.5339e-02,  1.5822e-02,
         1.8354e-02,  1.8671e-02,  1.7590e-02,  1.7930e-02,  1.9861e-02,
         2.1255e-02,  2.1425e-02,  2.2358e-02,  2.2471e-02,  2.3090e-02,
         2.3518e-02,  2.3297e-02,  2.3336e-02,  2.3654e-02,  2.3842e-02,
         2.3936e-02,  2.3961e-02,  2.3930e-02,  2.3858e-02,  2.3886e-02,
         2.3828e-02,  2.3737e-02,  2.3539e-02,  2.3401e-02,  2.3255e-02,
         2.3107e-02,  2.2957e-02,  2.2807e-02,  2.2846e-02,  2.2698e-02,
         2.2453e-02,  2.2152e-02,  2.2182e-02,  2.2214e-02,  2.1932e-02,
         2.1795e-02,  2.1559e-02,  2.1578e-02,  2.1366e-02,  2.1381e-02,
         2.1276e-02,  2.1100e-02,  2.1109e-02,  2.0870e-02,  2.0874e-02,
         2.0747e-02,  2.0747e-02,  2.0580e-02,  2.0577e-02,  2.0573e-02,
         2.0437e-02,  2.0430e-02,  2.0321e-02,  2.0312e-02,  2.0250e-02,
         2.0213e-02,  2.0161e-02,  2.0129e-02,  2.0116e-02,  2.0056e-02,
         2.0043e-02,  1.9993e-02,  1.9978e-02,  1.9936e-02,  1.9920e-02,
         1.9884e-02,  1.9862e-02,  1.9846e-02,  1.9816e-02,  1.9799e-02,
         1.9772e-02,  1.9752e-02,  1.9730e-02,  1.9711e-02,  1.9691e-02,
         1.9671e-02,  1.9652e-02,  1.9631e-02,  1.9614e-02,  1.9594e-02,
         1.9576e-02,  1.9557e-02,  1.9540e-02,  1.9522e-02,  1.9504e-02,
         1.9486e-02,  1.9468e-02,  1.9450e-02,  1.9433e-02,  1.9415e-02,
         1.9398e-02,  1.9380e-02,  1.9363e-02,  1.9346e-02,  1.9329e-02,
         1.9312e-02,  1.9294e-02,  1.9277e-02,  1.9260e-02,  1.9243e-02,
         1.9226e-02,  1.9209e-02,  1.9192e-02,  1.9175e-02,  1.9158e-02,
         1.9141e-02,  1.9125e-02,  1.9108e-02,  1.9091e-02,  1.9074e-02,
         1.9057e-02,  1.9041e-02,  1.9024e-02,  1.9007e-02,  1.8991e-02,
         1.8974e-02,  1.8957e-02,  1.8941e-02,  1.8924e-02,  1.8908e-02,
         1.8891e-02,  1.8875e-02,  1.8858e-02,  1.8841e-02,  1.8825e-02,
         1.8809e-02,  1.8792e-02,  1.8776e-02,  1.8759e-02,  1.8743e-02],
       device='cuda:0')
Selected points (indices): {499, 502}
Selected new x values (normalized): tensor([0.4995, 0.5175], device='cuda:0')
Corresponding new x values (unnormalized): tensor([-2.0020, 70.0701], device='cuda:0')
Plot saved to /raven/u/dvoss/al_pmssmwithgp/model/plots/Iter5/gp_plot.png
YAML file has been created: /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/data/new_config.yaml
Note: detected 144 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
Note: NumExpr detected 144 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
NumExpr defaulting to 8 threads.

  ___              ____ __  __          _       _   ___            
 | _ \ _  _  _ _  |__ /|  \/  | ___  __| | ___ | | / __| ___  _ _  
 |   /| || || ' \  |_ \| |\/| |/ _ \/ _` |/ -_)| || (_ |/ -_)| ' \ 
 |_|_\ \_,_||_||_||___/|_|  |_|\___/\__,_|\___||_| \___|\___||_||_|
                                                                   

2024-08-28 19:53:21 [info     ] Initialised ModelGenerator with: [Run3ModelGen.modelgen]
2024-08-28 19:53:21 [info     ] 	config_file = /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/data/new_config.yaml [Run3ModelGen.modelgen]
2024-08-28 19:53:21 [info     ] 	scan_dir = /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/scans/scan_5 [Run3ModelGen.modelgen]
2024-08-28 19:53:21 [info     ] 	rawfilen = /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/data/raw.slha [Run3ModelGen.modelgen]
2024-08-28 19:53:21 [info     ] 	seed = 123                    [Run3ModelGen.modelgen]
2024-08-28 19:53:21 [info     ] 	points = {}                   [Run3ModelGen.modelgen]
2024-08-28 19:53:21 [info     ] 	prior = fixed                 [Run3ModelGen.modelgen]
2024-08-28 19:53:21 [info     ] 	num_models = 2                [Run3ModelGen.modelgen]
2024-08-28 19:53:21 [info     ] 	isGMSB = False                [Run3ModelGen.modelgen]
2024-08-28 19:53:21 [info     ] 	parameters =                  [Run3ModelGen.modelgen]
2024-08-28 19:53:21 [info     ] 		tanb: [60, 60]               [Run3ModelGen.modelgen]
2024-08-28 19:53:21 [info     ] 		M_1: [-2.0, 70.07]           [Run3ModelGen.modelgen]
2024-08-28 19:53:21 [info     ] 		M_2: [2000, 2000]            [Run3ModelGen.modelgen]
2024-08-28 19:53:21 [info     ] 		M_3: [4000, 4000]            [Run3ModelGen.modelgen]
2024-08-28 19:53:21 [info     ] 		AT: [4000, 4000]             [Run3ModelGen.modelgen]
2024-08-28 19:53:21 [info     ] 		Ab: [2000, 2000]             [Run3ModelGen.modelgen]
2024-08-28 19:53:21 [info     ] 		Atau: [2000, 2000]           [Run3ModelGen.modelgen]
2024-08-28 19:53:21 [info     ] 		mu: [2000, 2000]             [Run3ModelGen.modelgen]
2024-08-28 19:53:21 [info     ] 		mA: [2000, 2000]             [Run3ModelGen.modelgen]
2024-08-28 19:53:21 [info     ] 		meL: [2000, 2000]            [Run3ModelGen.modelgen]
2024-08-28 19:53:21 [info     ] 		mtauL: [2000, 2000]          [Run3ModelGen.modelgen]
2024-08-28 19:53:21 [info     ] 		meR: [2000, 2000]            [Run3ModelGen.modelgen]
2024-08-28 19:53:21 [info     ] 		mtauR: [2000, 2000]          [Run3ModelGen.modelgen]
2024-08-28 19:53:21 [info     ] 		mqL1: [4000, 4000]           [Run3ModelGen.modelgen]
2024-08-28 19:53:21 [info     ] 		mqL3: [4000, 4000]           [Run3ModelGen.modelgen]
2024-08-28 19:53:21 [info     ] 		muR: [4000, 4000]            [Run3ModelGen.modelgen]
2024-08-28 19:53:21 [info     ] 		mtR: [4000, 4000]            [Run3ModelGen.modelgen]
2024-08-28 19:53:21 [info     ] 		mdR: [4000, 4000]            [Run3ModelGen.modelgen]
2024-08-28 19:53:21 [info     ] 		mbR: [4000, 4000]            [Run3ModelGen.modelgen]
2024-08-28 19:53:21 [info     ] 	steps =                       [Run3ModelGen.modelgen]
2024-08-28 19:53:21 [info     ] 		{'name': 'prep_input', 'output_dir': 'input', 'prefix': 'IN'} [Run3ModelGen.modelgen]
2024-08-28 19:53:21 [info     ] 		{'name': 'SPheno', 'input_dir': 'input', 'output_dir': 'SPheno', 'log_dir': 'SPheno_log', 'prefix': 'SP'} [Run3ModelGen.modelgen]
2024-08-28 19:53:21 [info     ] 		{'name': 'softsusy', 'input_dir': 'input', 'output_dir': 'softsusy', 'prefix': 'SS'} [Run3ModelGen.modelgen]
2024-08-28 19:53:21 [info     ] 		{'name': 'micromegas', 'input_dir': 'SPheno', 'output_dir': 'micromegas', 'prefix': 'MO'} [Run3ModelGen.modelgen]
2024-08-28 19:53:21 [info     ] 		{'name': 'superiso', 'input_dir': 'SPheno', 'output_dir': 'superiso', 'prefix': 'SI'} [Run3ModelGen.modelgen]
2024-08-28 19:53:21 [info     ] 		{'name': 'gm2calc', 'input_dir': 'SPheno', 'output_dir': 'gm2calc', 'prefix': 'GM2'} [Run3ModelGen.modelgen]
2024-08-28 19:53:21 [info     ] 		{'name': 'evade', 'input_dir': 'SPheno', 'output_dir': 'evade', 'prefix': 'EV'} [Run3ModelGen.modelgen]
2024-08-28 19:53:21 [info     ] Starting Model Generation...   [Run3ModelGen.modelgen]
2024-08-28 19:53:21 [info     ] Setting up output directories for scan [Run3ModelGen.modelgen]
2024-08-28 19:53:21 [info     ] Generating Model: 0.slha (1/2) [Run3ModelGen.modelgen]
2024-08-28 19:53:21 [info     ] 	running step: prep_input      [Run3ModelGen.modelgen]
2024-08-28 19:53:21 [info     ] 	running step: SPheno          [Run3ModelGen.modelgen]
2024-08-28 19:53:21 [info     ] 	running step: softsusy        [Run3ModelGen.modelgen]
2024-08-28 19:53:21 [info     ] 	running step: micromegas      [Run3ModelGen.modelgen]
2024-08-28 19:53:21 [info     ] 	running step: superiso        [Run3ModelGen.modelgen]
2024-08-28 19:53:21 [info     ] 	running step: gm2calc         [Run3ModelGen.modelgen]
2024-08-28 19:53:21 [info     ] 	running step: evade           [Run3ModelGen.modelgen]
2024-08-28 19:53:21 [info     ] Generating Model: 1.slha (2/2) [Run3ModelGen.modelgen]
2024-08-28 19:53:21 [info     ] 	running step: prep_input      [Run3ModelGen.modelgen]
2024-08-28 19:53:21 [info     ] 	running step: SPheno          [Run3ModelGen.modelgen]
2024-08-28 19:53:22 [info     ] 	running step: softsusy        [Run3ModelGen.modelgen]
2024-08-28 19:53:22 [info     ] 	running step: micromegas      [Run3ModelGen.modelgen]
2024-08-28 19:53:22 [info     ] 	running step: superiso        [Run3ModelGen.modelgen]
2024-08-28 19:53:22 [info     ] 	running step: gm2calc         [Run3ModelGen.modelgen]
2024-08-28 19:53:22 [info     ] 	running step: evade           [Run3ModelGen.modelgen]
2024-08-28 19:53:22 [info     ] Finished generating models! Stats for successful generations (successful/attempted): [Run3ModelGen.modelgen]
2024-08-28 19:53:22 [info     ] 	prep_input: 2/2.              [Run3ModelGen.modelgen]
2024-08-28 19:53:22 [info     ] 	SPheno: 2/2.                  [Run3ModelGen.modelgen]
2024-08-28 19:53:22 [info     ] 	softsusy: 2/2.                [Run3ModelGen.modelgen]
2024-08-28 19:53:22 [info     ] 	micromegas: 2/2.              [Run3ModelGen.modelgen]
2024-08-28 19:53:22 [info     ] 	superiso: 2/2.                [Run3ModelGen.modelgen]
2024-08-28 19:53:22 [info     ] 	gm2calc: 2/2.                 [Run3ModelGen.modelgen]
2024-08-28 19:53:22 [info     ] 	evade: 0/2.                   [Run3ModelGen.modelgen]
2024-08-28 19:53:22 [info     ] Will make NTuple for: /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/scans/scan_5, with 2 models [Run3ModelGen.ntupling]
2024-08-28 19:53:22 [info     ] 	Filling values for model: 0   [Run3ModelGen.ntupling]
2024-08-28 19:53:22 [info     ] Saving NTuple in /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/scans/scan_5/ntuple.0.0.root ... [Run3ModelGen.ntupling]
Completed iteration 5
Starting iteration 6
Initial Training points:  tensor([0.8528, 0.5812, 0.1617, 0.0046], device='cuda:0') torch.Size([4])
Training points after adding:  tensor([0.4324, 0.1617, 0.1091, 0.0046, 0.2132, 0.0561, 0.3373, 0.4995, 0.2723,
        0.5175, 0.2392, 0.5812, 0.0210, 0.8528], device='cuda:0') torch.Size([14])
These training_points are used in the GP tensor([0.4324, 0.1617, 0.1091, 0.0046, 0.2132, 0.0561, 0.3373, 0.4995, 0.2723,
        0.5175, 0.2392, 0.5812, 0.0210, 0.8528], device='cuda:0')
Point: tensor([0.0000, 0.0010, 0.0020, 0.0030, 0.0040, 0.0050, 0.0060, 0.0070, 0.0080,
        0.0090, 0.0100, 0.0110, 0.0120, 0.0130, 0.0140, 0.0150, 0.0160, 0.0170,
        0.0180, 0.0190, 0.0200, 0.0210, 0.0220, 0.0230, 0.0240, 0.0250, 0.0260,
        0.0270, 0.0280, 0.0290, 0.0300, 0.0310, 0.0320, 0.0330, 0.0340, 0.0350,
        0.0360, 0.0370, 0.0380, 0.0390, 0.0400, 0.0410, 0.0420, 0.0430, 0.0440,
        0.0450, 0.0460, 0.0470, 0.0480, 0.0490, 0.0501, 0.0511, 0.0521, 0.0531,
        0.0541, 0.0551, 0.0561, 0.0571, 0.0581, 0.0591, 0.0601, 0.0611, 0.0621,
        0.0631, 0.0641, 0.0651, 0.0661, 0.0671, 0.0681, 0.0691, 0.0701, 0.0711,
        0.0721, 0.0731, 0.0741, 0.0751, 0.0761, 0.0771, 0.0781, 0.0791, 0.0801,
        0.0811, 0.0821, 0.0831, 0.0841, 0.0851, 0.0861, 0.0871, 0.0881, 0.0891,
        0.0901, 0.0911, 0.0921, 0.0931, 0.0941, 0.0951, 0.0961, 0.0971, 0.0981,
        0.0991, 0.1001, 0.1011, 0.1021, 0.1031, 0.1041, 0.1051, 0.1061, 0.1071,
        0.1081, 0.1091, 0.1101, 0.1111, 0.1121, 0.1131, 0.1141, 0.1151, 0.1161,
        0.1171, 0.1181, 0.1191, 0.1201, 0.1211, 0.1221, 0.1231, 0.1241, 0.1251,
        0.1261, 0.1271, 0.1281, 0.1291, 0.1301, 0.1311, 0.1321, 0.1331, 0.1341,
        0.1351, 0.1361, 0.1371, 0.1381, 0.1391, 0.1401, 0.1411, 0.1421, 0.1431,
        0.1441, 0.1451, 0.1461, 0.1471, 0.1481, 0.1491, 0.1502, 0.1512, 0.1522,
        0.1532, 0.1542, 0.1552, 0.1562, 0.1572, 0.1582, 0.1592, 0.1602, 0.1612,
        0.1622, 0.1632, 0.1642, 0.1652, 0.1662, 0.1672, 0.1682, 0.1692, 0.1702,
        0.1712, 0.1722, 0.1732, 0.1742, 0.1752, 0.1762, 0.1772, 0.1782, 0.1792,
        0.1802, 0.1812, 0.1822, 0.1832, 0.1842, 0.1852, 0.1862, 0.1872, 0.1882,
        0.1892, 0.1902, 0.1912, 0.1922, 0.1932, 0.1942, 0.1952, 0.1962, 0.1972,
        0.1982, 0.1992, 0.2002, 0.2012, 0.2022, 0.2032, 0.2042, 0.2052, 0.2062,
        0.2072, 0.2082, 0.2092, 0.2102, 0.2112, 0.2122, 0.2132, 0.2142, 0.2152,
        0.2162, 0.2172, 0.2182, 0.2192, 0.2202, 0.2212, 0.2222, 0.2232, 0.2242,
        0.2252, 0.2262, 0.2272, 0.2282, 0.2292, 0.2302, 0.2312, 0.2322, 0.2332,
        0.2342, 0.2352, 0.2362, 0.2372, 0.2382, 0.2392, 0.2402, 0.2412, 0.2422,
        0.2432, 0.2442, 0.2452, 0.2462, 0.2472, 0.2482, 0.2492, 0.2503, 0.2513,
        0.2523, 0.2533, 0.2543, 0.2553, 0.2563, 0.2573, 0.2583, 0.2593, 0.2603,
        0.2613, 0.2623, 0.2633, 0.2643, 0.2653, 0.2663, 0.2673, 0.2683, 0.2693,
        0.2703, 0.2713, 0.2723, 0.2733, 0.2743, 0.2753, 0.2763, 0.2773, 0.2783,
        0.2793, 0.2803, 0.2813, 0.2823, 0.2833, 0.2843, 0.2853, 0.2863, 0.2873,
        0.2883, 0.2893, 0.2903, 0.2913, 0.2923, 0.2933, 0.2943, 0.2953, 0.2963,
        0.2973, 0.2983, 0.2993, 0.3003, 0.3013, 0.3023, 0.3033, 0.3043, 0.3053,
        0.3063, 0.3073, 0.3083, 0.3093, 0.3103, 0.3113, 0.3123, 0.3133, 0.3143,
        0.3153, 0.3163, 0.3173, 0.3183, 0.3193, 0.3203, 0.3213, 0.3223, 0.3233,
        0.3243, 0.3253, 0.3263, 0.3273, 0.3283, 0.3293, 0.3303, 0.3313, 0.3323,
        0.3333, 0.3343, 0.3353, 0.3363, 0.3373, 0.3383, 0.3393, 0.3403, 0.3413,
        0.3423, 0.3433, 0.3443, 0.3453, 0.3463, 0.3473, 0.3483, 0.3493, 0.3504,
        0.3514, 0.3524, 0.3534, 0.3544, 0.3554, 0.3564, 0.3574, 0.3584, 0.3594,
        0.3604, 0.3614, 0.3624, 0.3634, 0.3644, 0.3654, 0.3664, 0.3674, 0.3684,
        0.3694, 0.3704, 0.3714, 0.3724, 0.3734, 0.3744, 0.3754, 0.3764, 0.3774,
        0.3784, 0.3794, 0.3804, 0.3814, 0.3824, 0.3834, 0.3844, 0.3854, 0.3864,
        0.3874, 0.3884, 0.3894, 0.3904, 0.3914, 0.3924, 0.3934, 0.3944, 0.3954,
        0.3964, 0.3974, 0.3984, 0.3994, 0.4004, 0.4014, 0.4024, 0.4034, 0.4044,
        0.4054, 0.4064, 0.4074, 0.4084, 0.4094, 0.4104, 0.4114, 0.4124, 0.4134,
        0.4144, 0.4154, 0.4164, 0.4174, 0.4184, 0.4194, 0.4204, 0.4214, 0.4224,
        0.4234, 0.4244, 0.4254, 0.4264, 0.4274, 0.4284, 0.4294, 0.4304, 0.4314,
        0.4324, 0.4334, 0.4344, 0.4354, 0.4364, 0.4374, 0.4384, 0.4394, 0.4404,
        0.4414, 0.4424, 0.4434, 0.4444, 0.4454, 0.4464, 0.4474, 0.4484, 0.4494,
        0.4505, 0.4515, 0.4525, 0.4535, 0.4545, 0.4555, 0.4565, 0.4575, 0.4585,
        0.4595, 0.4605, 0.4615, 0.4625, 0.4635, 0.4645, 0.4655, 0.4665, 0.4675,
        0.4685, 0.4695, 0.4705, 0.4715, 0.4725, 0.4735, 0.4745, 0.4755, 0.4765,
        0.4775, 0.4785, 0.4795, 0.4805, 0.4815, 0.4825, 0.4835, 0.4845, 0.4855,
        0.4865, 0.4875, 0.4885, 0.4895, 0.4905, 0.4915, 0.4925, 0.4935, 0.4945,
        0.4955, 0.4965, 0.4975, 0.4985, 0.4995, 0.5005, 0.5015, 0.5025, 0.5035,
        0.5045, 0.5055, 0.5065, 0.5075, 0.5085, 0.5095, 0.5105, 0.5115, 0.5125,
        0.5135, 0.5145, 0.5155, 0.5165, 0.5175, 0.5185, 0.5195, 0.5205, 0.5215,
        0.5225, 0.5235, 0.5245, 0.5255, 0.5265, 0.5275, 0.5285, 0.5295, 0.5305,
        0.5315, 0.5325, 0.5335, 0.5345, 0.5355, 0.5365, 0.5375, 0.5385, 0.5395,
        0.5405, 0.5415, 0.5425, 0.5435, 0.5445, 0.5455, 0.5465, 0.5475, 0.5485,
        0.5495, 0.5506, 0.5516, 0.5526, 0.5536, 0.5546, 0.5556, 0.5566, 0.5576,
        0.5586, 0.5596, 0.5606, 0.5616, 0.5626, 0.5636, 0.5646, 0.5656, 0.5666,
        0.5676, 0.5686, 0.5696, 0.5706, 0.5716, 0.5726, 0.5736, 0.5746, 0.5756,
        0.5766, 0.5776, 0.5786, 0.5796, 0.5806, 0.5816, 0.5826, 0.5836, 0.5846,
        0.5856, 0.5866, 0.5876, 0.5886, 0.5896, 0.5906, 0.5916, 0.5926, 0.5936,
        0.5946, 0.5956, 0.5966, 0.5976, 0.5986, 0.5996, 0.6006, 0.6016, 0.6026,
        0.6036, 0.6046, 0.6056, 0.6066, 0.6076, 0.6086, 0.6096, 0.6106, 0.6116,
        0.6126, 0.6136, 0.6146, 0.6156, 0.6166, 0.6176, 0.6186, 0.6196, 0.6206,
        0.6216, 0.6226, 0.6236, 0.6246, 0.6256, 0.6266, 0.6276, 0.6286, 0.6296,
        0.6306, 0.6316, 0.6326, 0.6336, 0.6346, 0.6356, 0.6366, 0.6376, 0.6386,
        0.6396, 0.6406, 0.6416, 0.6426, 0.6436, 0.6446, 0.6456, 0.6466, 0.6476,
        0.6486, 0.6496, 0.6507, 0.6517, 0.6527, 0.6537, 0.6547, 0.6557, 0.6567,
        0.6577, 0.6587, 0.6597, 0.6607, 0.6617, 0.6627, 0.6637, 0.6647, 0.6657,
        0.6667, 0.6677, 0.6687, 0.6697, 0.6707, 0.6717, 0.6727, 0.6737, 0.6747,
        0.6757, 0.6767, 0.6777, 0.6787, 0.6797, 0.6807, 0.6817, 0.6827, 0.6837,
        0.6847, 0.6857, 0.6867, 0.6877, 0.6887, 0.6897, 0.6907, 0.6917, 0.6927,
        0.6937, 0.6947, 0.6957, 0.6967, 0.6977, 0.6987, 0.6997, 0.7007, 0.7017,
        0.7027, 0.7037, 0.7047, 0.7057, 0.7067, 0.7077, 0.7087, 0.7097, 0.7107,
        0.7117, 0.7127, 0.7137, 0.7147, 0.7157, 0.7167, 0.7177, 0.7187, 0.7197,
        0.7207, 0.7217, 0.7227, 0.7237, 0.7247, 0.7257, 0.7267, 0.7277, 0.7287,
        0.7297, 0.7307, 0.7317, 0.7327, 0.7337, 0.7347, 0.7357, 0.7367, 0.7377,
        0.7387, 0.7397, 0.7407, 0.7417, 0.7427, 0.7437, 0.7447, 0.7457, 0.7467,
        0.7477, 0.7487, 0.7497, 0.7508, 0.7518, 0.7528, 0.7538, 0.7548, 0.7558,
        0.7568, 0.7578, 0.7588, 0.7598, 0.7608, 0.7618, 0.7628, 0.7638, 0.7648,
        0.7658, 0.7668, 0.7678, 0.7688, 0.7698, 0.7708, 0.7718, 0.7728, 0.7738,
        0.7748, 0.7758, 0.7768, 0.7778, 0.7788, 0.7798, 0.7808, 0.7818, 0.7828,
        0.7838, 0.7848, 0.7858, 0.7868, 0.7878, 0.7888, 0.7898, 0.7908, 0.7918,
        0.7928, 0.7938, 0.7948, 0.7958, 0.7968, 0.7978, 0.7988, 0.7998, 0.8008,
        0.8018, 0.8028, 0.8038, 0.8048, 0.8058, 0.8068, 0.8078, 0.8088, 0.8098,
        0.8108, 0.8118, 0.8128, 0.8138, 0.8148, 0.8158, 0.8168, 0.8178, 0.8188,
        0.8198, 0.8208, 0.8218, 0.8228, 0.8238, 0.8248, 0.8258, 0.8268, 0.8278,
        0.8288, 0.8298, 0.8308, 0.8318, 0.8328, 0.8338, 0.8348, 0.8358, 0.8368,
        0.8378, 0.8388, 0.8398, 0.8408, 0.8418, 0.8428, 0.8438, 0.8448, 0.8458,
        0.8468, 0.8478, 0.8488, 0.8498, 0.8509, 0.8519, 0.8529, 0.8539, 0.8549,
        0.8559, 0.8569, 0.8579, 0.8589, 0.8599, 0.8609, 0.8619, 0.8629, 0.8639,
        0.8649, 0.8659, 0.8669, 0.8679, 0.8689, 0.8699, 0.8709, 0.8719, 0.8729,
        0.8739, 0.8749, 0.8759, 0.8769, 0.8779, 0.8789, 0.8799, 0.8809, 0.8819,
        0.8829, 0.8839, 0.8849, 0.8859, 0.8869, 0.8879, 0.8889, 0.8899, 0.8909,
        0.8919, 0.8929, 0.8939, 0.8949, 0.8959, 0.8969, 0.8979, 0.8989, 0.8999,
        0.9009, 0.9019, 0.9029, 0.9039, 0.9049, 0.9059, 0.9069, 0.9079, 0.9089,
        0.9099, 0.9109, 0.9119, 0.9129, 0.9139, 0.9149, 0.9159, 0.9169, 0.9179,
        0.9189, 0.9199, 0.9209, 0.9219, 0.9229, 0.9239, 0.9249, 0.9259, 0.9269,
        0.9279, 0.9289, 0.9299, 0.9309, 0.9319, 0.9329, 0.9339, 0.9349, 0.9359,
        0.9369, 0.9379, 0.9389, 0.9399, 0.9409, 0.9419, 0.9429, 0.9439, 0.9449,
        0.9459, 0.9469, 0.9479, 0.9489, 0.9499, 0.9510, 0.9520, 0.9530, 0.9540,
        0.9550, 0.9560, 0.9570, 0.9580, 0.9590, 0.9600, 0.9610, 0.9620, 0.9630,
        0.9640, 0.9650, 0.9660, 0.9670, 0.9680, 0.9690, 0.9700, 0.9710, 0.9720,
        0.9730, 0.9740, 0.9750, 0.9760, 0.9770, 0.9780, 0.9790, 0.9800, 0.9810,
        0.9820, 0.9830, 0.9840, 0.9850, 0.9860, 0.9870, 0.9880, 0.9890, 0.9900,
        0.9910, 0.9920, 0.9930, 0.9940, 0.9950, 0.9960, 0.9970, 0.9980, 0.9990,
        1.0000], device='cuda:0') - Variance: tensor([[1.0000e-06],
        [1.0000e-06],
        [4.1015e-01],
        [1.0000e-06],
        [1.0000e-06],
        [1.0000e-06],
        [1.1616e-02],
        [9.0707e-03],
        [1.0000e-06],
        [1.3755e-01],
        [1.0000e-06],
        [1.0000e-06],
        [1.0000e-06],
        [1.0000e-06],
        [7.3847e-01],
        [4.0984e-01],
        [5.1435e-01],
        [1.0000e-06],
        [6.6873e-02],
        [1.0000e-06],
        [7.6676e-01],
        [7.9351e-01],
        [4.8309e-01],
        [5.2612e-01],
        [1.0000e-06],
        [1.0000e-06],
        [8.6048e-01],
        [6.1982e-01],
        [6.2636e-01],
        [6.2897e-01],
        [7.7621e-01],
        [6.1978e-01],
        [6.0770e-01],
        [7.5001e-01],
        [7.3254e-01],
        [5.1501e-01],
        [8.0035e-01],
        [6.2013e-01],
        [5.5915e-01],
        [6.7263e-01],
        [6.0530e-01],
        [2.0093e-01],
        [6.2187e-01],
        [2.1709e-01],
        [1.7533e-02],
        [5.2103e-01],
        [1.0000e-06],
        [1.0000e-06],
        [3.6810e-01],
        [9.1845e-03],
        [9.5959e-03],
        [1.5384e-01],
        [1.3528e-02],
        [1.3521e-02],
        [1.2923e-02],
        [1.2852e-02],
        [1.2778e-02],
        [7.3893e-02],
        [1.0591e-02],
        [1.0498e-02],
        [2.4667e-01],
        [3.6040e-02],
        [8.5156e-03],
        [3.8769e-01],
        [2.1622e-01],
        [7.0854e-03],
        [5.0266e-01],
        [3.6301e-01],
        [1.8462e-01],
        [5.9608e-01],
        [4.8203e-01],
        [3.3604e-01],
        [6.7109e-01],
        [5.7678e-01],
        [4.5495e-01],
        [8.3789e-01],
        [7.8809e-01],
        [7.2143e-01],
        [8.5793e-01],
        [8.8496e-01],
        [8.3812e-01],
        [9.0893e-01],
        [9.1704e-01],
        [8.6425e-01],
        [9.0656e-01],
        [8.9947e-01],
        [8.1032e-01],
        [9.0982e-01],
        [8.1984e-01],
        [6.4177e-01],
        [8.1662e-01],
        [6.2982e-01],
        [5.5207e-01],
        [6.1058e-01],
        [5.2708e-01],
        [4.2553e-01],
        [4.9859e-01],
        [3.9048e-01],
        [1.2918e-02],
        [3.5268e-01],
        [2.1323e-01],
        [1.3375e-02],
        [1.6420e-01],
        [1.3514e-02],
        [1.3504e-02],
        [1.3537e-02],
        [1.3540e-02],
        [1.3543e-02],
        [1.5847e-01],
        [1.3546e-02],
        [1.3546e-02],
        [1.3546e-02],
        [1.3543e-02],
        [1.3536e-02],
        [1.5264e-01],
        [1.3522e-02],
        [1.3492e-02],
        [3.3925e-01],
        [1.9681e-01],
        [2.3941e-02],
        [4.8496e-01],
        [3.7349e-01],
        [2.3808e-01],
        [5.9839e-01],
        [5.1072e-01],
        [4.0385e-01],
        [6.8557e-01],
        [6.1518e-01],
        [7.1646e-01],
        [8.5030e-01],
        [8.1314e-01],
        [7.6464e-01],
        [8.7034e-01],
        [8.9871e-01],
        [8.6114e-01],
        [9.1349e-01],
        [9.2266e-01],
        [8.7540e-01],
        [9.0479e-01],
        [9.0052e-01],
        [8.1873e-01],
        [8.4192e-01],
        [8.2254e-01],
        [6.6407e-01],
        [8.1692e-01],
        [6.4961e-01],
        [5.9825e-01],
        [6.2965e-01],
        [5.7423e-01],
        [1.8836e-01],
        [5.4737e-01],
        [4.7923e-01],
        [5.1829e-01],
        [4.4570e-01],
        [1.3415e-02],
        [4.0988e-01],
        [3.2111e-01],
        [2.1913e-01],
        [2.7727e-01],
        [1.6876e-01],
        [4.4104e-02],
        [4.6598e-01],
        [1.3547e-02],
        [1.3546e-02],
        [3.4579e-01],
        [2.4747e-01],
        [1.3528e-02],
        [3.7758e-01],
        [2.8397e-01],
        [1.7641e-01],
        [5.4026e-01],
        [4.7080e-01],
        [3.9094e-01],
        [6.6068e-01],
        [6.0896e-01],
        [5.4937e-01],
        [7.4934e-01],
        [7.1032e-01],
        [6.6492e-01],
        [8.1323e-01],
        [8.3186e-01],
        [8.0271e-01],
        [8.8853e-01],
        [8.9678e-01],
        [8.7292e-01],
        [9.2296e-01],
        [9.2174e-01],
        [8.9125e-01],
        [9.4024e-01],
        [9.2865e-01],
        [8.8400e-01],
        [9.2347e-01],
        [8.6691e-01],
        [8.2061e-01],
        [8.7066e-01],
        [8.2149e-01],
        [7.5328e-01],
        [7.6354e-01],
        [6.7207e-01],
        [5.4567e-01],
        [6.6041e-01],
        [5.2905e-01],
        [3.4776e-01],
        [6.2094e-01],
        [4.7449e-01],
        [2.7250e-01],
        [4.5484e-01],
        [2.4546e-01],
        [1.8892e-01],
        [3.9194e-01],
        [1.5837e-01],
        [9.4693e-02],
        [3.1998e-01],
        [2.6718e-01],
        [1.2061e-02],
        [4.0299e-01],
        [1.6676e-01],
        [9.3615e-02],
        [3.0589e-01],
        [2.3636e-01],
        [1.5400e-01],
        [4.8226e-01],
        [2.4533e-01],
        [1.3556e-01],
        [4.4446e-01],
        [3.4283e-01],
        [2.1077e-01],
        [4.6064e-01],
        [3.2945e-01],
        [1.5510e-01],
        [3.8916e-01],
        [2.0978e-01],
        [1.0000e-06],
        [3.9876e-01],
        [2.0312e-01],
        [1.0000e-06],
        [1.8836e-01],
        [1.5430e-01],
        [1.2190e-02],
        [3.1832e-01],
        [8.0935e-02],
        [9.8660e-03],
        [2.5189e-01],
        [2.1301e-01],
        [1.0000e-06],
        [3.5224e-01],
        [1.1663e-01],
        [6.0364e-02],
        [4.2036e-01],
        [3.7482e-01],
        [3.1970e-01],
        [4.3987e-01],
        [3.7288e-01],
        [2.8635e-01],
        [5.1240e-01],
        [4.1998e-01],
        [2.9565e-01],
        [4.7656e-01],
        [3.3500e-01],
        [3.3252e-01],
        [4.6832e-01],
        [2.9174e-01],
        [4.8103e-02],
        [3.8656e-01],
        [1.6280e-01],
        [2.0770e-01],
        [1.0000e-06],
        [5.7596e-03],
        [9.9060e-03],
        [1.0552e-02],
        [1.2505e-02],
        [2.4933e-02],
        [1.3530e-02],
        [1.3444e-02],
        [3.6060e-02],
        [1.2763e-02],
        [1.1964e-02],
        [4.6150e-02],
        [1.1008e-02],
        [1.3181e-01],
        [2.6648e-01],
        [2.0993e-01],
        [3.3263e-01],
        [4.3647e-01],
        [5.2433e-01],
        [4.8754e-01],
        [5.6756e-01],
        [6.7892e-01],
        [6.5394e-01],
        [7.4340e-01],
        [7.8400e-01],
        [7.9519e-01],
        [8.4881e-01],
        [8.7312e-01],
        [8.7976e-01],
        [9.1185e-01],
        [9.3574e-01],
        [9.3903e-01],
        [9.5578e-01],
        [9.7266e-01],
        [9.7358e-01],
        [9.8079e-01],
        [9.8817e-01],
        [9.8686e-01],
        [9.9066e-01],
        [9.9249e-01],
        [9.8765e-01],
        [9.8677e-01],
        [9.8452e-01],
        [9.7603e-01],
        [9.7038e-01],
        [9.6304e-01],
        [9.5118e-01],
        [9.3913e-01],
        [9.2424e-01],
        [9.0106e-01],
        [8.7752e-01],
        [8.6724e-01],
        [8.0400e-01],
        [7.8779e-01],
        [7.7028e-01],
        [7.0341e-01],
        [6.7913e-01],
        [6.5290e-01],
        [5.5283e-01],
        [5.1648e-01],
        [4.7722e-01],
        [3.2747e-01],
        [3.5929e-01],
        [3.0745e-01],
        [2.1510e-01],
        [1.5171e-01],
        [1.9176e-01],
        [2.4639e-02],
        [7.0644e-02],
        [1.1449e-01],
        [1.3547e-02],
        [4.3989e-02],
        [8.9087e-02],
        [1.3547e-02],
        [7.6568e-02],
        [1.2014e-01],
        [6.3878e-02],
        [1.6250e-01],
        [2.5080e-01],
        [2.0283e-01],
        [3.3053e-01],
        [4.0126e-01],
        [4.0185e-01],
        [4.9794e-01],
        [5.7877e-01],
        [5.7919e-01],
        [6.4711e-01],
        [7.2257e-01],
        [7.2285e-01],
        [7.8242e-01],
        [8.2950e-01],
        [8.4040e-01],
        [8.7531e-01],
        [9.0290e-01],
        [9.1520e-01],
        [9.3441e-01],
        [9.5307e-01],
        [9.5943e-01],
        [9.7163e-01],
        [9.8069e-01],
        [9.8378e-01],
        [9.9075e-01],
        [9.9560e-01],
        [9.9693e-01],
        [9.9992e-01],
        [1.0020e+00],
        [1.0035e+00],
        [1.0042e+00],
        [1.0050e+00],
        [1.0056e+00],
        [1.0059e+00],
        [1.0062e+00],
        [1.0064e+00],
        [1.0065e+00],
        [1.0066e+00],
        [1.0067e+00],
        [1.0067e+00],
        [1.0067e+00],
        [1.0067e+00],
        [1.0067e+00],
        [1.0067e+00],
        [1.0067e+00],
        [1.0066e+00],
        [1.0066e+00],
        [1.0065e+00],
        [1.0064e+00],
        [1.0061e+00],
        [1.0058e+00],
        [1.0055e+00],
        [1.0049e+00],
        [1.0041e+00],
        [1.0032e+00],
        [1.0018e+00],
        [9.9984e-01],
        [9.9775e-01],
        [9.9461e-01],
        [9.9036e-01],
        [9.8542e-01],
        [9.7886e-01],
        [9.7029e-01],
        [9.6219e-01],
        [9.4851e-01],
        [9.3298e-01],
        [9.1936e-01],
        [8.9606e-01],
        [8.7087e-01],
        [8.4580e-01],
        [8.1525e-01],
        [7.7168e-01],
        [7.3688e-01],
        [6.9065e-01],
        [6.4224e-01],
        [6.0117e-01],
        [5.3905e-01],
        [4.7578e-01],
        [4.3414e-01],
        [3.6676e-01],
        [3.0254e-01],
        [2.5909e-01],
        [1.9682e-01],
        [1.2937e-01],
        [1.1789e-01],
        [5.8800e-02],
        [2.6885e-02],
        [1.4062e-02],
        [1.3547e-02],
        [1.3547e-02],
        [1.3547e-02],
        [1.3547e-02],
        [5.9205e-02],
        [7.2157e-02],
        [9.9225e-02],
        [1.6963e-01],
        [2.0648e-01],
        [2.5357e-01],
        [3.1044e-01],
        [3.7300e-01],
        [4.2410e-01],
        [4.8467e-01],
        [5.4620e-01],
        [6.0047e-01],
        [6.4755e-01],
        [6.9722e-01],
        [7.4210e-01],
        [7.7818e-01],
        [8.1584e-01],
        [8.4728e-01],
        [8.7323e-01],
        [8.9777e-01],
        [9.1846e-01],
        [9.3445e-01],
        [9.4941e-01],
        [9.6134e-01],
        [9.7041e-01],
        [9.7815e-01],
        [9.8409e-01],
        [9.8824e-01],
        [9.9136e-01],
        [9.9323e-01],
        [9.9385e-01],
        [9.9350e-01],
        [9.9206e-01],
        [9.8940e-01],
        [9.8537e-01],
        [9.7986e-01],
        [9.7272e-01],
        [9.6369e-01],
        [9.5239e-01],
        [9.3860e-01],
        [9.2187e-01],
        [9.0199e-01],
        [8.7869e-01],
        [8.5177e-01],
        [8.2086e-01],
        [7.8579e-01],
        [7.4656e-01],
        [7.0342e-01],
        [6.5638e-01],
        [6.0623e-01],
        [5.5303e-01],
        [4.9779e-01],
        [4.4106e-01],
        [3.8436e-01],
        [3.2812e-01],
        [2.7389e-01],
        [2.2275e-01],
        [1.7558e-01],
        [1.3346e-01],
        [9.7135e-02],
        [6.7176e-02],
        [4.3993e-02],
        [2.7475e-02],
        [1.7423e-02],
        [1.3278e-02],
        [1.4217e-02],
        [1.9215e-02],
        [2.7101e-02],
        [3.6616e-02],
        [4.6405e-02],
        [5.5533e-02],
        [6.2863e-02],
        [6.7626e-02],
        [6.9192e-02],
        [6.7341e-02],
        [6.2469e-02],
        [5.5848e-02],
        [4.6301e-02],
        [3.6473e-02],
        [2.6601e-02],
        [1.9481e-02],
        [1.2715e-02],
        [1.3278e-02],
        [1.7611e-02],
        [2.6594e-02],
        [4.3983e-02],
        [6.7713e-02],
        [9.5533e-02],
        [1.3255e-01],
        [1.7583e-01],
        [2.2253e-01],
        [2.7417e-01],
        [3.2748e-01],
        [3.8350e-01],
        [4.4065e-01],
        [4.9750e-01],
        [5.5461e-01],
        [6.0667e-01],
        [6.5565e-01],
        [7.0235e-01],
        [7.4627e-01],
        [7.8546e-01],
        [8.2027e-01],
        [8.5159e-01],
        [8.7853e-01],
        [9.0156e-01],
        [9.2143e-01],
        [9.3784e-01],
        [9.5105e-01],
        [9.6227e-01],
        [9.7079e-01],
        [9.7715e-01],
        [9.8180e-01],
        [9.8465e-01],
        [9.8563e-01],
        [9.8545e-01],
        [9.8334e-01],
        [9.7949e-01],
        [9.7436e-01],
        [9.6693e-01],
        [9.5685e-01],
        [9.4450e-01],
        [9.3082e-01],
        [9.1256e-01],
        [8.9147e-01],
        [8.6850e-01],
        [8.4342e-01],
        [8.0526e-01],
        [7.7228e-01],
        [7.3809e-01],
        [6.8540e-01],
        [6.4424e-01],
        [5.7984e-01],
        [5.3260e-01],
        [4.8013e-01],
        [4.1498e-01],
        [3.5969e-01],
        [3.2099e-01],
        [2.3615e-01],
        [2.0273e-01],
        [1.6787e-01],
        [1.0725e-01],
        [8.2793e-02],
        [7.2386e-02],
        [2.0404e-02],
        [2.4759e-02],
        [4.4253e-02],
        [2.1968e-02],
        [4.1517e-02],
        [7.5346e-02],
        [8.3103e-02],
        [1.5629e-01],
        [1.9882e-01],
        [2.3020e-01],
        [2.9173e-01],
        [3.6865e-01],
        [3.9343e-01],
        [4.5941e-01],
        [5.3332e-01],
        [5.6571e-01],
        [6.2526e-01],
        [6.7678e-01],
        [7.1798e-01],
        [7.6466e-01],
        [8.0380e-01],
        [8.2915e-01],
        [8.6244e-01],
        [8.8950e-01],
        [9.0730e-01],
        [9.2595e-01],
        [9.4312e-01],
        [9.5445e-01],
        [9.6684e-01],
        [9.7629e-01],
        [9.8249e-01],
        [9.8824e-01],
        [9.9201e-01],
        [9.9586e-01],
        [9.9895e-01],
        [1.0007e+00],
        [1.0025e+00],
        [1.0037e+00],
        [1.0045e+00],
        [1.0052e+00],
        [1.0056e+00],
        [1.0060e+00],
        [1.0063e+00],
        [1.0064e+00],
        [1.0065e+00],
        [1.0066e+00],
        [1.0067e+00],
        [1.0067e+00],
        [1.0067e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0067e+00],
        [1.0067e+00],
        [1.0067e+00],
        [1.0065e+00],
        [1.0065e+00],
        [1.0065e+00],
        [1.0059e+00],
        [1.0059e+00],
        [1.0057e+00],
        [1.0046e+00],
        [1.0046e+00],
        [1.0040e+00],
        [1.0010e+00],
        [1.0011e+00],
        [9.9959e-01],
        [9.9183e-01],
        [9.9206e-01],
        [9.9229e-01],
        [9.7669e-01],
        [9.7715e-01],
        [9.7761e-01],
        [9.4621e-01],
        [9.4715e-01],
        [9.4807e-01],
        [8.8488e-01],
        [8.8677e-01],
        [8.8863e-01],
        [7.6148e-01],
        [8.1871e-01],
        [8.2162e-01],
        [6.2237e-01],
        [6.2833e-01],
        [7.8080e-01],
        [5.3762e-01],
        [5.4489e-01],
        [5.5205e-01],
        [4.3419e-01],
        [4.4307e-01],
        [4.5181e-01],
        [1.3547e-02],
        [3.1879e-01],
        [3.2946e-01],
        [1.5390e-01],
        [1.6712e-01],
        [1.3547e-02],
        [1.3547e-02],
        [1.3547e-02],
        [1.3547e-02],
        [1.3547e-02],
        [2.4821e-01],
        [1.3547e-02],
        [6.6407e-02],
        [8.0986e-02],
        [1.3547e-02],
        [3.1068e-01],
        [3.2147e-01],
        [1.4385e-01],
        [4.9151e-01],
        [4.9949e-01],
        [3.6801e-01],
        [6.2536e-01],
        [6.3127e-01],
        [5.3394e-01],
        [7.2444e-01],
        [8.3819e-01],
        [7.9449e-01],
        [7.9779e-01],
        [8.8199e-01],
        [8.4964e-01],
        [9.1295e-01],
        [9.5076e-01],
        [9.3624e-01],
        [9.6466e-01],
        [9.8164e-01],
        [9.7512e-01],
        [9.8788e-01],
        [9.9550e-01],
        [9.9257e-01],
        [9.9830e-01],
        [1.0017e+00],
        [1.0004e+00],
        [1.0030e+00],
        [1.0045e+00],
        [1.0039e+00],
        [1.0051e+00],
        [1.0062e+00],
        [1.0060e+00],
        [1.0063e+00],
        [1.0065e+00],
        [1.0064e+00],
        [1.0067e+00],
        [1.0067e+00],
        [1.0067e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00],
        [1.0068e+00]], device='cuda:0') 
Smoothed_batch_entropy:  tensor([-9.3219e-02, -2.4557e-02,  5.0878e-02, -8.9925e-02, -1.0835e-02,
        -3.8521e-02,  2.9282e-03,  2.4053e-03, -1.8324e-02,  2.6056e-02,
        -1.7736e-03, -3.3518e-03, -2.1322e-02, -1.6550e-02,  6.6284e-02,
         5.7915e-02,  6.2838e-02, -2.6116e-03,  1.4326e-02, -4.2343e-02,
         6.1710e-02,  6.0087e-02,  5.0771e-02,  5.0374e-02, -8.8207e-03,
        -3.1585e-03,  5.2266e-02,  4.5132e-02,  4.3878e-02,  4.2632e-02,
         4.6366e-02,  3.9850e-02,  3.8474e-02,  4.3145e-02,  4.1964e-02,
         3.2085e-02,  4.3253e-02,  3.5993e-02,  3.2899e-02,  3.7503e-02,
         3.4360e-02,  1.2869e-02,  3.4604e-02,  1.3598e-02,  1.1601e-03,
         2.9503e-02, -9.8546e-05, -4.2136e-05,  2.1546e-02,  5.9067e-04,
         6.0151e-04,  9.4349e-03,  8.4486e-04,  8.5260e-04,  8.0935e-04,
         8.1090e-04,  8.1227e-04,  4.5655e-03,  6.7065e-04,  6.6789e-04,
         1.4472e-02,  2.2135e-03,  5.3745e-04,  2.1797e-02,  1.2688e-02,
         4.6504e-04,  2.7217e-02,  2.0424e-02,  1.0879e-02,  3.1287e-02,
         2.6185e-02,  1.9036e-02,  3.4360e-02,  3.0514e-02,  2.5061e-02,
         4.0268e-02,  3.8803e-02,  3.6637e-02,  4.1053e-02,  4.1777e-02,
         4.0681e-02,  4.2320e-02,  4.2396e-02,  4.1201e-02,  4.1872e-02,
         4.1486e-02,  3.8875e-02,  4.1118e-02,  3.8469e-02,  3.1871e-02,
         3.7813e-02,  3.0852e-02,  2.7467e-02,  2.9691e-02,  2.6077e-02,
         2.1495e-02,  2.4624e-02,  1.9699e-02,  7.0788e-04,  1.7819e-02,
         1.1091e-02,  7.2637e-04,  8.5885e-03,  7.2964e-04,  7.2774e-04,
         7.2757e-04,  7.2620e-04,  7.2491e-04,  8.1595e-03,  7.2198e-04,
         7.2044e-04,  7.1889e-04,  7.1751e-04,  7.1579e-04,  7.8023e-03,
         7.1252e-04,  7.0994e-04,  1.6573e-02,  9.9053e-03,  1.2528e-03,
         2.2888e-02,  1.8063e-02,  1.1873e-02,  2.7460e-02,  2.3992e-02,
         1.9496e-02,  3.0838e-02,  2.8321e-02,  3.2113e-02,  3.6427e-02,
         3.5534e-02,  3.4267e-02,  3.7459e-02,  3.8235e-02,  3.7638e-02,
         3.8800e-02,  3.8975e-02,  3.8232e-02,  3.8658e-02,  3.8465e-02,
         3.6597e-02,  3.6929e-02,  3.6268e-02,  3.1285e-02,  3.5747e-02,
         3.0410e-02,  2.8418e-02,  2.9406e-02,  2.7250e-02,  1.0041e-02,
         2.6020e-02,  2.3226e-02,  2.4732e-02,  2.1698e-02,  7.4030e-04,
         2.0065e-02,  1.6101e-02,  1.1293e-02,  1.4030e-02,  8.7910e-03,
         2.3635e-03,  2.2152e-02,  7.3428e-04,  7.3273e-04,  1.6943e-02,
         1.2427e-02,  7.2723e-04,  1.8233e-02,  1.4069e-02,  8.9939e-03,
         2.4716e-02,  2.1987e-02,  1.8690e-02,  2.8921e-02,  2.7126e-02,
         2.4949e-02,  3.1661e-02,  3.0483e-02,  2.9043e-02,  3.3433e-02,
         3.3915e-02,  3.3231e-02,  3.5193e-02,  3.5356e-02,  3.4947e-02,
         3.5748e-02,  3.5681e-02,  3.5133e-02,  3.5630e-02,  3.5318e-02,
         3.4356e-02,  3.4773e-02,  3.3435e-02,  3.2066e-02,  3.3055e-02,
         3.1654e-02,  2.9571e-02,  2.9733e-02,  2.6824e-02,  2.2453e-02,
         2.6328e-02,  2.1802e-02,  1.4976e-02,  2.5039e-02,  1.9913e-02,
         1.2052e-02,  1.9348e-02,  1.1070e-02,  8.7240e-03,  1.7345e-02,
         7.5578e-03,  4.6555e-03,  1.5031e-02,  1.2981e-02,  6.5310e-04,
         1.9487e-02,  9.0402e-03,  5.3797e-03,  1.6716e-02,  1.3789e-02,
         9.7180e-03,  2.6555e-02,  1.6136e-02,  9.9562e-03,  2.7697e-02,
         2.3739e-02,  1.6653e-02,  3.0713e-02,  2.5003e-02,  1.3852e-02,
         2.8806e-02,  1.8295e-02, -3.3340e-03,  2.9494e-02,  1.7826e-02,
        -4.5137e-04,  1.6635e-02,  1.4002e-02,  1.2471e-03,  2.4845e-02,
         7.7698e-03,  1.0347e-03,  2.0824e-02,  1.8244e-02, -8.4899e-04,
         2.6893e-02,  1.1115e-02,  6.0684e-03,  3.0624e-02,  2.8620e-02,
         2.5788e-02,  3.1645e-02,  2.8476e-02,  2.3508e-02,  3.3406e-02,
         2.9245e-02,  2.2320e-02,  2.9710e-02,  2.2620e-02,  2.1620e-02,
         2.6477e-02,  1.7900e-02,  3.2498e-03,  2.0833e-02,  9.5833e-03,
         1.1556e-02, -6.0452e-05,  3.3795e-04,  5.5361e-04,  5.8380e-04,
         6.7116e-04,  1.3197e-03,  7.2456e-04,  6.9378e-04,  1.8130e-03,
         6.4588e-04,  5.9738e-04,  2.2660e-03,  5.3727e-04,  6.2193e-03,
         1.1942e-02,  9.5425e-03,  1.4459e-02,  1.8228e-02,  2.1130e-02,
         1.9861e-02,  2.2387e-02,  2.5534e-02,  2.4785e-02,  2.7049e-02,
         2.7913e-02,  2.8090e-02,  2.9081e-02,  2.9425e-02,  2.9451e-02,
         2.9825e-02,  2.9993e-02,  2.9922e-02,  2.9931e-02,  2.9868e-02,
         2.9723e-02,  2.9554e-02,  2.9354e-02,  2.9121e-02,  2.8876e-02,
         2.8622e-02,  2.8248e-02,  2.7944e-02,  2.7628e-02,  2.7124e-02,
         2.6729e-02,  2.6300e-02,  2.5738e-02,  2.5191e-02,  2.4568e-02,
         2.3701e-02,  2.2856e-02,  2.2465e-02,  2.0464e-02,  1.9936e-02,
         1.9381e-02,  1.7444e-02,  1.6744e-02,  1.6001e-02,  1.3319e-02,
         1.2364e-02,  1.1354e-02,  7.6390e-03,  8.3988e-03,  7.1394e-03,
         4.9436e-03,  3.4631e-03,  4.3786e-03,  5.4604e-04,  1.5923e-03,
         2.5876e-03,  3.0484e-04,  9.8598e-04,  1.9978e-03,  3.0346e-04,
         1.7068e-03,  2.6919e-03,  1.4267e-03,  3.6534e-03,  5.6642e-03,
         4.5543e-03,  7.5187e-03,  9.1871e-03,  9.1843e-03,  1.1501e-02,
         1.3505e-02,  1.3488e-02,  1.5213e-02,  1.7178e-02,  1.7162e-02,
         1.8769e-02,  2.0082e-02,  2.0376e-02,  2.1390e-02,  2.2216e-02,
         2.2582e-02,  2.3187e-02,  2.3806e-02,  2.4004e-02,  2.4434e-02,
         2.4767e-02,  2.4864e-02,  2.5143e-02,  2.5348e-02,  2.5382e-02,
         2.5515e-02,  2.5611e-02,  2.5677e-02,  2.5698e-02,  2.5729e-02,
         2.5755e-02,  2.5746e-02,  2.5749e-02,  2.5742e-02,  2.5716e-02,
         2.5695e-02,  2.5671e-02,  2.5633e-02,  2.5598e-02,  2.5555e-02,
         2.5506e-02,  2.5456e-02,  2.5394e-02,  2.5327e-02,  2.5258e-02,
         2.5177e-02,  2.5090e-02,  2.4985e-02,  2.4863e-02,  2.4741e-02,
         2.4590e-02,  2.4427e-02,  2.4257e-02,  2.4042e-02,  2.3789e-02,
         2.3551e-02,  2.3243e-02,  2.2883e-02,  2.2512e-02,  2.2075e-02,
         2.1569e-02,  2.1128e-02,  2.0465e-02,  1.9782e-02,  1.9224e-02,
         1.8354e-02,  1.7489e-02,  1.6684e-02,  1.5768e-02,  1.4562e-02,
         1.3654e-02,  1.2520e-02,  1.1400e-02,  1.0491e-02,  9.1945e-03,
         7.9433e-03,  7.1411e-03,  5.9104e-03,  4.7820e-03,  4.0472e-03,
         3.0270e-03,  1.9529e-03,  1.7798e-03,  8.7324e-04,  4.0038e-04,
         2.0698e-04,  1.9959e-04,  1.9924e-04,  1.9924e-04,  1.9890e-04,
         8.7341e-04,  1.0636e-03,  1.4715e-03,  2.5502e-03,  3.1321e-03,
         3.8894e-03,  4.8186e-03,  5.8794e-03,  6.7718e-03,  7.8700e-03,
         9.0331e-03,  1.0101e-02,  1.1063e-02,  1.2124e-02,  1.3128e-02,
         1.3960e-02,  1.4874e-02,  1.5666e-02,  1.6334e-02,  1.6989e-02,
         1.7548e-02,  1.7963e-02,  1.8353e-02,  1.8631e-02,  1.8790e-02,
         1.8881e-02,  1.8872e-02,  1.8756e-02,  1.8564e-02,  1.8276e-02,
         1.7897e-02,  1.7441e-02,  1.6914e-02,  1.6311e-02,  1.5636e-02,
         1.4904e-02,  1.4131e-02,  1.3322e-02,  1.2481e-02,  1.1624e-02,
         1.0755e-02,  9.8892e-03,  9.0349e-03,  8.2045e-03,  7.3964e-03,
         6.6184e-03,  5.8782e-03,  5.1806e-03,  4.5246e-03,  3.9189e-03,
         3.3574e-03,  2.8453e-03,  2.3807e-03,  1.9658e-03,  1.5959e-03,
         1.2710e-03,  9.9079e-04,  7.5337e-04,  5.5327e-04,  3.8963e-04,
         2.6460e-04,  1.6906e-04,  1.0525e-04,  6.8621e-05,  4.9875e-05,
         1.4240e-04,  1.0027e-04,  1.0955e-04,  1.6029e-04,  2.5892e-04,
         4.4612e-04,  7.4116e-04,  1.1077e-03,  1.6316e-03,  2.3004e-03,
         3.0782e-03,  3.9887e-03,  4.9924e-03,  6.1038e-03,  7.2903e-03,
         8.5217e-03,  9.7985e-03,  1.1013e-02,  1.2182e-02,  1.3319e-02,
         1.4389e-02,  1.5362e-02,  1.6226e-02,  1.7000e-02,  1.7661e-02,
         1.8217e-02,  1.8683e-02,  1.9056e-02,  1.9339e-02,  1.9562e-02,
         1.9711e-02,  1.9798e-02,  1.9837e-02,  1.9823e-02,  1.9758e-02,
         1.9664e-02,  1.9518e-02,  1.9326e-02,  1.9109e-02,  1.8836e-02,
         1.8501e-02,  1.8120e-02,  1.7723e-02,  1.7227e-02,  1.6682e-02,
         1.6113e-02,  1.5517e-02,  1.4651e-02,  1.3927e-02,  1.3199e-02,
         1.2117e-02,  1.1294e-02,  1.0042e-02,  9.1483e-03,  8.1819e-03,
         7.0000e-03,  6.0183e-03,  5.3405e-03,  3.8850e-03,  3.3202e-03,
         2.7416e-03,  1.7347e-03,  1.3375e-03,  1.1681e-03,  3.2677e-04,
         3.9625e-04,  7.1218e-04,  3.4878e-04,  6.6463e-04,  1.2059e-03,
         1.3256e-03,  2.5168e-03,  3.2064e-03,  3.7193e-03,  4.7413e-03,
         6.0421e-03,  6.4593e-03,  7.6011e-03,  8.9035e-03,  9.4720e-03,
         1.0559e-02,  1.1512e-02,  1.2297e-02,  1.3203e-02,  1.3983e-02,
         1.4495e-02,  1.5193e-02,  1.5777e-02,  1.6169e-02,  1.6593e-02,
         1.7000e-02,  1.7273e-02,  1.7590e-02,  1.7841e-02,  1.8009e-02,
         1.8173e-02,  1.8281e-02,  1.8401e-02,  1.8504e-02,  1.8560e-02,
         1.8621e-02,  1.8664e-02,  1.8689e-02,  1.8719e-02,  1.8727e-02,
         1.8735e-02,  1.8742e-02,  1.8739e-02,  1.8732e-02,  1.8725e-02,
         1.8712e-02,  1.8699e-02,  1.8685e-02,  1.8668e-02,  1.8651e-02,
         1.8632e-02,  1.8614e-02,  1.8594e-02,  1.8574e-02,  1.8554e-02,
         1.8533e-02,  1.8512e-02,  1.8490e-02,  1.8469e-02,  1.8447e-02,
         1.8426e-02,  1.8404e-02,  1.8382e-02,  1.8361e-02,  1.8339e-02,
         1.8317e-02,  1.8295e-02,  1.8273e-02,  1.8252e-02,  1.8230e-02,
         1.8208e-02,  1.8187e-02,  1.8165e-02,  1.8144e-02,  1.8122e-02,
         1.8101e-02,  1.8080e-02,  1.8058e-02,  1.8037e-02,  1.8016e-02,
         1.7994e-02,  1.7973e-02,  1.7952e-02,  1.7930e-02,  1.7909e-02,
         1.7888e-02,  1.7867e-02,  1.7846e-02,  1.7825e-02,  1.7804e-02,
         1.7783e-02,  1.7762e-02,  1.7742e-02,  1.7721e-02,  1.7700e-02,
         1.7679e-02,  1.7659e-02,  1.7638e-02,  1.7617e-02,  1.7597e-02,
         1.7576e-02,  1.7556e-02,  1.7535e-02,  1.7515e-02,  1.7494e-02,
         1.7474e-02,  1.7454e-02,  1.7434e-02,  1.7413e-02,  1.7393e-02,
         1.7373e-02,  1.7353e-02,  1.7332e-02,  1.7312e-02,  1.7292e-02,
         1.7272e-02,  1.7252e-02,  1.7232e-02,  1.7212e-02,  1.7193e-02,
         1.7173e-02,  1.7153e-02,  1.7133e-02,  1.7113e-02,  1.7094e-02,
         1.7074e-02,  1.7054e-02,  1.7035e-02,  1.7015e-02,  1.6996e-02,
         1.6976e-02,  1.6957e-02,  1.6937e-02,  1.6918e-02,  1.6898e-02,
         1.6879e-02,  1.6860e-02,  1.6840e-02,  1.6821e-02,  1.6802e-02,
         1.6783e-02,  1.6763e-02,  1.6744e-02,  1.6725e-02,  1.6706e-02,
         1.6687e-02,  1.6668e-02,  1.6649e-02,  1.6630e-02,  1.6612e-02,
         1.6593e-02,  1.6574e-02,  1.6555e-02,  1.6536e-02,  1.6518e-02,
         1.6499e-02,  1.6480e-02,  1.6462e-02,  1.6443e-02,  1.6425e-02,
         1.6406e-02,  1.6387e-02,  1.6369e-02,  1.6350e-02,  1.6332e-02,
         1.6314e-02,  1.6295e-02,  1.6277e-02,  1.6259e-02,  1.6240e-02,
         1.6222e-02,  1.6204e-02,  1.6186e-02,  1.6168e-02,  1.6150e-02,
         1.6132e-02,  1.6113e-02,  1.6096e-02,  1.6077e-02,  1.6059e-02,
         1.6042e-02,  1.6024e-02,  1.6006e-02,  1.5988e-02,  1.5970e-02,
         1.5952e-02,  1.5935e-02,  1.5917e-02,  1.5899e-02,  1.5881e-02,
         1.5864e-02,  1.5846e-02,  1.5829e-02,  1.5811e-02,  1.5794e-02,
         1.5776e-02,  1.5759e-02,  1.5741e-02,  1.5724e-02,  1.5706e-02,
         1.5689e-02,  1.5672e-02,  1.5655e-02,  1.5638e-02,  1.5620e-02,
         1.5603e-02,  1.5586e-02,  1.5570e-02,  1.5553e-02,  1.5536e-02,
         1.5519e-02,  1.5503e-02,  1.5487e-02,  1.5471e-02,  1.5457e-02,
         1.5441e-02,  1.5425e-02,  1.5416e-02,  1.5401e-02,  1.5387e-02,
         1.5384e-02,  1.5371e-02,  1.5360e-02,  1.5363e-02,  1.5354e-02,
         1.5346e-02,  1.5381e-02,  1.5380e-02,  1.5363e-02,  1.5439e-02,
         1.5420e-02,  1.5432e-02,  1.5583e-02,  1.5562e-02,  1.5598e-02,
         1.5790e-02,  1.5767e-02,  1.5833e-02,  1.6144e-02,  1.6118e-02,
         1.6230e-02,  1.6719e-02,  1.6687e-02,  1.6655e-02,  1.7317e-02,
         1.7280e-02,  1.7243e-02,  1.8095e-02,  1.8052e-02,  1.8010e-02,
         1.8976e-02,  1.8931e-02,  1.8885e-02,  1.9496e-02,  1.9355e-02,
         1.9312e-02,  1.8801e-02,  1.8825e-02,  1.9347e-02,  1.7774e-02,
         1.7844e-02,  1.7912e-02,  1.5966e-02,  1.6112e-02,  1.6240e-02,
         7.6524e-04,  1.3120e-02,  1.3381e-02,  7.4465e-03,  7.9763e-03,
         7.5715e-04,  7.5561e-04,  7.5389e-04,  7.5234e-04,  7.5096e-04,
         1.0777e-02,  7.4769e-04,  3.4492e-03,  4.1389e-03,  7.4305e-04,
         1.2556e-02,  1.2836e-02,  6.8459e-03,  1.6432e-02,  1.6523e-02,
         1.3934e-02,  1.7991e-02,  1.8016e-02,  1.6923e-02,  1.8475e-02,
         1.8359e-02,  1.8475e-02,  1.8446e-02,  1.7989e-02,  1.8204e-02,
         1.7607e-02,  1.6966e-02,  1.7219e-02,  1.6608e-02,  1.6080e-02,
         1.6278e-02,  1.5802e-02,  1.5414e-02,  1.5552e-02,  1.5208e-02,
         1.4933e-02,  1.5025e-02,  1.4782e-02,  1.4589e-02,  1.4647e-02,
         1.4478e-02,  1.4252e-02,  1.4276e-02,  1.4179e-02,  1.4101e-02,
         1.4112e-02,  1.4002e-02,  1.3954e-02,  1.3953e-02,  1.3889e-02,
         1.3857e-02,  1.3850e-02,  1.3809e-02,  1.3786e-02,  1.3768e-02,
         1.3748e-02,  1.3729e-02,  1.3713e-02,  1.3695e-02,  1.3677e-02,
         1.3663e-02,  1.3647e-02,  1.3631e-02,  1.3617e-02,  1.3602e-02,
         1.3588e-02,  1.3573e-02,  1.3559e-02,  1.3545e-02,  1.3531e-02,
         1.3517e-02,  1.3503e-02,  1.3489e-02,  1.3475e-02,  1.3461e-02,
         1.3448e-02,  1.3434e-02,  1.3420e-02,  1.3406e-02,  1.3392e-02,
         1.3379e-02,  1.3365e-02,  1.3351e-02,  1.3338e-02,  1.3324e-02,
         1.3311e-02,  1.3297e-02,  1.3284e-02,  1.3270e-02,  1.3257e-02,
         1.3243e-02,  1.3230e-02,  1.3216e-02,  1.3203e-02,  1.3190e-02,
         1.3176e-02,  1.3163e-02,  1.3149e-02,  1.3136e-02,  1.3123e-02,
         1.3110e-02,  1.3096e-02,  1.3083e-02,  1.3070e-02,  1.3057e-02,
         1.3044e-02,  1.3031e-02,  1.3017e-02,  1.3004e-02,  1.2991e-02,
         1.2978e-02,  1.2965e-02,  1.2952e-02,  1.2939e-02,  1.2926e-02,
         1.2913e-02,  1.2900e-02,  1.2887e-02,  1.2874e-02,  1.2861e-02,
         1.2848e-02,  1.2836e-02,  1.2823e-02,  1.2810e-02,  1.2797e-02,
         1.2785e-02,  1.2772e-02,  1.2759e-02,  1.2746e-02,  1.2734e-02,
         1.2721e-02,  1.2708e-02,  1.2696e-02,  1.2683e-02,  1.2670e-02,
         1.2658e-02,  1.2645e-02,  1.2632e-02,  1.2620e-02,  1.2608e-02,
         1.2595e-02,  1.2583e-02,  1.2570e-02,  1.2558e-02,  1.2545e-02,
         1.2533e-02,  1.2521e-02,  1.2508e-02,  1.2496e-02,  1.2483e-02],
       device='cuda:0')
Selected points (indices): {259, 14}
Selected new x values (normalized): tensor([0.2593, 0.0140], device='cuda:0')
Corresponding new x values (unnormalized): tensor([ -962.9630, -1943.9440], device='cuda:0')
Plot saved to /raven/u/dvoss/al_pmssmwithgp/model/plots/Iter6/gp_plot.png
YAML file has been created: /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/data/new_config.yaml
Note: detected 144 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
Note: NumExpr detected 144 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
NumExpr defaulting to 8 threads.

  ___              ____ __  __          _       _   ___            
 | _ \ _  _  _ _  |__ /|  \/  | ___  __| | ___ | | / __| ___  _ _  
 |   /| || || ' \  |_ \| |\/| |/ _ \/ _` |/ -_)| || (_ |/ -_)| ' \ 
 |_|_\ \_,_||_||_||___/|_|  |_|\___/\__,_|\___||_| \___|\___||_||_|
                                                                   

2024-08-28 19:54:07 [info     ] Initialised ModelGenerator with: [Run3ModelGen.modelgen]
2024-08-28 19:54:07 [info     ] 	config_file = /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/data/new_config.yaml [Run3ModelGen.modelgen]
2024-08-28 19:54:07 [info     ] 	scan_dir = /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/scans/scan_6 [Run3ModelGen.modelgen]
2024-08-28 19:54:07 [info     ] 	rawfilen = /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/data/raw.slha [Run3ModelGen.modelgen]
2024-08-28 19:54:07 [info     ] 	seed = 123                    [Run3ModelGen.modelgen]
2024-08-28 19:54:07 [info     ] 	points = {}                   [Run3ModelGen.modelgen]
2024-08-28 19:54:07 [info     ] 	prior = fixed                 [Run3ModelGen.modelgen]
2024-08-28 19:54:07 [info     ] 	num_models = 2                [Run3ModelGen.modelgen]
2024-08-28 19:54:07 [info     ] 	isGMSB = False                [Run3ModelGen.modelgen]
2024-08-28 19:54:07 [info     ] 	parameters =                  [Run3ModelGen.modelgen]
2024-08-28 19:54:07 [info     ] 		tanb: [60, 60]               [Run3ModelGen.modelgen]
2024-08-28 19:54:07 [info     ] 		M_1: [-962.96, -1943.94]     [Run3ModelGen.modelgen]
2024-08-28 19:54:07 [info     ] 		M_2: [2000, 2000]            [Run3ModelGen.modelgen]
2024-08-28 19:54:07 [info     ] 		M_3: [4000, 4000]            [Run3ModelGen.modelgen]
2024-08-28 19:54:07 [info     ] 		AT: [4000, 4000]             [Run3ModelGen.modelgen]
2024-08-28 19:54:07 [info     ] 		Ab: [2000, 2000]             [Run3ModelGen.modelgen]
2024-08-28 19:54:07 [info     ] 		Atau: [2000, 2000]           [Run3ModelGen.modelgen]
2024-08-28 19:54:07 [info     ] 		mu: [2000, 2000]             [Run3ModelGen.modelgen]
2024-08-28 19:54:07 [info     ] 		mA: [2000, 2000]             [Run3ModelGen.modelgen]
2024-08-28 19:54:07 [info     ] 		meL: [2000, 2000]            [Run3ModelGen.modelgen]
2024-08-28 19:54:07 [info     ] 		mtauL: [2000, 2000]          [Run3ModelGen.modelgen]
2024-08-28 19:54:07 [info     ] 		meR: [2000, 2000]            [Run3ModelGen.modelgen]
2024-08-28 19:54:07 [info     ] 		mtauR: [2000, 2000]          [Run3ModelGen.modelgen]
2024-08-28 19:54:07 [info     ] 		mqL1: [4000, 4000]           [Run3ModelGen.modelgen]
2024-08-28 19:54:07 [info     ] 		mqL3: [4000, 4000]           [Run3ModelGen.modelgen]
2024-08-28 19:54:07 [info     ] 		muR: [4000, 4000]            [Run3ModelGen.modelgen]
2024-08-28 19:54:07 [info     ] 		mtR: [4000, 4000]            [Run3ModelGen.modelgen]
2024-08-28 19:54:07 [info     ] 		mdR: [4000, 4000]            [Run3ModelGen.modelgen]
2024-08-28 19:54:07 [info     ] 		mbR: [4000, 4000]            [Run3ModelGen.modelgen]
2024-08-28 19:54:07 [info     ] 	steps =                       [Run3ModelGen.modelgen]
2024-08-28 19:54:07 [info     ] 		{'name': 'prep_input', 'output_dir': 'input', 'prefix': 'IN'} [Run3ModelGen.modelgen]
2024-08-28 19:54:07 [info     ] 		{'name': 'SPheno', 'input_dir': 'input', 'output_dir': 'SPheno', 'log_dir': 'SPheno_log', 'prefix': 'SP'} [Run3ModelGen.modelgen]
2024-08-28 19:54:07 [info     ] 		{'name': 'softsusy', 'input_dir': 'input', 'output_dir': 'softsusy', 'prefix': 'SS'} [Run3ModelGen.modelgen]
2024-08-28 19:54:07 [info     ] 		{'name': 'micromegas', 'input_dir': 'SPheno', 'output_dir': 'micromegas', 'prefix': 'MO'} [Run3ModelGen.modelgen]
2024-08-28 19:54:07 [info     ] 		{'name': 'superiso', 'input_dir': 'SPheno', 'output_dir': 'superiso', 'prefix': 'SI'} [Run3ModelGen.modelgen]
2024-08-28 19:54:07 [info     ] 		{'name': 'gm2calc', 'input_dir': 'SPheno', 'output_dir': 'gm2calc', 'prefix': 'GM2'} [Run3ModelGen.modelgen]
2024-08-28 19:54:07 [info     ] 		{'name': 'evade', 'input_dir': 'SPheno', 'output_dir': 'evade', 'prefix': 'EV'} [Run3ModelGen.modelgen]
2024-08-28 19:54:07 [info     ] Starting Model Generation...   [Run3ModelGen.modelgen]
2024-08-28 19:54:07 [info     ] Setting up output directories for scan [Run3ModelGen.modelgen]
2024-08-28 19:54:07 [info     ] Generating Model: 0.slha (1/2) [Run3ModelGen.modelgen]
2024-08-28 19:54:07 [info     ] 	running step: prep_input      [Run3ModelGen.modelgen]
2024-08-28 19:54:07 [info     ] 	running step: SPheno          [Run3ModelGen.modelgen]
2024-08-28 19:54:07 [info     ] 	running step: softsusy        [Run3ModelGen.modelgen]
2024-08-28 19:54:07 [info     ] 	running step: micromegas      [Run3ModelGen.modelgen]
2024-08-28 19:54:08 [info     ] 	running step: superiso        [Run3ModelGen.modelgen]
2024-08-28 19:54:08 [info     ] 	running step: gm2calc         [Run3ModelGen.modelgen]
2024-08-28 19:54:08 [info     ] 	running step: evade           [Run3ModelGen.modelgen]
2024-08-28 19:54:08 [info     ] Generating Model: 1.slha (2/2) [Run3ModelGen.modelgen]
2024-08-28 19:54:08 [info     ] 	running step: prep_input      [Run3ModelGen.modelgen]
2024-08-28 19:54:08 [info     ] 	running step: SPheno          [Run3ModelGen.modelgen]
2024-08-28 19:54:08 [info     ] 	running step: softsusy        [Run3ModelGen.modelgen]
2024-08-28 19:54:08 [info     ] 	running step: micromegas      [Run3ModelGen.modelgen]
2024-08-28 19:54:10 [info     ] 	running step: superiso        [Run3ModelGen.modelgen]
2024-08-28 19:54:10 [info     ] 	running step: gm2calc         [Run3ModelGen.modelgen]
2024-08-28 19:54:10 [info     ] 	running step: evade           [Run3ModelGen.modelgen]
2024-08-28 19:54:10 [info     ] Finished generating models! Stats for successful generations (successful/attempted): [Run3ModelGen.modelgen]
2024-08-28 19:54:10 [info     ] 	prep_input: 2/2.              [Run3ModelGen.modelgen]
2024-08-28 19:54:10 [info     ] 	SPheno: 2/2.                  [Run3ModelGen.modelgen]
2024-08-28 19:54:10 [info     ] 	softsusy: 2/2.                [Run3ModelGen.modelgen]
2024-08-28 19:54:10 [info     ] 	micromegas: 2/2.              [Run3ModelGen.modelgen]
2024-08-28 19:54:10 [info     ] 	superiso: 2/2.                [Run3ModelGen.modelgen]
2024-08-28 19:54:10 [info     ] 	gm2calc: 2/2.                 [Run3ModelGen.modelgen]
2024-08-28 19:54:10 [info     ] 	evade: 0/2.                   [Run3ModelGen.modelgen]
2024-08-28 19:54:10 [info     ] Will make NTuple for: /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/scans/scan_6, with 2 models [Run3ModelGen.ntupling]
2024-08-28 19:54:10 [info     ] 	Filling values for model: 0   [Run3ModelGen.ntupling]
2024-08-28 19:54:10 [info     ] Saving NTuple in /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/scans/scan_6/ntuple.0.0.root ... [Run3ModelGen.ntupling]
Completed iteration 6
Starting iteration 7
Initial Training points:  tensor([0.8528, 0.5812, 0.1617, 0.0046], device='cuda:0') torch.Size([4])
Training points after adding:  tensor([0.0140, 0.4324, 0.1617, 0.1091, 0.0046, 0.2132, 0.0561, 0.3373, 0.2593,
        0.4995, 0.2723, 0.5175, 0.2392, 0.5812, 0.0210, 0.8528],
       device='cuda:0') torch.Size([16])
These training_points are used in the GP tensor([0.0140, 0.4324, 0.1617, 0.1091, 0.0046, 0.2132, 0.0561, 0.3373, 0.2593,
        0.4995, 0.2723, 0.5175, 0.2392, 0.5812, 0.0210, 0.8528],
       device='cuda:0')
Point: tensor([0.0000, 0.0010, 0.0020, 0.0030, 0.0040, 0.0050, 0.0060, 0.0070, 0.0080,
        0.0090, 0.0100, 0.0110, 0.0120, 0.0130, 0.0140, 0.0150, 0.0160, 0.0170,
        0.0180, 0.0190, 0.0200, 0.0210, 0.0220, 0.0230, 0.0240, 0.0250, 0.0260,
        0.0270, 0.0280, 0.0290, 0.0300, 0.0310, 0.0320, 0.0330, 0.0340, 0.0350,
        0.0360, 0.0370, 0.0380, 0.0390, 0.0400, 0.0410, 0.0420, 0.0430, 0.0440,
        0.0450, 0.0460, 0.0470, 0.0480, 0.0490, 0.0501, 0.0511, 0.0521, 0.0531,
        0.0541, 0.0551, 0.0561, 0.0571, 0.0581, 0.0591, 0.0601, 0.0611, 0.0621,
        0.0631, 0.0641, 0.0651, 0.0661, 0.0671, 0.0681, 0.0691, 0.0701, 0.0711,
        0.0721, 0.0731, 0.0741, 0.0751, 0.0761, 0.0771, 0.0781, 0.0791, 0.0801,
        0.0811, 0.0821, 0.0831, 0.0841, 0.0851, 0.0861, 0.0871, 0.0881, 0.0891,
        0.0901, 0.0911, 0.0921, 0.0931, 0.0941, 0.0951, 0.0961, 0.0971, 0.0981,
        0.0991, 0.1001, 0.1011, 0.1021, 0.1031, 0.1041, 0.1051, 0.1061, 0.1071,
        0.1081, 0.1091, 0.1101, 0.1111, 0.1121, 0.1131, 0.1141, 0.1151, 0.1161,
        0.1171, 0.1181, 0.1191, 0.1201, 0.1211, 0.1221, 0.1231, 0.1241, 0.1251,
        0.1261, 0.1271, 0.1281, 0.1291, 0.1301, 0.1311, 0.1321, 0.1331, 0.1341,
        0.1351, 0.1361, 0.1371, 0.1381, 0.1391, 0.1401, 0.1411, 0.1421, 0.1431,
        0.1441, 0.1451, 0.1461, 0.1471, 0.1481, 0.1491, 0.1502, 0.1512, 0.1522,
        0.1532, 0.1542, 0.1552, 0.1562, 0.1572, 0.1582, 0.1592, 0.1602, 0.1612,
        0.1622, 0.1632, 0.1642, 0.1652, 0.1662, 0.1672, 0.1682, 0.1692, 0.1702,
        0.1712, 0.1722, 0.1732, 0.1742, 0.1752, 0.1762, 0.1772, 0.1782, 0.1792,
        0.1802, 0.1812, 0.1822, 0.1832, 0.1842, 0.1852, 0.1862, 0.1872, 0.1882,
        0.1892, 0.1902, 0.1912, 0.1922, 0.1932, 0.1942, 0.1952, 0.1962, 0.1972,
        0.1982, 0.1992, 0.2002, 0.2012, 0.2022, 0.2032, 0.2042, 0.2052, 0.2062,
        0.2072, 0.2082, 0.2092, 0.2102, 0.2112, 0.2122, 0.2132, 0.2142, 0.2152,
        0.2162, 0.2172, 0.2182, 0.2192, 0.2202, 0.2212, 0.2222, 0.2232, 0.2242,
        0.2252, 0.2262, 0.2272, 0.2282, 0.2292, 0.2302, 0.2312, 0.2322, 0.2332,
        0.2342, 0.2352, 0.2362, 0.2372, 0.2382, 0.2392, 0.2402, 0.2412, 0.2422,
        0.2432, 0.2442, 0.2452, 0.2462, 0.2472, 0.2482, 0.2492, 0.2503, 0.2513,
        0.2523, 0.2533, 0.2543, 0.2553, 0.2563, 0.2573, 0.2583, 0.2593, 0.2603,
        0.2613, 0.2623, 0.2633, 0.2643, 0.2653, 0.2663, 0.2673, 0.2683, 0.2693,
        0.2703, 0.2713, 0.2723, 0.2733, 0.2743, 0.2753, 0.2763, 0.2773, 0.2783,
        0.2793, 0.2803, 0.2813, 0.2823, 0.2833, 0.2843, 0.2853, 0.2863, 0.2873,
        0.2883, 0.2893, 0.2903, 0.2913, 0.2923, 0.2933, 0.2943, 0.2953, 0.2963,
        0.2973, 0.2983, 0.2993, 0.3003, 0.3013, 0.3023, 0.3033, 0.3043, 0.3053,
        0.3063, 0.3073, 0.3083, 0.3093, 0.3103, 0.3113, 0.3123, 0.3133, 0.3143,
        0.3153, 0.3163, 0.3173, 0.3183, 0.3193, 0.3203, 0.3213, 0.3223, 0.3233,
        0.3243, 0.3253, 0.3263, 0.3273, 0.3283, 0.3293, 0.3303, 0.3313, 0.3323,
        0.3333, 0.3343, 0.3353, 0.3363, 0.3373, 0.3383, 0.3393, 0.3403, 0.3413,
        0.3423, 0.3433, 0.3443, 0.3453, 0.3463, 0.3473, 0.3483, 0.3493, 0.3504,
        0.3514, 0.3524, 0.3534, 0.3544, 0.3554, 0.3564, 0.3574, 0.3584, 0.3594,
        0.3604, 0.3614, 0.3624, 0.3634, 0.3644, 0.3654, 0.3664, 0.3674, 0.3684,
        0.3694, 0.3704, 0.3714, 0.3724, 0.3734, 0.3744, 0.3754, 0.3764, 0.3774,
        0.3784, 0.3794, 0.3804, 0.3814, 0.3824, 0.3834, 0.3844, 0.3854, 0.3864,
        0.3874, 0.3884, 0.3894, 0.3904, 0.3914, 0.3924, 0.3934, 0.3944, 0.3954,
        0.3964, 0.3974, 0.3984, 0.3994, 0.4004, 0.4014, 0.4024, 0.4034, 0.4044,
        0.4054, 0.4064, 0.4074, 0.4084, 0.4094, 0.4104, 0.4114, 0.4124, 0.4134,
        0.4144, 0.4154, 0.4164, 0.4174, 0.4184, 0.4194, 0.4204, 0.4214, 0.4224,
        0.4234, 0.4244, 0.4254, 0.4264, 0.4274, 0.4284, 0.4294, 0.4304, 0.4314,
        0.4324, 0.4334, 0.4344, 0.4354, 0.4364, 0.4374, 0.4384, 0.4394, 0.4404,
        0.4414, 0.4424, 0.4434, 0.4444, 0.4454, 0.4464, 0.4474, 0.4484, 0.4494,
        0.4505, 0.4515, 0.4525, 0.4535, 0.4545, 0.4555, 0.4565, 0.4575, 0.4585,
        0.4595, 0.4605, 0.4615, 0.4625, 0.4635, 0.4645, 0.4655, 0.4665, 0.4675,
        0.4685, 0.4695, 0.4705, 0.4715, 0.4725, 0.4735, 0.4745, 0.4755, 0.4765,
        0.4775, 0.4785, 0.4795, 0.4805, 0.4815, 0.4825, 0.4835, 0.4845, 0.4855,
        0.4865, 0.4875, 0.4885, 0.4895, 0.4905, 0.4915, 0.4925, 0.4935, 0.4945,
        0.4955, 0.4965, 0.4975, 0.4985, 0.4995, 0.5005, 0.5015, 0.5025, 0.5035,
        0.5045, 0.5055, 0.5065, 0.5075, 0.5085, 0.5095, 0.5105, 0.5115, 0.5125,
        0.5135, 0.5145, 0.5155, 0.5165, 0.5175, 0.5185, 0.5195, 0.5205, 0.5215,
        0.5225, 0.5235, 0.5245, 0.5255, 0.5265, 0.5275, 0.5285, 0.5295, 0.5305,
        0.5315, 0.5325, 0.5335, 0.5345, 0.5355, 0.5365, 0.5375, 0.5385, 0.5395,
        0.5405, 0.5415, 0.5425, 0.5435, 0.5445, 0.5455, 0.5465, 0.5475, 0.5485,
        0.5495, 0.5506, 0.5516, 0.5526, 0.5536, 0.5546, 0.5556, 0.5566, 0.5576,
        0.5586, 0.5596, 0.5606, 0.5616, 0.5626, 0.5636, 0.5646, 0.5656, 0.5666,
        0.5676, 0.5686, 0.5696, 0.5706, 0.5716, 0.5726, 0.5736, 0.5746, 0.5756,
        0.5766, 0.5776, 0.5786, 0.5796, 0.5806, 0.5816, 0.5826, 0.5836, 0.5846,
        0.5856, 0.5866, 0.5876, 0.5886, 0.5896, 0.5906, 0.5916, 0.5926, 0.5936,
        0.5946, 0.5956, 0.5966, 0.5976, 0.5986, 0.5996, 0.6006, 0.6016, 0.6026,
        0.6036, 0.6046, 0.6056, 0.6066, 0.6076, 0.6086, 0.6096, 0.6106, 0.6116,
        0.6126, 0.6136, 0.6146, 0.6156, 0.6166, 0.6176, 0.6186, 0.6196, 0.6206,
        0.6216, 0.6226, 0.6236, 0.6246, 0.6256, 0.6266, 0.6276, 0.6286, 0.6296,
        0.6306, 0.6316, 0.6326, 0.6336, 0.6346, 0.6356, 0.6366, 0.6376, 0.6386,
        0.6396, 0.6406, 0.6416, 0.6426, 0.6436, 0.6446, 0.6456, 0.6466, 0.6476,
        0.6486, 0.6496, 0.6507, 0.6517, 0.6527, 0.6537, 0.6547, 0.6557, 0.6567,
        0.6577, 0.6587, 0.6597, 0.6607, 0.6617, 0.6627, 0.6637, 0.6647, 0.6657,
        0.6667, 0.6677, 0.6687, 0.6697, 0.6707, 0.6717, 0.6727, 0.6737, 0.6747,
        0.6757, 0.6767, 0.6777, 0.6787, 0.6797, 0.6807, 0.6817, 0.6827, 0.6837,
        0.6847, 0.6857, 0.6867, 0.6877, 0.6887, 0.6897, 0.6907, 0.6917, 0.6927,
        0.6937, 0.6947, 0.6957, 0.6967, 0.6977, 0.6987, 0.6997, 0.7007, 0.7017,
        0.7027, 0.7037, 0.7047, 0.7057, 0.7067, 0.7077, 0.7087, 0.7097, 0.7107,
        0.7117, 0.7127, 0.7137, 0.7147, 0.7157, 0.7167, 0.7177, 0.7187, 0.7197,
        0.7207, 0.7217, 0.7227, 0.7237, 0.7247, 0.7257, 0.7267, 0.7277, 0.7287,
        0.7297, 0.7307, 0.7317, 0.7327, 0.7337, 0.7347, 0.7357, 0.7367, 0.7377,
        0.7387, 0.7397, 0.7407, 0.7417, 0.7427, 0.7437, 0.7447, 0.7457, 0.7467,
        0.7477, 0.7487, 0.7497, 0.7508, 0.7518, 0.7528, 0.7538, 0.7548, 0.7558,
        0.7568, 0.7578, 0.7588, 0.7598, 0.7608, 0.7618, 0.7628, 0.7638, 0.7648,
        0.7658, 0.7668, 0.7678, 0.7688, 0.7698, 0.7708, 0.7718, 0.7728, 0.7738,
        0.7748, 0.7758, 0.7768, 0.7778, 0.7788, 0.7798, 0.7808, 0.7818, 0.7828,
        0.7838, 0.7848, 0.7858, 0.7868, 0.7878, 0.7888, 0.7898, 0.7908, 0.7918,
        0.7928, 0.7938, 0.7948, 0.7958, 0.7968, 0.7978, 0.7988, 0.7998, 0.8008,
        0.8018, 0.8028, 0.8038, 0.8048, 0.8058, 0.8068, 0.8078, 0.8088, 0.8098,
        0.8108, 0.8118, 0.8128, 0.8138, 0.8148, 0.8158, 0.8168, 0.8178, 0.8188,
        0.8198, 0.8208, 0.8218, 0.8228, 0.8238, 0.8248, 0.8258, 0.8268, 0.8278,
        0.8288, 0.8298, 0.8308, 0.8318, 0.8328, 0.8338, 0.8348, 0.8358, 0.8368,
        0.8378, 0.8388, 0.8398, 0.8408, 0.8418, 0.8428, 0.8438, 0.8448, 0.8458,
        0.8468, 0.8478, 0.8488, 0.8498, 0.8509, 0.8519, 0.8529, 0.8539, 0.8549,
        0.8559, 0.8569, 0.8579, 0.8589, 0.8599, 0.8609, 0.8619, 0.8629, 0.8639,
        0.8649, 0.8659, 0.8669, 0.8679, 0.8689, 0.8699, 0.8709, 0.8719, 0.8729,
        0.8739, 0.8749, 0.8759, 0.8769, 0.8779, 0.8789, 0.8799, 0.8809, 0.8819,
        0.8829, 0.8839, 0.8849, 0.8859, 0.8869, 0.8879, 0.8889, 0.8899, 0.8909,
        0.8919, 0.8929, 0.8939, 0.8949, 0.8959, 0.8969, 0.8979, 0.8989, 0.8999,
        0.9009, 0.9019, 0.9029, 0.9039, 0.9049, 0.9059, 0.9069, 0.9079, 0.9089,
        0.9099, 0.9109, 0.9119, 0.9129, 0.9139, 0.9149, 0.9159, 0.9169, 0.9179,
        0.9189, 0.9199, 0.9209, 0.9219, 0.9229, 0.9239, 0.9249, 0.9259, 0.9269,
        0.9279, 0.9289, 0.9299, 0.9309, 0.9319, 0.9329, 0.9339, 0.9349, 0.9359,
        0.9369, 0.9379, 0.9389, 0.9399, 0.9409, 0.9419, 0.9429, 0.9439, 0.9449,
        0.9459, 0.9469, 0.9479, 0.9489, 0.9499, 0.9510, 0.9520, 0.9530, 0.9540,
        0.9550, 0.9560, 0.9570, 0.9580, 0.9590, 0.9600, 0.9610, 0.9620, 0.9630,
        0.9640, 0.9650, 0.9660, 0.9670, 0.9680, 0.9690, 0.9700, 0.9710, 0.9720,
        0.9730, 0.9740, 0.9750, 0.9760, 0.9770, 0.9780, 0.9790, 0.9800, 0.9810,
        0.9820, 0.9830, 0.9840, 0.9850, 0.9860, 0.9870, 0.9880, 0.9890, 0.9900,
        0.9910, 0.9920, 0.9930, 0.9940, 0.9950, 0.9960, 0.9970, 0.9980, 0.9990,
        1.0000], device='cuda:0') - Variance: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0') 
Smoothed_batch_entropy:  tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan], device='cuda:0')
Selected points (indices): {0}
Selected new x values (normalized): tensor([0.], device='cuda:0')
Corresponding new x values (unnormalized): tensor([-2000.], device='cuda:0')
Plot saved to /raven/u/dvoss/al_pmssmwithgp/model/plots/Iter7/gp_plot.png
YAML file has been created: /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/data/new_config.yaml
Note: detected 144 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
Note: NumExpr detected 144 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
NumExpr defaulting to 8 threads.

  ___              ____ __  __          _       _   ___            
 | _ \ _  _  _ _  |__ /|  \/  | ___  __| | ___ | | / __| ___  _ _  
 |   /| || || ' \  |_ \| |\/| |/ _ \/ _` |/ -_)| || (_ |/ -_)| ' \ 
 |_|_\ \_,_||_||_||___/|_|  |_|\___/\__,_|\___||_| \___|\___||_||_|
                                                                   

2024-08-28 19:54:54 [info     ] Initialised ModelGenerator with: [Run3ModelGen.modelgen]
2024-08-28 19:54:54 [info     ] 	config_file = /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/data/new_config.yaml [Run3ModelGen.modelgen]
2024-08-28 19:54:54 [info     ] 	scan_dir = /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/scans/scan_7 [Run3ModelGen.modelgen]
2024-08-28 19:54:54 [info     ] 	rawfilen = /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/data/raw.slha [Run3ModelGen.modelgen]
2024-08-28 19:54:54 [info     ] 	seed = 123                    [Run3ModelGen.modelgen]
2024-08-28 19:54:54 [info     ] 	points = {}                   [Run3ModelGen.modelgen]
2024-08-28 19:54:54 [info     ] 	prior = fixed                 [Run3ModelGen.modelgen]
2024-08-28 19:54:54 [info     ] 	num_models = 1                [Run3ModelGen.modelgen]
2024-08-28 19:54:54 [info     ] 	isGMSB = False                [Run3ModelGen.modelgen]
2024-08-28 19:54:54 [info     ] 	parameters =                  [Run3ModelGen.modelgen]
2024-08-28 19:54:54 [info     ] 		tanb: [60]                   [Run3ModelGen.modelgen]
2024-08-28 19:54:54 [info     ] 		M_1: [-2000.0]               [Run3ModelGen.modelgen]
2024-08-28 19:54:54 [info     ] 		M_2: [2000]                  [Run3ModelGen.modelgen]
2024-08-28 19:54:54 [info     ] 		M_3: [4000]                  [Run3ModelGen.modelgen]
2024-08-28 19:54:54 [info     ] 		AT: [4000]                   [Run3ModelGen.modelgen]
2024-08-28 19:54:54 [info     ] 		Ab: [2000]                   [Run3ModelGen.modelgen]
2024-08-28 19:54:54 [info     ] 		Atau: [2000]                 [Run3ModelGen.modelgen]
2024-08-28 19:54:54 [info     ] 		mu: [2000]                   [Run3ModelGen.modelgen]
2024-08-28 19:54:54 [info     ] 		mA: [2000]                   [Run3ModelGen.modelgen]
2024-08-28 19:54:54 [info     ] 		meL: [2000]                  [Run3ModelGen.modelgen]
2024-08-28 19:54:54 [info     ] 		mtauL: [2000]                [Run3ModelGen.modelgen]
2024-08-28 19:54:54 [info     ] 		meR: [2000]                  [Run3ModelGen.modelgen]
2024-08-28 19:54:54 [info     ] 		mtauR: [2000]                [Run3ModelGen.modelgen]
2024-08-28 19:54:54 [info     ] 		mqL1: [4000]                 [Run3ModelGen.modelgen]
2024-08-28 19:54:54 [info     ] 		mqL3: [4000]                 [Run3ModelGen.modelgen]
2024-08-28 19:54:54 [info     ] 		muR: [4000]                  [Run3ModelGen.modelgen]
2024-08-28 19:54:54 [info     ] 		mtR: [4000]                  [Run3ModelGen.modelgen]
2024-08-28 19:54:54 [info     ] 		mdR: [4000]                  [Run3ModelGen.modelgen]
2024-08-28 19:54:54 [info     ] 		mbR: [4000]                  [Run3ModelGen.modelgen]
2024-08-28 19:54:54 [info     ] 	steps =                       [Run3ModelGen.modelgen]
2024-08-28 19:54:54 [info     ] 		{'name': 'prep_input', 'output_dir': 'input', 'prefix': 'IN'} [Run3ModelGen.modelgen]
2024-08-28 19:54:54 [info     ] 		{'name': 'SPheno', 'input_dir': 'input', 'output_dir': 'SPheno', 'log_dir': 'SPheno_log', 'prefix': 'SP'} [Run3ModelGen.modelgen]
2024-08-28 19:54:54 [info     ] 		{'name': 'softsusy', 'input_dir': 'input', 'output_dir': 'softsusy', 'prefix': 'SS'} [Run3ModelGen.modelgen]
2024-08-28 19:54:54 [info     ] 		{'name': 'micromegas', 'input_dir': 'SPheno', 'output_dir': 'micromegas', 'prefix': 'MO'} [Run3ModelGen.modelgen]
2024-08-28 19:54:54 [info     ] 		{'name': 'superiso', 'input_dir': 'SPheno', 'output_dir': 'superiso', 'prefix': 'SI'} [Run3ModelGen.modelgen]
2024-08-28 19:54:54 [info     ] 		{'name': 'gm2calc', 'input_dir': 'SPheno', 'output_dir': 'gm2calc', 'prefix': 'GM2'} [Run3ModelGen.modelgen]
2024-08-28 19:54:54 [info     ] 		{'name': 'evade', 'input_dir': 'SPheno', 'output_dir': 'evade', 'prefix': 'EV'} [Run3ModelGen.modelgen]
2024-08-28 19:54:54 [info     ] Starting Model Generation...   [Run3ModelGen.modelgen]
2024-08-28 19:54:54 [info     ] Setting up output directories for scan [Run3ModelGen.modelgen]
2024-08-28 19:54:54 [info     ] Generating Model: 0.slha (1/1) [Run3ModelGen.modelgen]
2024-08-28 19:54:54 [info     ] 	running step: prep_input      [Run3ModelGen.modelgen]
2024-08-28 19:54:54 [info     ] 	running step: SPheno          [Run3ModelGen.modelgen]
2024-08-28 19:54:55 [info     ] 	running step: softsusy        [Run3ModelGen.modelgen]
2024-08-28 19:54:55 [info     ] 	running step: micromegas      [Run3ModelGen.modelgen]
2024-08-28 19:54:56 [info     ] 	running step: superiso        [Run3ModelGen.modelgen]
2024-08-28 19:54:56 [info     ] 	running step: gm2calc         [Run3ModelGen.modelgen]
2024-08-28 19:54:56 [info     ] 	running step: evade           [Run3ModelGen.modelgen]
2024-08-28 19:54:56 [info     ] Finished generating models! Stats for successful generations (successful/attempted): [Run3ModelGen.modelgen]
2024-08-28 19:54:56 [info     ] 	prep_input: 1/1.              [Run3ModelGen.modelgen]
2024-08-28 19:54:56 [info     ] 	SPheno: 1/1.                  [Run3ModelGen.modelgen]
2024-08-28 19:54:56 [info     ] 	softsusy: 1/1.                [Run3ModelGen.modelgen]
2024-08-28 19:54:56 [info     ] 	micromegas: 1/1.              [Run3ModelGen.modelgen]
2024-08-28 19:54:56 [info     ] 	superiso: 1/1.                [Run3ModelGen.modelgen]
2024-08-28 19:54:56 [info     ] 	gm2calc: 1/1.                 [Run3ModelGen.modelgen]
2024-08-28 19:54:56 [info     ] 	evade: 0/1.                   [Run3ModelGen.modelgen]
2024-08-28 19:54:56 [info     ] Will make NTuple for: /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/scans/scan_7, with 1 models [Run3ModelGen.ntupling]
2024-08-28 19:54:56 [info     ] 	Filling values for model: 0   [Run3ModelGen.ntupling]
2024-08-28 19:54:56 [info     ] Saving NTuple in /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/scans/scan_7/ntuple.0.0.root ... [Run3ModelGen.ntupling]
Completed iteration 7
Starting iteration 8
Initial Training points:  tensor([0.8528, 0.5812, 0.1617, 0.0046], device='cuda:0') torch.Size([4])
Training points after adding:  tensor([0.0000, 0.0140, 0.4324, 0.1617, 0.1091, 0.0046, 0.2132, 0.0561, 0.3373,
        0.2593, 0.4995, 0.2723, 0.5175, 0.2392, 0.5812, 0.0210, 0.8528],
       device='cuda:0') torch.Size([17])
These training_points are used in the GP tensor([0.0000, 0.0140, 0.4324, 0.1617, 0.1091, 0.0046, 0.2132, 0.0561, 0.3373,
        0.2593, 0.4995, 0.2723, 0.5175, 0.2392, 0.5812, 0.0210, 0.8528],
       device='cuda:0')
Point: tensor([0.0000, 0.0010, 0.0020, 0.0030, 0.0040, 0.0050, 0.0060, 0.0070, 0.0080,
        0.0090, 0.0100, 0.0110, 0.0120, 0.0130, 0.0140, 0.0150, 0.0160, 0.0170,
        0.0180, 0.0190, 0.0200, 0.0210, 0.0220, 0.0230, 0.0240, 0.0250, 0.0260,
        0.0270, 0.0280, 0.0290, 0.0300, 0.0310, 0.0320, 0.0330, 0.0340, 0.0350,
        0.0360, 0.0370, 0.0380, 0.0390, 0.0400, 0.0410, 0.0420, 0.0430, 0.0440,
        0.0450, 0.0460, 0.0470, 0.0480, 0.0490, 0.0501, 0.0511, 0.0521, 0.0531,
        0.0541, 0.0551, 0.0561, 0.0571, 0.0581, 0.0591, 0.0601, 0.0611, 0.0621,
        0.0631, 0.0641, 0.0651, 0.0661, 0.0671, 0.0681, 0.0691, 0.0701, 0.0711,
        0.0721, 0.0731, 0.0741, 0.0751, 0.0761, 0.0771, 0.0781, 0.0791, 0.0801,
        0.0811, 0.0821, 0.0831, 0.0841, 0.0851, 0.0861, 0.0871, 0.0881, 0.0891,
        0.0901, 0.0911, 0.0921, 0.0931, 0.0941, 0.0951, 0.0961, 0.0971, 0.0981,
        0.0991, 0.1001, 0.1011, 0.1021, 0.1031, 0.1041, 0.1051, 0.1061, 0.1071,
        0.1081, 0.1091, 0.1101, 0.1111, 0.1121, 0.1131, 0.1141, 0.1151, 0.1161,
        0.1171, 0.1181, 0.1191, 0.1201, 0.1211, 0.1221, 0.1231, 0.1241, 0.1251,
        0.1261, 0.1271, 0.1281, 0.1291, 0.1301, 0.1311, 0.1321, 0.1331, 0.1341,
        0.1351, 0.1361, 0.1371, 0.1381, 0.1391, 0.1401, 0.1411, 0.1421, 0.1431,
        0.1441, 0.1451, 0.1461, 0.1471, 0.1481, 0.1491, 0.1502, 0.1512, 0.1522,
        0.1532, 0.1542, 0.1552, 0.1562, 0.1572, 0.1582, 0.1592, 0.1602, 0.1612,
        0.1622, 0.1632, 0.1642, 0.1652, 0.1662, 0.1672, 0.1682, 0.1692, 0.1702,
        0.1712, 0.1722, 0.1732, 0.1742, 0.1752, 0.1762, 0.1772, 0.1782, 0.1792,
        0.1802, 0.1812, 0.1822, 0.1832, 0.1842, 0.1852, 0.1862, 0.1872, 0.1882,
        0.1892, 0.1902, 0.1912, 0.1922, 0.1932, 0.1942, 0.1952, 0.1962, 0.1972,
        0.1982, 0.1992, 0.2002, 0.2012, 0.2022, 0.2032, 0.2042, 0.2052, 0.2062,
        0.2072, 0.2082, 0.2092, 0.2102, 0.2112, 0.2122, 0.2132, 0.2142, 0.2152,
        0.2162, 0.2172, 0.2182, 0.2192, 0.2202, 0.2212, 0.2222, 0.2232, 0.2242,
        0.2252, 0.2262, 0.2272, 0.2282, 0.2292, 0.2302, 0.2312, 0.2322, 0.2332,
        0.2342, 0.2352, 0.2362, 0.2372, 0.2382, 0.2392, 0.2402, 0.2412, 0.2422,
        0.2432, 0.2442, 0.2452, 0.2462, 0.2472, 0.2482, 0.2492, 0.2503, 0.2513,
        0.2523, 0.2533, 0.2543, 0.2553, 0.2563, 0.2573, 0.2583, 0.2593, 0.2603,
        0.2613, 0.2623, 0.2633, 0.2643, 0.2653, 0.2663, 0.2673, 0.2683, 0.2693,
        0.2703, 0.2713, 0.2723, 0.2733, 0.2743, 0.2753, 0.2763, 0.2773, 0.2783,
        0.2793, 0.2803, 0.2813, 0.2823, 0.2833, 0.2843, 0.2853, 0.2863, 0.2873,
        0.2883, 0.2893, 0.2903, 0.2913, 0.2923, 0.2933, 0.2943, 0.2953, 0.2963,
        0.2973, 0.2983, 0.2993, 0.3003, 0.3013, 0.3023, 0.3033, 0.3043, 0.3053,
        0.3063, 0.3073, 0.3083, 0.3093, 0.3103, 0.3113, 0.3123, 0.3133, 0.3143,
        0.3153, 0.3163, 0.3173, 0.3183, 0.3193, 0.3203, 0.3213, 0.3223, 0.3233,
        0.3243, 0.3253, 0.3263, 0.3273, 0.3283, 0.3293, 0.3303, 0.3313, 0.3323,
        0.3333, 0.3343, 0.3353, 0.3363, 0.3373, 0.3383, 0.3393, 0.3403, 0.3413,
        0.3423, 0.3433, 0.3443, 0.3453, 0.3463, 0.3473, 0.3483, 0.3493, 0.3504,
        0.3514, 0.3524, 0.3534, 0.3544, 0.3554, 0.3564, 0.3574, 0.3584, 0.3594,
        0.3604, 0.3614, 0.3624, 0.3634, 0.3644, 0.3654, 0.3664, 0.3674, 0.3684,
        0.3694, 0.3704, 0.3714, 0.3724, 0.3734, 0.3744, 0.3754, 0.3764, 0.3774,
        0.3784, 0.3794, 0.3804, 0.3814, 0.3824, 0.3834, 0.3844, 0.3854, 0.3864,
        0.3874, 0.3884, 0.3894, 0.3904, 0.3914, 0.3924, 0.3934, 0.3944, 0.3954,
        0.3964, 0.3974, 0.3984, 0.3994, 0.4004, 0.4014, 0.4024, 0.4034, 0.4044,
        0.4054, 0.4064, 0.4074, 0.4084, 0.4094, 0.4104, 0.4114, 0.4124, 0.4134,
        0.4144, 0.4154, 0.4164, 0.4174, 0.4184, 0.4194, 0.4204, 0.4214, 0.4224,
        0.4234, 0.4244, 0.4254, 0.4264, 0.4274, 0.4284, 0.4294, 0.4304, 0.4314,
        0.4324, 0.4334, 0.4344, 0.4354, 0.4364, 0.4374, 0.4384, 0.4394, 0.4404,
        0.4414, 0.4424, 0.4434, 0.4444, 0.4454, 0.4464, 0.4474, 0.4484, 0.4494,
        0.4505, 0.4515, 0.4525, 0.4535, 0.4545, 0.4555, 0.4565, 0.4575, 0.4585,
        0.4595, 0.4605, 0.4615, 0.4625, 0.4635, 0.4645, 0.4655, 0.4665, 0.4675,
        0.4685, 0.4695, 0.4705, 0.4715, 0.4725, 0.4735, 0.4745, 0.4755, 0.4765,
        0.4775, 0.4785, 0.4795, 0.4805, 0.4815, 0.4825, 0.4835, 0.4845, 0.4855,
        0.4865, 0.4875, 0.4885, 0.4895, 0.4905, 0.4915, 0.4925, 0.4935, 0.4945,
        0.4955, 0.4965, 0.4975, 0.4985, 0.4995, 0.5005, 0.5015, 0.5025, 0.5035,
        0.5045, 0.5055, 0.5065, 0.5075, 0.5085, 0.5095, 0.5105, 0.5115, 0.5125,
        0.5135, 0.5145, 0.5155, 0.5165, 0.5175, 0.5185, 0.5195, 0.5205, 0.5215,
        0.5225, 0.5235, 0.5245, 0.5255, 0.5265, 0.5275, 0.5285, 0.5295, 0.5305,
        0.5315, 0.5325, 0.5335, 0.5345, 0.5355, 0.5365, 0.5375, 0.5385, 0.5395,
        0.5405, 0.5415, 0.5425, 0.5435, 0.5445, 0.5455, 0.5465, 0.5475, 0.5485,
        0.5495, 0.5506, 0.5516, 0.5526, 0.5536, 0.5546, 0.5556, 0.5566, 0.5576,
        0.5586, 0.5596, 0.5606, 0.5616, 0.5626, 0.5636, 0.5646, 0.5656, 0.5666,
        0.5676, 0.5686, 0.5696, 0.5706, 0.5716, 0.5726, 0.5736, 0.5746, 0.5756,
        0.5766, 0.5776, 0.5786, 0.5796, 0.5806, 0.5816, 0.5826, 0.5836, 0.5846,
        0.5856, 0.5866, 0.5876, 0.5886, 0.5896, 0.5906, 0.5916, 0.5926, 0.5936,
        0.5946, 0.5956, 0.5966, 0.5976, 0.5986, 0.5996, 0.6006, 0.6016, 0.6026,
        0.6036, 0.6046, 0.6056, 0.6066, 0.6076, 0.6086, 0.6096, 0.6106, 0.6116,
        0.6126, 0.6136, 0.6146, 0.6156, 0.6166, 0.6176, 0.6186, 0.6196, 0.6206,
        0.6216, 0.6226, 0.6236, 0.6246, 0.6256, 0.6266, 0.6276, 0.6286, 0.6296,
        0.6306, 0.6316, 0.6326, 0.6336, 0.6346, 0.6356, 0.6366, 0.6376, 0.6386,
        0.6396, 0.6406, 0.6416, 0.6426, 0.6436, 0.6446, 0.6456, 0.6466, 0.6476,
        0.6486, 0.6496, 0.6507, 0.6517, 0.6527, 0.6537, 0.6547, 0.6557, 0.6567,
        0.6577, 0.6587, 0.6597, 0.6607, 0.6617, 0.6627, 0.6637, 0.6647, 0.6657,
        0.6667, 0.6677, 0.6687, 0.6697, 0.6707, 0.6717, 0.6727, 0.6737, 0.6747,
        0.6757, 0.6767, 0.6777, 0.6787, 0.6797, 0.6807, 0.6817, 0.6827, 0.6837,
        0.6847, 0.6857, 0.6867, 0.6877, 0.6887, 0.6897, 0.6907, 0.6917, 0.6927,
        0.6937, 0.6947, 0.6957, 0.6967, 0.6977, 0.6987, 0.6997, 0.7007, 0.7017,
        0.7027, 0.7037, 0.7047, 0.7057, 0.7067, 0.7077, 0.7087, 0.7097, 0.7107,
        0.7117, 0.7127, 0.7137, 0.7147, 0.7157, 0.7167, 0.7177, 0.7187, 0.7197,
        0.7207, 0.7217, 0.7227, 0.7237, 0.7247, 0.7257, 0.7267, 0.7277, 0.7287,
        0.7297, 0.7307, 0.7317, 0.7327, 0.7337, 0.7347, 0.7357, 0.7367, 0.7377,
        0.7387, 0.7397, 0.7407, 0.7417, 0.7427, 0.7437, 0.7447, 0.7457, 0.7467,
        0.7477, 0.7487, 0.7497, 0.7508, 0.7518, 0.7528, 0.7538, 0.7548, 0.7558,
        0.7568, 0.7578, 0.7588, 0.7598, 0.7608, 0.7618, 0.7628, 0.7638, 0.7648,
        0.7658, 0.7668, 0.7678, 0.7688, 0.7698, 0.7708, 0.7718, 0.7728, 0.7738,
        0.7748, 0.7758, 0.7768, 0.7778, 0.7788, 0.7798, 0.7808, 0.7818, 0.7828,
        0.7838, 0.7848, 0.7858, 0.7868, 0.7878, 0.7888, 0.7898, 0.7908, 0.7918,
        0.7928, 0.7938, 0.7948, 0.7958, 0.7968, 0.7978, 0.7988, 0.7998, 0.8008,
        0.8018, 0.8028, 0.8038, 0.8048, 0.8058, 0.8068, 0.8078, 0.8088, 0.8098,
        0.8108, 0.8118, 0.8128, 0.8138, 0.8148, 0.8158, 0.8168, 0.8178, 0.8188,
        0.8198, 0.8208, 0.8218, 0.8228, 0.8238, 0.8248, 0.8258, 0.8268, 0.8278,
        0.8288, 0.8298, 0.8308, 0.8318, 0.8328, 0.8338, 0.8348, 0.8358, 0.8368,
        0.8378, 0.8388, 0.8398, 0.8408, 0.8418, 0.8428, 0.8438, 0.8448, 0.8458,
        0.8468, 0.8478, 0.8488, 0.8498, 0.8509, 0.8519, 0.8529, 0.8539, 0.8549,
        0.8559, 0.8569, 0.8579, 0.8589, 0.8599, 0.8609, 0.8619, 0.8629, 0.8639,
        0.8649, 0.8659, 0.8669, 0.8679, 0.8689, 0.8699, 0.8709, 0.8719, 0.8729,
        0.8739, 0.8749, 0.8759, 0.8769, 0.8779, 0.8789, 0.8799, 0.8809, 0.8819,
        0.8829, 0.8839, 0.8849, 0.8859, 0.8869, 0.8879, 0.8889, 0.8899, 0.8909,
        0.8919, 0.8929, 0.8939, 0.8949, 0.8959, 0.8969, 0.8979, 0.8989, 0.8999,
        0.9009, 0.9019, 0.9029, 0.9039, 0.9049, 0.9059, 0.9069, 0.9079, 0.9089,
        0.9099, 0.9109, 0.9119, 0.9129, 0.9139, 0.9149, 0.9159, 0.9169, 0.9179,
        0.9189, 0.9199, 0.9209, 0.9219, 0.9229, 0.9239, 0.9249, 0.9259, 0.9269,
        0.9279, 0.9289, 0.9299, 0.9309, 0.9319, 0.9329, 0.9339, 0.9349, 0.9359,
        0.9369, 0.9379, 0.9389, 0.9399, 0.9409, 0.9419, 0.9429, 0.9439, 0.9449,
        0.9459, 0.9469, 0.9479, 0.9489, 0.9499, 0.9510, 0.9520, 0.9530, 0.9540,
        0.9550, 0.9560, 0.9570, 0.9580, 0.9590, 0.9600, 0.9610, 0.9620, 0.9630,
        0.9640, 0.9650, 0.9660, 0.9670, 0.9680, 0.9690, 0.9700, 0.9710, 0.9720,
        0.9730, 0.9740, 0.9750, 0.9760, 0.9770, 0.9780, 0.9790, 0.9800, 0.9810,
        0.9820, 0.9830, 0.9840, 0.9850, 0.9860, 0.9870, 0.9880, 0.9890, 0.9900,
        0.9910, 0.9920, 0.9930, 0.9940, 0.9950, 0.9960, 0.9970, 0.9980, 0.9990,
        1.0000], device='cuda:0') - Variance: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0') 
Smoothed_batch_entropy:  tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan], device='cuda:0')
Selected points (indices): {0}
Selected new x values (normalized): tensor([0.], device='cuda:0')
Corresponding new x values (unnormalized): tensor([-2000.], device='cuda:0')
Plot saved to /raven/u/dvoss/al_pmssmwithgp/model/plots/Iter8/gp_plot.png
YAML file has been created: /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/data/new_config.yaml
Note: detected 144 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
Note: NumExpr detected 144 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
NumExpr defaulting to 8 threads.

  ___              ____ __  __          _       _   ___            
 | _ \ _  _  _ _  |__ /|  \/  | ___  __| | ___ | | / __| ___  _ _  
 |   /| || || ' \  |_ \| |\/| |/ _ \/ _` |/ -_)| || (_ |/ -_)| ' \ 
 |_|_\ \_,_||_||_||___/|_|  |_|\___/\__,_|\___||_| \___|\___||_||_|
                                                                   

2024-08-28 19:55:41 [info     ] Initialised ModelGenerator with: [Run3ModelGen.modelgen]
2024-08-28 19:55:41 [info     ] 	config_file = /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/data/new_config.yaml [Run3ModelGen.modelgen]
2024-08-28 19:55:41 [info     ] 	scan_dir = /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/scans/scan_8 [Run3ModelGen.modelgen]
2024-08-28 19:55:41 [info     ] 	rawfilen = /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/data/raw.slha [Run3ModelGen.modelgen]
2024-08-28 19:55:41 [info     ] 	seed = 123                    [Run3ModelGen.modelgen]
2024-08-28 19:55:41 [info     ] 	points = {}                   [Run3ModelGen.modelgen]
2024-08-28 19:55:41 [info     ] 	prior = fixed                 [Run3ModelGen.modelgen]
2024-08-28 19:55:41 [info     ] 	num_models = 1                [Run3ModelGen.modelgen]
2024-08-28 19:55:41 [info     ] 	isGMSB = False                [Run3ModelGen.modelgen]
2024-08-28 19:55:41 [info     ] 	parameters =                  [Run3ModelGen.modelgen]
2024-08-28 19:55:41 [info     ] 		tanb: [60]                   [Run3ModelGen.modelgen]
2024-08-28 19:55:41 [info     ] 		M_1: [-2000.0]               [Run3ModelGen.modelgen]
2024-08-28 19:55:41 [info     ] 		M_2: [2000]                  [Run3ModelGen.modelgen]
2024-08-28 19:55:41 [info     ] 		M_3: [4000]                  [Run3ModelGen.modelgen]
2024-08-28 19:55:41 [info     ] 		AT: [4000]                   [Run3ModelGen.modelgen]
2024-08-28 19:55:41 [info     ] 		Ab: [2000]                   [Run3ModelGen.modelgen]
2024-08-28 19:55:41 [info     ] 		Atau: [2000]                 [Run3ModelGen.modelgen]
2024-08-28 19:55:41 [info     ] 		mu: [2000]                   [Run3ModelGen.modelgen]
2024-08-28 19:55:41 [info     ] 		mA: [2000]                   [Run3ModelGen.modelgen]
2024-08-28 19:55:41 [info     ] 		meL: [2000]                  [Run3ModelGen.modelgen]
2024-08-28 19:55:41 [info     ] 		mtauL: [2000]                [Run3ModelGen.modelgen]
2024-08-28 19:55:41 [info     ] 		meR: [2000]                  [Run3ModelGen.modelgen]
2024-08-28 19:55:41 [info     ] 		mtauR: [2000]                [Run3ModelGen.modelgen]
2024-08-28 19:55:41 [info     ] 		mqL1: [4000]                 [Run3ModelGen.modelgen]
2024-08-28 19:55:41 [info     ] 		mqL3: [4000]                 [Run3ModelGen.modelgen]
2024-08-28 19:55:41 [info     ] 		muR: [4000]                  [Run3ModelGen.modelgen]
2024-08-28 19:55:41 [info     ] 		mtR: [4000]                  [Run3ModelGen.modelgen]
2024-08-28 19:55:41 [info     ] 		mdR: [4000]                  [Run3ModelGen.modelgen]
2024-08-28 19:55:41 [info     ] 		mbR: [4000]                  [Run3ModelGen.modelgen]
2024-08-28 19:55:41 [info     ] 	steps =                       [Run3ModelGen.modelgen]
2024-08-28 19:55:41 [info     ] 		{'name': 'prep_input', 'output_dir': 'input', 'prefix': 'IN'} [Run3ModelGen.modelgen]
2024-08-28 19:55:41 [info     ] 		{'name': 'SPheno', 'input_dir': 'input', 'output_dir': 'SPheno', 'log_dir': 'SPheno_log', 'prefix': 'SP'} [Run3ModelGen.modelgen]
2024-08-28 19:55:41 [info     ] 		{'name': 'softsusy', 'input_dir': 'input', 'output_dir': 'softsusy', 'prefix': 'SS'} [Run3ModelGen.modelgen]
2024-08-28 19:55:41 [info     ] 		{'name': 'micromegas', 'input_dir': 'SPheno', 'output_dir': 'micromegas', 'prefix': 'MO'} [Run3ModelGen.modelgen]
2024-08-28 19:55:41 [info     ] 		{'name': 'superiso', 'input_dir': 'SPheno', 'output_dir': 'superiso', 'prefix': 'SI'} [Run3ModelGen.modelgen]
2024-08-28 19:55:41 [info     ] 		{'name': 'gm2calc', 'input_dir': 'SPheno', 'output_dir': 'gm2calc', 'prefix': 'GM2'} [Run3ModelGen.modelgen]
2024-08-28 19:55:41 [info     ] 		{'name': 'evade', 'input_dir': 'SPheno', 'output_dir': 'evade', 'prefix': 'EV'} [Run3ModelGen.modelgen]
2024-08-28 19:55:41 [info     ] Starting Model Generation...   [Run3ModelGen.modelgen]
2024-08-28 19:55:41 [info     ] Setting up output directories for scan [Run3ModelGen.modelgen]
2024-08-28 19:55:41 [info     ] Generating Model: 0.slha (1/1) [Run3ModelGen.modelgen]
2024-08-28 19:55:41 [info     ] 	running step: prep_input      [Run3ModelGen.modelgen]
2024-08-28 19:55:41 [info     ] 	running step: SPheno          [Run3ModelGen.modelgen]
2024-08-28 19:55:41 [info     ] 	running step: softsusy        [Run3ModelGen.modelgen]
2024-08-28 19:55:41 [info     ] 	running step: micromegas      [Run3ModelGen.modelgen]
2024-08-28 19:55:42 [info     ] 	running step: superiso        [Run3ModelGen.modelgen]
2024-08-28 19:55:43 [info     ] 	running step: gm2calc         [Run3ModelGen.modelgen]
2024-08-28 19:55:43 [info     ] 	running step: evade           [Run3ModelGen.modelgen]
2024-08-28 19:55:43 [info     ] Finished generating models! Stats for successful generations (successful/attempted): [Run3ModelGen.modelgen]
2024-08-28 19:55:43 [info     ] 	prep_input: 1/1.              [Run3ModelGen.modelgen]
2024-08-28 19:55:43 [info     ] 	SPheno: 1/1.                  [Run3ModelGen.modelgen]
2024-08-28 19:55:43 [info     ] 	softsusy: 1/1.                [Run3ModelGen.modelgen]
2024-08-28 19:55:43 [info     ] 	micromegas: 1/1.              [Run3ModelGen.modelgen]
2024-08-28 19:55:43 [info     ] 	superiso: 1/1.                [Run3ModelGen.modelgen]
2024-08-28 19:55:43 [info     ] 	gm2calc: 1/1.                 [Run3ModelGen.modelgen]
2024-08-28 19:55:43 [info     ] 	evade: 0/1.                   [Run3ModelGen.modelgen]
2024-08-28 19:55:43 [info     ] Will make NTuple for: /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/scans/scan_8, with 1 models [Run3ModelGen.ntupling]
2024-08-28 19:55:43 [info     ] 	Filling values for model: 0   [Run3ModelGen.ntupling]
2024-08-28 19:55:43 [info     ] Saving NTuple in /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/scans/scan_8/ntuple.0.0.root ... [Run3ModelGen.ntupling]
Completed iteration 8
Starting iteration 9
Initial Training points:  tensor([0.8528, 0.5812, 0.1617, 0.0046], device='cuda:0') torch.Size([4])
Training points after adding:  tensor([0.0000, 0.0140, 0.4324, 0.1617, 0.1091, 0.0046, 0.2132, 0.0561, 0.3373,
        0.2593, 0.4995, 0.2723, 0.5175, 0.2392, 0.5812, 0.0210, 0.8528],
       device='cuda:0') torch.Size([17])
These training_points are used in the GP tensor([0.0000, 0.0140, 0.4324, 0.1617, 0.1091, 0.0046, 0.2132, 0.0561, 0.3373,
        0.2593, 0.4995, 0.2723, 0.5175, 0.2392, 0.5812, 0.0210, 0.8528],
       device='cuda:0')
Point: tensor([0.0000, 0.0010, 0.0020, 0.0030, 0.0040, 0.0050, 0.0060, 0.0070, 0.0080,
        0.0090, 0.0100, 0.0110, 0.0120, 0.0130, 0.0140, 0.0150, 0.0160, 0.0170,
        0.0180, 0.0190, 0.0200, 0.0210, 0.0220, 0.0230, 0.0240, 0.0250, 0.0260,
        0.0270, 0.0280, 0.0290, 0.0300, 0.0310, 0.0320, 0.0330, 0.0340, 0.0350,
        0.0360, 0.0370, 0.0380, 0.0390, 0.0400, 0.0410, 0.0420, 0.0430, 0.0440,
        0.0450, 0.0460, 0.0470, 0.0480, 0.0490, 0.0501, 0.0511, 0.0521, 0.0531,
        0.0541, 0.0551, 0.0561, 0.0571, 0.0581, 0.0591, 0.0601, 0.0611, 0.0621,
        0.0631, 0.0641, 0.0651, 0.0661, 0.0671, 0.0681, 0.0691, 0.0701, 0.0711,
        0.0721, 0.0731, 0.0741, 0.0751, 0.0761, 0.0771, 0.0781, 0.0791, 0.0801,
        0.0811, 0.0821, 0.0831, 0.0841, 0.0851, 0.0861, 0.0871, 0.0881, 0.0891,
        0.0901, 0.0911, 0.0921, 0.0931, 0.0941, 0.0951, 0.0961, 0.0971, 0.0981,
        0.0991, 0.1001, 0.1011, 0.1021, 0.1031, 0.1041, 0.1051, 0.1061, 0.1071,
        0.1081, 0.1091, 0.1101, 0.1111, 0.1121, 0.1131, 0.1141, 0.1151, 0.1161,
        0.1171, 0.1181, 0.1191, 0.1201, 0.1211, 0.1221, 0.1231, 0.1241, 0.1251,
        0.1261, 0.1271, 0.1281, 0.1291, 0.1301, 0.1311, 0.1321, 0.1331, 0.1341,
        0.1351, 0.1361, 0.1371, 0.1381, 0.1391, 0.1401, 0.1411, 0.1421, 0.1431,
        0.1441, 0.1451, 0.1461, 0.1471, 0.1481, 0.1491, 0.1502, 0.1512, 0.1522,
        0.1532, 0.1542, 0.1552, 0.1562, 0.1572, 0.1582, 0.1592, 0.1602, 0.1612,
        0.1622, 0.1632, 0.1642, 0.1652, 0.1662, 0.1672, 0.1682, 0.1692, 0.1702,
        0.1712, 0.1722, 0.1732, 0.1742, 0.1752, 0.1762, 0.1772, 0.1782, 0.1792,
        0.1802, 0.1812, 0.1822, 0.1832, 0.1842, 0.1852, 0.1862, 0.1872, 0.1882,
        0.1892, 0.1902, 0.1912, 0.1922, 0.1932, 0.1942, 0.1952, 0.1962, 0.1972,
        0.1982, 0.1992, 0.2002, 0.2012, 0.2022, 0.2032, 0.2042, 0.2052, 0.2062,
        0.2072, 0.2082, 0.2092, 0.2102, 0.2112, 0.2122, 0.2132, 0.2142, 0.2152,
        0.2162, 0.2172, 0.2182, 0.2192, 0.2202, 0.2212, 0.2222, 0.2232, 0.2242,
        0.2252, 0.2262, 0.2272, 0.2282, 0.2292, 0.2302, 0.2312, 0.2322, 0.2332,
        0.2342, 0.2352, 0.2362, 0.2372, 0.2382, 0.2392, 0.2402, 0.2412, 0.2422,
        0.2432, 0.2442, 0.2452, 0.2462, 0.2472, 0.2482, 0.2492, 0.2503, 0.2513,
        0.2523, 0.2533, 0.2543, 0.2553, 0.2563, 0.2573, 0.2583, 0.2593, 0.2603,
        0.2613, 0.2623, 0.2633, 0.2643, 0.2653, 0.2663, 0.2673, 0.2683, 0.2693,
        0.2703, 0.2713, 0.2723, 0.2733, 0.2743, 0.2753, 0.2763, 0.2773, 0.2783,
        0.2793, 0.2803, 0.2813, 0.2823, 0.2833, 0.2843, 0.2853, 0.2863, 0.2873,
        0.2883, 0.2893, 0.2903, 0.2913, 0.2923, 0.2933, 0.2943, 0.2953, 0.2963,
        0.2973, 0.2983, 0.2993, 0.3003, 0.3013, 0.3023, 0.3033, 0.3043, 0.3053,
        0.3063, 0.3073, 0.3083, 0.3093, 0.3103, 0.3113, 0.3123, 0.3133, 0.3143,
        0.3153, 0.3163, 0.3173, 0.3183, 0.3193, 0.3203, 0.3213, 0.3223, 0.3233,
        0.3243, 0.3253, 0.3263, 0.3273, 0.3283, 0.3293, 0.3303, 0.3313, 0.3323,
        0.3333, 0.3343, 0.3353, 0.3363, 0.3373, 0.3383, 0.3393, 0.3403, 0.3413,
        0.3423, 0.3433, 0.3443, 0.3453, 0.3463, 0.3473, 0.3483, 0.3493, 0.3504,
        0.3514, 0.3524, 0.3534, 0.3544, 0.3554, 0.3564, 0.3574, 0.3584, 0.3594,
        0.3604, 0.3614, 0.3624, 0.3634, 0.3644, 0.3654, 0.3664, 0.3674, 0.3684,
        0.3694, 0.3704, 0.3714, 0.3724, 0.3734, 0.3744, 0.3754, 0.3764, 0.3774,
        0.3784, 0.3794, 0.3804, 0.3814, 0.3824, 0.3834, 0.3844, 0.3854, 0.3864,
        0.3874, 0.3884, 0.3894, 0.3904, 0.3914, 0.3924, 0.3934, 0.3944, 0.3954,
        0.3964, 0.3974, 0.3984, 0.3994, 0.4004, 0.4014, 0.4024, 0.4034, 0.4044,
        0.4054, 0.4064, 0.4074, 0.4084, 0.4094, 0.4104, 0.4114, 0.4124, 0.4134,
        0.4144, 0.4154, 0.4164, 0.4174, 0.4184, 0.4194, 0.4204, 0.4214, 0.4224,
        0.4234, 0.4244, 0.4254, 0.4264, 0.4274, 0.4284, 0.4294, 0.4304, 0.4314,
        0.4324, 0.4334, 0.4344, 0.4354, 0.4364, 0.4374, 0.4384, 0.4394, 0.4404,
        0.4414, 0.4424, 0.4434, 0.4444, 0.4454, 0.4464, 0.4474, 0.4484, 0.4494,
        0.4505, 0.4515, 0.4525, 0.4535, 0.4545, 0.4555, 0.4565, 0.4575, 0.4585,
        0.4595, 0.4605, 0.4615, 0.4625, 0.4635, 0.4645, 0.4655, 0.4665, 0.4675,
        0.4685, 0.4695, 0.4705, 0.4715, 0.4725, 0.4735, 0.4745, 0.4755, 0.4765,
        0.4775, 0.4785, 0.4795, 0.4805, 0.4815, 0.4825, 0.4835, 0.4845, 0.4855,
        0.4865, 0.4875, 0.4885, 0.4895, 0.4905, 0.4915, 0.4925, 0.4935, 0.4945,
        0.4955, 0.4965, 0.4975, 0.4985, 0.4995, 0.5005, 0.5015, 0.5025, 0.5035,
        0.5045, 0.5055, 0.5065, 0.5075, 0.5085, 0.5095, 0.5105, 0.5115, 0.5125,
        0.5135, 0.5145, 0.5155, 0.5165, 0.5175, 0.5185, 0.5195, 0.5205, 0.5215,
        0.5225, 0.5235, 0.5245, 0.5255, 0.5265, 0.5275, 0.5285, 0.5295, 0.5305,
        0.5315, 0.5325, 0.5335, 0.5345, 0.5355, 0.5365, 0.5375, 0.5385, 0.5395,
        0.5405, 0.5415, 0.5425, 0.5435, 0.5445, 0.5455, 0.5465, 0.5475, 0.5485,
        0.5495, 0.5506, 0.5516, 0.5526, 0.5536, 0.5546, 0.5556, 0.5566, 0.5576,
        0.5586, 0.5596, 0.5606, 0.5616, 0.5626, 0.5636, 0.5646, 0.5656, 0.5666,
        0.5676, 0.5686, 0.5696, 0.5706, 0.5716, 0.5726, 0.5736, 0.5746, 0.5756,
        0.5766, 0.5776, 0.5786, 0.5796, 0.5806, 0.5816, 0.5826, 0.5836, 0.5846,
        0.5856, 0.5866, 0.5876, 0.5886, 0.5896, 0.5906, 0.5916, 0.5926, 0.5936,
        0.5946, 0.5956, 0.5966, 0.5976, 0.5986, 0.5996, 0.6006, 0.6016, 0.6026,
        0.6036, 0.6046, 0.6056, 0.6066, 0.6076, 0.6086, 0.6096, 0.6106, 0.6116,
        0.6126, 0.6136, 0.6146, 0.6156, 0.6166, 0.6176, 0.6186, 0.6196, 0.6206,
        0.6216, 0.6226, 0.6236, 0.6246, 0.6256, 0.6266, 0.6276, 0.6286, 0.6296,
        0.6306, 0.6316, 0.6326, 0.6336, 0.6346, 0.6356, 0.6366, 0.6376, 0.6386,
        0.6396, 0.6406, 0.6416, 0.6426, 0.6436, 0.6446, 0.6456, 0.6466, 0.6476,
        0.6486, 0.6496, 0.6507, 0.6517, 0.6527, 0.6537, 0.6547, 0.6557, 0.6567,
        0.6577, 0.6587, 0.6597, 0.6607, 0.6617, 0.6627, 0.6637, 0.6647, 0.6657,
        0.6667, 0.6677, 0.6687, 0.6697, 0.6707, 0.6717, 0.6727, 0.6737, 0.6747,
        0.6757, 0.6767, 0.6777, 0.6787, 0.6797, 0.6807, 0.6817, 0.6827, 0.6837,
        0.6847, 0.6857, 0.6867, 0.6877, 0.6887, 0.6897, 0.6907, 0.6917, 0.6927,
        0.6937, 0.6947, 0.6957, 0.6967, 0.6977, 0.6987, 0.6997, 0.7007, 0.7017,
        0.7027, 0.7037, 0.7047, 0.7057, 0.7067, 0.7077, 0.7087, 0.7097, 0.7107,
        0.7117, 0.7127, 0.7137, 0.7147, 0.7157, 0.7167, 0.7177, 0.7187, 0.7197,
        0.7207, 0.7217, 0.7227, 0.7237, 0.7247, 0.7257, 0.7267, 0.7277, 0.7287,
        0.7297, 0.7307, 0.7317, 0.7327, 0.7337, 0.7347, 0.7357, 0.7367, 0.7377,
        0.7387, 0.7397, 0.7407, 0.7417, 0.7427, 0.7437, 0.7447, 0.7457, 0.7467,
        0.7477, 0.7487, 0.7497, 0.7508, 0.7518, 0.7528, 0.7538, 0.7548, 0.7558,
        0.7568, 0.7578, 0.7588, 0.7598, 0.7608, 0.7618, 0.7628, 0.7638, 0.7648,
        0.7658, 0.7668, 0.7678, 0.7688, 0.7698, 0.7708, 0.7718, 0.7728, 0.7738,
        0.7748, 0.7758, 0.7768, 0.7778, 0.7788, 0.7798, 0.7808, 0.7818, 0.7828,
        0.7838, 0.7848, 0.7858, 0.7868, 0.7878, 0.7888, 0.7898, 0.7908, 0.7918,
        0.7928, 0.7938, 0.7948, 0.7958, 0.7968, 0.7978, 0.7988, 0.7998, 0.8008,
        0.8018, 0.8028, 0.8038, 0.8048, 0.8058, 0.8068, 0.8078, 0.8088, 0.8098,
        0.8108, 0.8118, 0.8128, 0.8138, 0.8148, 0.8158, 0.8168, 0.8178, 0.8188,
        0.8198, 0.8208, 0.8218, 0.8228, 0.8238, 0.8248, 0.8258, 0.8268, 0.8278,
        0.8288, 0.8298, 0.8308, 0.8318, 0.8328, 0.8338, 0.8348, 0.8358, 0.8368,
        0.8378, 0.8388, 0.8398, 0.8408, 0.8418, 0.8428, 0.8438, 0.8448, 0.8458,
        0.8468, 0.8478, 0.8488, 0.8498, 0.8509, 0.8519, 0.8529, 0.8539, 0.8549,
        0.8559, 0.8569, 0.8579, 0.8589, 0.8599, 0.8609, 0.8619, 0.8629, 0.8639,
        0.8649, 0.8659, 0.8669, 0.8679, 0.8689, 0.8699, 0.8709, 0.8719, 0.8729,
        0.8739, 0.8749, 0.8759, 0.8769, 0.8779, 0.8789, 0.8799, 0.8809, 0.8819,
        0.8829, 0.8839, 0.8849, 0.8859, 0.8869, 0.8879, 0.8889, 0.8899, 0.8909,
        0.8919, 0.8929, 0.8939, 0.8949, 0.8959, 0.8969, 0.8979, 0.8989, 0.8999,
        0.9009, 0.9019, 0.9029, 0.9039, 0.9049, 0.9059, 0.9069, 0.9079, 0.9089,
        0.9099, 0.9109, 0.9119, 0.9129, 0.9139, 0.9149, 0.9159, 0.9169, 0.9179,
        0.9189, 0.9199, 0.9209, 0.9219, 0.9229, 0.9239, 0.9249, 0.9259, 0.9269,
        0.9279, 0.9289, 0.9299, 0.9309, 0.9319, 0.9329, 0.9339, 0.9349, 0.9359,
        0.9369, 0.9379, 0.9389, 0.9399, 0.9409, 0.9419, 0.9429, 0.9439, 0.9449,
        0.9459, 0.9469, 0.9479, 0.9489, 0.9499, 0.9510, 0.9520, 0.9530, 0.9540,
        0.9550, 0.9560, 0.9570, 0.9580, 0.9590, 0.9600, 0.9610, 0.9620, 0.9630,
        0.9640, 0.9650, 0.9660, 0.9670, 0.9680, 0.9690, 0.9700, 0.9710, 0.9720,
        0.9730, 0.9740, 0.9750, 0.9760, 0.9770, 0.9780, 0.9790, 0.9800, 0.9810,
        0.9820, 0.9830, 0.9840, 0.9850, 0.9860, 0.9870, 0.9880, 0.9890, 0.9900,
        0.9910, 0.9920, 0.9930, 0.9940, 0.9950, 0.9960, 0.9970, 0.9980, 0.9990,
        1.0000], device='cuda:0') - Variance: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0') 
Smoothed_batch_entropy:  tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan], device='cuda:0')
Selected points (indices): {0}
Selected new x values (normalized): tensor([0.], device='cuda:0')
Corresponding new x values (unnormalized): tensor([-2000.], device='cuda:0')
Plot saved to /raven/u/dvoss/al_pmssmwithgp/model/plots/Iter9/gp_plot.png
YAML file has been created: /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/data/new_config.yaml
Note: detected 144 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
Note: NumExpr detected 144 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
NumExpr defaulting to 8 threads.

  ___              ____ __  __          _       _   ___            
 | _ \ _  _  _ _  |__ /|  \/  | ___  __| | ___ | | / __| ___  _ _  
 |   /| || || ' \  |_ \| |\/| |/ _ \/ _` |/ -_)| || (_ |/ -_)| ' \ 
 |_|_\ \_,_||_||_||___/|_|  |_|\___/\__,_|\___||_| \___|\___||_||_|
                                                                   

2024-08-28 19:56:27 [info     ] Initialised ModelGenerator with: [Run3ModelGen.modelgen]
2024-08-28 19:56:27 [info     ] 	config_file = /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/data/new_config.yaml [Run3ModelGen.modelgen]
2024-08-28 19:56:27 [info     ] 	scan_dir = /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/scans/scan_9 [Run3ModelGen.modelgen]
2024-08-28 19:56:27 [info     ] 	rawfilen = /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/data/raw.slha [Run3ModelGen.modelgen]
2024-08-28 19:56:27 [info     ] 	seed = 123                    [Run3ModelGen.modelgen]
2024-08-28 19:56:27 [info     ] 	points = {}                   [Run3ModelGen.modelgen]
2024-08-28 19:56:27 [info     ] 	prior = fixed                 [Run3ModelGen.modelgen]
2024-08-28 19:56:27 [info     ] 	num_models = 1                [Run3ModelGen.modelgen]
2024-08-28 19:56:27 [info     ] 	isGMSB = False                [Run3ModelGen.modelgen]
2024-08-28 19:56:27 [info     ] 	parameters =                  [Run3ModelGen.modelgen]
2024-08-28 19:56:27 [info     ] 		tanb: [60]                   [Run3ModelGen.modelgen]
2024-08-28 19:56:27 [info     ] 		M_1: [-2000.0]               [Run3ModelGen.modelgen]
2024-08-28 19:56:27 [info     ] 		M_2: [2000]                  [Run3ModelGen.modelgen]
2024-08-28 19:56:27 [info     ] 		M_3: [4000]                  [Run3ModelGen.modelgen]
2024-08-28 19:56:27 [info     ] 		AT: [4000]                   [Run3ModelGen.modelgen]
2024-08-28 19:56:27 [info     ] 		Ab: [2000]                   [Run3ModelGen.modelgen]
2024-08-28 19:56:27 [info     ] 		Atau: [2000]                 [Run3ModelGen.modelgen]
2024-08-28 19:56:27 [info     ] 		mu: [2000]                   [Run3ModelGen.modelgen]
2024-08-28 19:56:27 [info     ] 		mA: [2000]                   [Run3ModelGen.modelgen]
2024-08-28 19:56:27 [info     ] 		meL: [2000]                  [Run3ModelGen.modelgen]
2024-08-28 19:56:27 [info     ] 		mtauL: [2000]                [Run3ModelGen.modelgen]
2024-08-28 19:56:27 [info     ] 		meR: [2000]                  [Run3ModelGen.modelgen]
2024-08-28 19:56:27 [info     ] 		mtauR: [2000]                [Run3ModelGen.modelgen]
2024-08-28 19:56:27 [info     ] 		mqL1: [4000]                 [Run3ModelGen.modelgen]
2024-08-28 19:56:27 [info     ] 		mqL3: [4000]                 [Run3ModelGen.modelgen]
2024-08-28 19:56:27 [info     ] 		muR: [4000]                  [Run3ModelGen.modelgen]
2024-08-28 19:56:27 [info     ] 		mtR: [4000]                  [Run3ModelGen.modelgen]
2024-08-28 19:56:27 [info     ] 		mdR: [4000]                  [Run3ModelGen.modelgen]
2024-08-28 19:56:27 [info     ] 		mbR: [4000]                  [Run3ModelGen.modelgen]
2024-08-28 19:56:27 [info     ] 	steps =                       [Run3ModelGen.modelgen]
2024-08-28 19:56:27 [info     ] 		{'name': 'prep_input', 'output_dir': 'input', 'prefix': 'IN'} [Run3ModelGen.modelgen]
2024-08-28 19:56:27 [info     ] 		{'name': 'SPheno', 'input_dir': 'input', 'output_dir': 'SPheno', 'log_dir': 'SPheno_log', 'prefix': 'SP'} [Run3ModelGen.modelgen]
2024-08-28 19:56:27 [info     ] 		{'name': 'softsusy', 'input_dir': 'input', 'output_dir': 'softsusy', 'prefix': 'SS'} [Run3ModelGen.modelgen]
2024-08-28 19:56:27 [info     ] 		{'name': 'micromegas', 'input_dir': 'SPheno', 'output_dir': 'micromegas', 'prefix': 'MO'} [Run3ModelGen.modelgen]
2024-08-28 19:56:27 [info     ] 		{'name': 'superiso', 'input_dir': 'SPheno', 'output_dir': 'superiso', 'prefix': 'SI'} [Run3ModelGen.modelgen]
2024-08-28 19:56:27 [info     ] 		{'name': 'gm2calc', 'input_dir': 'SPheno', 'output_dir': 'gm2calc', 'prefix': 'GM2'} [Run3ModelGen.modelgen]
2024-08-28 19:56:27 [info     ] 		{'name': 'evade', 'input_dir': 'SPheno', 'output_dir': 'evade', 'prefix': 'EV'} [Run3ModelGen.modelgen]
2024-08-28 19:56:27 [info     ] Starting Model Generation...   [Run3ModelGen.modelgen]
2024-08-28 19:56:27 [info     ] Setting up output directories for scan [Run3ModelGen.modelgen]
2024-08-28 19:56:27 [info     ] Generating Model: 0.slha (1/1) [Run3ModelGen.modelgen]
2024-08-28 19:56:27 [info     ] 	running step: prep_input      [Run3ModelGen.modelgen]
2024-08-28 19:56:27 [info     ] 	running step: SPheno          [Run3ModelGen.modelgen]
2024-08-28 19:56:27 [info     ] 	running step: softsusy        [Run3ModelGen.modelgen]
2024-08-28 19:56:27 [info     ] 	running step: micromegas      [Run3ModelGen.modelgen]
2024-08-28 19:56:29 [info     ] 	running step: superiso        [Run3ModelGen.modelgen]
2024-08-28 19:56:29 [info     ] 	running step: gm2calc         [Run3ModelGen.modelgen]
2024-08-28 19:56:29 [info     ] 	running step: evade           [Run3ModelGen.modelgen]
2024-08-28 19:56:29 [info     ] Finished generating models! Stats for successful generations (successful/attempted): [Run3ModelGen.modelgen]
2024-08-28 19:56:29 [info     ] 	prep_input: 1/1.              [Run3ModelGen.modelgen]
2024-08-28 19:56:29 [info     ] 	SPheno: 1/1.                  [Run3ModelGen.modelgen]
2024-08-28 19:56:29 [info     ] 	softsusy: 1/1.                [Run3ModelGen.modelgen]
2024-08-28 19:56:29 [info     ] 	micromegas: 1/1.              [Run3ModelGen.modelgen]
2024-08-28 19:56:29 [info     ] 	superiso: 1/1.                [Run3ModelGen.modelgen]
2024-08-28 19:56:29 [info     ] 	gm2calc: 1/1.                 [Run3ModelGen.modelgen]
2024-08-28 19:56:29 [info     ] 	evade: 0/1.                   [Run3ModelGen.modelgen]
2024-08-28 19:56:29 [info     ] Will make NTuple for: /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/scans/scan_9, with 1 models [Run3ModelGen.ntupling]
2024-08-28 19:56:29 [info     ] 	Filling values for model: 0   [Run3ModelGen.ntupling]
2024-08-28 19:56:29 [info     ] Saving NTuple in /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/scans/scan_9/ntuple.0.0.root ... [Run3ModelGen.ntupling]
Completed iteration 9
Starting iteration 10
Initial Training points:  tensor([0.8528, 0.5812, 0.1617, 0.0046], device='cuda:0') torch.Size([4])
Training points after adding:  tensor([0.0000, 0.0140, 0.4324, 0.1617, 0.1091, 0.0046, 0.2132, 0.0561, 0.3373,
        0.2593, 0.4995, 0.2723, 0.5175, 0.2392, 0.5812, 0.0210, 0.8528],
       device='cuda:0') torch.Size([17])
These training_points are used in the GP tensor([0.0000, 0.0140, 0.4324, 0.1617, 0.1091, 0.0046, 0.2132, 0.0561, 0.3373,
        0.2593, 0.4995, 0.2723, 0.5175, 0.2392, 0.5812, 0.0210, 0.8528],
       device='cuda:0')
Point: tensor([0.0000, 0.0010, 0.0020, 0.0030, 0.0040, 0.0050, 0.0060, 0.0070, 0.0080,
        0.0090, 0.0100, 0.0110, 0.0120, 0.0130, 0.0140, 0.0150, 0.0160, 0.0170,
        0.0180, 0.0190, 0.0200, 0.0210, 0.0220, 0.0230, 0.0240, 0.0250, 0.0260,
        0.0270, 0.0280, 0.0290, 0.0300, 0.0310, 0.0320, 0.0330, 0.0340, 0.0350,
        0.0360, 0.0370, 0.0380, 0.0390, 0.0400, 0.0410, 0.0420, 0.0430, 0.0440,
        0.0450, 0.0460, 0.0470, 0.0480, 0.0490, 0.0501, 0.0511, 0.0521, 0.0531,
        0.0541, 0.0551, 0.0561, 0.0571, 0.0581, 0.0591, 0.0601, 0.0611, 0.0621,
        0.0631, 0.0641, 0.0651, 0.0661, 0.0671, 0.0681, 0.0691, 0.0701, 0.0711,
        0.0721, 0.0731, 0.0741, 0.0751, 0.0761, 0.0771, 0.0781, 0.0791, 0.0801,
        0.0811, 0.0821, 0.0831, 0.0841, 0.0851, 0.0861, 0.0871, 0.0881, 0.0891,
        0.0901, 0.0911, 0.0921, 0.0931, 0.0941, 0.0951, 0.0961, 0.0971, 0.0981,
        0.0991, 0.1001, 0.1011, 0.1021, 0.1031, 0.1041, 0.1051, 0.1061, 0.1071,
        0.1081, 0.1091, 0.1101, 0.1111, 0.1121, 0.1131, 0.1141, 0.1151, 0.1161,
        0.1171, 0.1181, 0.1191, 0.1201, 0.1211, 0.1221, 0.1231, 0.1241, 0.1251,
        0.1261, 0.1271, 0.1281, 0.1291, 0.1301, 0.1311, 0.1321, 0.1331, 0.1341,
        0.1351, 0.1361, 0.1371, 0.1381, 0.1391, 0.1401, 0.1411, 0.1421, 0.1431,
        0.1441, 0.1451, 0.1461, 0.1471, 0.1481, 0.1491, 0.1502, 0.1512, 0.1522,
        0.1532, 0.1542, 0.1552, 0.1562, 0.1572, 0.1582, 0.1592, 0.1602, 0.1612,
        0.1622, 0.1632, 0.1642, 0.1652, 0.1662, 0.1672, 0.1682, 0.1692, 0.1702,
        0.1712, 0.1722, 0.1732, 0.1742, 0.1752, 0.1762, 0.1772, 0.1782, 0.1792,
        0.1802, 0.1812, 0.1822, 0.1832, 0.1842, 0.1852, 0.1862, 0.1872, 0.1882,
        0.1892, 0.1902, 0.1912, 0.1922, 0.1932, 0.1942, 0.1952, 0.1962, 0.1972,
        0.1982, 0.1992, 0.2002, 0.2012, 0.2022, 0.2032, 0.2042, 0.2052, 0.2062,
        0.2072, 0.2082, 0.2092, 0.2102, 0.2112, 0.2122, 0.2132, 0.2142, 0.2152,
        0.2162, 0.2172, 0.2182, 0.2192, 0.2202, 0.2212, 0.2222, 0.2232, 0.2242,
        0.2252, 0.2262, 0.2272, 0.2282, 0.2292, 0.2302, 0.2312, 0.2322, 0.2332,
        0.2342, 0.2352, 0.2362, 0.2372, 0.2382, 0.2392, 0.2402, 0.2412, 0.2422,
        0.2432, 0.2442, 0.2452, 0.2462, 0.2472, 0.2482, 0.2492, 0.2503, 0.2513,
        0.2523, 0.2533, 0.2543, 0.2553, 0.2563, 0.2573, 0.2583, 0.2593, 0.2603,
        0.2613, 0.2623, 0.2633, 0.2643, 0.2653, 0.2663, 0.2673, 0.2683, 0.2693,
        0.2703, 0.2713, 0.2723, 0.2733, 0.2743, 0.2753, 0.2763, 0.2773, 0.2783,
        0.2793, 0.2803, 0.2813, 0.2823, 0.2833, 0.2843, 0.2853, 0.2863, 0.2873,
        0.2883, 0.2893, 0.2903, 0.2913, 0.2923, 0.2933, 0.2943, 0.2953, 0.2963,
        0.2973, 0.2983, 0.2993, 0.3003, 0.3013, 0.3023, 0.3033, 0.3043, 0.3053,
        0.3063, 0.3073, 0.3083, 0.3093, 0.3103, 0.3113, 0.3123, 0.3133, 0.3143,
        0.3153, 0.3163, 0.3173, 0.3183, 0.3193, 0.3203, 0.3213, 0.3223, 0.3233,
        0.3243, 0.3253, 0.3263, 0.3273, 0.3283, 0.3293, 0.3303, 0.3313, 0.3323,
        0.3333, 0.3343, 0.3353, 0.3363, 0.3373, 0.3383, 0.3393, 0.3403, 0.3413,
        0.3423, 0.3433, 0.3443, 0.3453, 0.3463, 0.3473, 0.3483, 0.3493, 0.3504,
        0.3514, 0.3524, 0.3534, 0.3544, 0.3554, 0.3564, 0.3574, 0.3584, 0.3594,
        0.3604, 0.3614, 0.3624, 0.3634, 0.3644, 0.3654, 0.3664, 0.3674, 0.3684,
        0.3694, 0.3704, 0.3714, 0.3724, 0.3734, 0.3744, 0.3754, 0.3764, 0.3774,
        0.3784, 0.3794, 0.3804, 0.3814, 0.3824, 0.3834, 0.3844, 0.3854, 0.3864,
        0.3874, 0.3884, 0.3894, 0.3904, 0.3914, 0.3924, 0.3934, 0.3944, 0.3954,
        0.3964, 0.3974, 0.3984, 0.3994, 0.4004, 0.4014, 0.4024, 0.4034, 0.4044,
        0.4054, 0.4064, 0.4074, 0.4084, 0.4094, 0.4104, 0.4114, 0.4124, 0.4134,
        0.4144, 0.4154, 0.4164, 0.4174, 0.4184, 0.4194, 0.4204, 0.4214, 0.4224,
        0.4234, 0.4244, 0.4254, 0.4264, 0.4274, 0.4284, 0.4294, 0.4304, 0.4314,
        0.4324, 0.4334, 0.4344, 0.4354, 0.4364, 0.4374, 0.4384, 0.4394, 0.4404,
        0.4414, 0.4424, 0.4434, 0.4444, 0.4454, 0.4464, 0.4474, 0.4484, 0.4494,
        0.4505, 0.4515, 0.4525, 0.4535, 0.4545, 0.4555, 0.4565, 0.4575, 0.4585,
        0.4595, 0.4605, 0.4615, 0.4625, 0.4635, 0.4645, 0.4655, 0.4665, 0.4675,
        0.4685, 0.4695, 0.4705, 0.4715, 0.4725, 0.4735, 0.4745, 0.4755, 0.4765,
        0.4775, 0.4785, 0.4795, 0.4805, 0.4815, 0.4825, 0.4835, 0.4845, 0.4855,
        0.4865, 0.4875, 0.4885, 0.4895, 0.4905, 0.4915, 0.4925, 0.4935, 0.4945,
        0.4955, 0.4965, 0.4975, 0.4985, 0.4995, 0.5005, 0.5015, 0.5025, 0.5035,
        0.5045, 0.5055, 0.5065, 0.5075, 0.5085, 0.5095, 0.5105, 0.5115, 0.5125,
        0.5135, 0.5145, 0.5155, 0.5165, 0.5175, 0.5185, 0.5195, 0.5205, 0.5215,
        0.5225, 0.5235, 0.5245, 0.5255, 0.5265, 0.5275, 0.5285, 0.5295, 0.5305,
        0.5315, 0.5325, 0.5335, 0.5345, 0.5355, 0.5365, 0.5375, 0.5385, 0.5395,
        0.5405, 0.5415, 0.5425, 0.5435, 0.5445, 0.5455, 0.5465, 0.5475, 0.5485,
        0.5495, 0.5506, 0.5516, 0.5526, 0.5536, 0.5546, 0.5556, 0.5566, 0.5576,
        0.5586, 0.5596, 0.5606, 0.5616, 0.5626, 0.5636, 0.5646, 0.5656, 0.5666,
        0.5676, 0.5686, 0.5696, 0.5706, 0.5716, 0.5726, 0.5736, 0.5746, 0.5756,
        0.5766, 0.5776, 0.5786, 0.5796, 0.5806, 0.5816, 0.5826, 0.5836, 0.5846,
        0.5856, 0.5866, 0.5876, 0.5886, 0.5896, 0.5906, 0.5916, 0.5926, 0.5936,
        0.5946, 0.5956, 0.5966, 0.5976, 0.5986, 0.5996, 0.6006, 0.6016, 0.6026,
        0.6036, 0.6046, 0.6056, 0.6066, 0.6076, 0.6086, 0.6096, 0.6106, 0.6116,
        0.6126, 0.6136, 0.6146, 0.6156, 0.6166, 0.6176, 0.6186, 0.6196, 0.6206,
        0.6216, 0.6226, 0.6236, 0.6246, 0.6256, 0.6266, 0.6276, 0.6286, 0.6296,
        0.6306, 0.6316, 0.6326, 0.6336, 0.6346, 0.6356, 0.6366, 0.6376, 0.6386,
        0.6396, 0.6406, 0.6416, 0.6426, 0.6436, 0.6446, 0.6456, 0.6466, 0.6476,
        0.6486, 0.6496, 0.6507, 0.6517, 0.6527, 0.6537, 0.6547, 0.6557, 0.6567,
        0.6577, 0.6587, 0.6597, 0.6607, 0.6617, 0.6627, 0.6637, 0.6647, 0.6657,
        0.6667, 0.6677, 0.6687, 0.6697, 0.6707, 0.6717, 0.6727, 0.6737, 0.6747,
        0.6757, 0.6767, 0.6777, 0.6787, 0.6797, 0.6807, 0.6817, 0.6827, 0.6837,
        0.6847, 0.6857, 0.6867, 0.6877, 0.6887, 0.6897, 0.6907, 0.6917, 0.6927,
        0.6937, 0.6947, 0.6957, 0.6967, 0.6977, 0.6987, 0.6997, 0.7007, 0.7017,
        0.7027, 0.7037, 0.7047, 0.7057, 0.7067, 0.7077, 0.7087, 0.7097, 0.7107,
        0.7117, 0.7127, 0.7137, 0.7147, 0.7157, 0.7167, 0.7177, 0.7187, 0.7197,
        0.7207, 0.7217, 0.7227, 0.7237, 0.7247, 0.7257, 0.7267, 0.7277, 0.7287,
        0.7297, 0.7307, 0.7317, 0.7327, 0.7337, 0.7347, 0.7357, 0.7367, 0.7377,
        0.7387, 0.7397, 0.7407, 0.7417, 0.7427, 0.7437, 0.7447, 0.7457, 0.7467,
        0.7477, 0.7487, 0.7497, 0.7508, 0.7518, 0.7528, 0.7538, 0.7548, 0.7558,
        0.7568, 0.7578, 0.7588, 0.7598, 0.7608, 0.7618, 0.7628, 0.7638, 0.7648,
        0.7658, 0.7668, 0.7678, 0.7688, 0.7698, 0.7708, 0.7718, 0.7728, 0.7738,
        0.7748, 0.7758, 0.7768, 0.7778, 0.7788, 0.7798, 0.7808, 0.7818, 0.7828,
        0.7838, 0.7848, 0.7858, 0.7868, 0.7878, 0.7888, 0.7898, 0.7908, 0.7918,
        0.7928, 0.7938, 0.7948, 0.7958, 0.7968, 0.7978, 0.7988, 0.7998, 0.8008,
        0.8018, 0.8028, 0.8038, 0.8048, 0.8058, 0.8068, 0.8078, 0.8088, 0.8098,
        0.8108, 0.8118, 0.8128, 0.8138, 0.8148, 0.8158, 0.8168, 0.8178, 0.8188,
        0.8198, 0.8208, 0.8218, 0.8228, 0.8238, 0.8248, 0.8258, 0.8268, 0.8278,
        0.8288, 0.8298, 0.8308, 0.8318, 0.8328, 0.8338, 0.8348, 0.8358, 0.8368,
        0.8378, 0.8388, 0.8398, 0.8408, 0.8418, 0.8428, 0.8438, 0.8448, 0.8458,
        0.8468, 0.8478, 0.8488, 0.8498, 0.8509, 0.8519, 0.8529, 0.8539, 0.8549,
        0.8559, 0.8569, 0.8579, 0.8589, 0.8599, 0.8609, 0.8619, 0.8629, 0.8639,
        0.8649, 0.8659, 0.8669, 0.8679, 0.8689, 0.8699, 0.8709, 0.8719, 0.8729,
        0.8739, 0.8749, 0.8759, 0.8769, 0.8779, 0.8789, 0.8799, 0.8809, 0.8819,
        0.8829, 0.8839, 0.8849, 0.8859, 0.8869, 0.8879, 0.8889, 0.8899, 0.8909,
        0.8919, 0.8929, 0.8939, 0.8949, 0.8959, 0.8969, 0.8979, 0.8989, 0.8999,
        0.9009, 0.9019, 0.9029, 0.9039, 0.9049, 0.9059, 0.9069, 0.9079, 0.9089,
        0.9099, 0.9109, 0.9119, 0.9129, 0.9139, 0.9149, 0.9159, 0.9169, 0.9179,
        0.9189, 0.9199, 0.9209, 0.9219, 0.9229, 0.9239, 0.9249, 0.9259, 0.9269,
        0.9279, 0.9289, 0.9299, 0.9309, 0.9319, 0.9329, 0.9339, 0.9349, 0.9359,
        0.9369, 0.9379, 0.9389, 0.9399, 0.9409, 0.9419, 0.9429, 0.9439, 0.9449,
        0.9459, 0.9469, 0.9479, 0.9489, 0.9499, 0.9510, 0.9520, 0.9530, 0.9540,
        0.9550, 0.9560, 0.9570, 0.9580, 0.9590, 0.9600, 0.9610, 0.9620, 0.9630,
        0.9640, 0.9650, 0.9660, 0.9670, 0.9680, 0.9690, 0.9700, 0.9710, 0.9720,
        0.9730, 0.9740, 0.9750, 0.9760, 0.9770, 0.9780, 0.9790, 0.9800, 0.9810,
        0.9820, 0.9830, 0.9840, 0.9850, 0.9860, 0.9870, 0.9880, 0.9890, 0.9900,
        0.9910, 0.9920, 0.9930, 0.9940, 0.9950, 0.9960, 0.9970, 0.9980, 0.9990,
        1.0000], device='cuda:0') - Variance: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0') 
Smoothed_batch_entropy:  tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan], device='cuda:0')
Selected points (indices): {0}
Selected new x values (normalized): tensor([0.], device='cuda:0')
Corresponding new x values (unnormalized): tensor([-2000.], device='cuda:0')
Plot saved to /raven/u/dvoss/al_pmssmwithgp/model/plots/Iter10/gp_plot.png
YAML file has been created: /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/data/new_config.yaml
Note: detected 144 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
Note: NumExpr detected 144 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
NumExpr defaulting to 8 threads.

  ___              ____ __  __          _       _   ___            
 | _ \ _  _  _ _  |__ /|  \/  | ___  __| | ___ | | / __| ___  _ _  
 |   /| || || ' \  |_ \| |\/| |/ _ \/ _` |/ -_)| || (_ |/ -_)| ' \ 
 |_|_\ \_,_||_||_||___/|_|  |_|\___/\__,_|\___||_| \___|\___||_||_|
                                                                   

2024-08-28 19:57:14 [info     ] Initialised ModelGenerator with: [Run3ModelGen.modelgen]
2024-08-28 19:57:14 [info     ] 	config_file = /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/data/new_config.yaml [Run3ModelGen.modelgen]
2024-08-28 19:57:14 [info     ] 	scan_dir = /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/scans/scan_10 [Run3ModelGen.modelgen]
2024-08-28 19:57:14 [info     ] 	rawfilen = /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/data/raw.slha [Run3ModelGen.modelgen]
2024-08-28 19:57:14 [info     ] 	seed = 123                    [Run3ModelGen.modelgen]
2024-08-28 19:57:14 [info     ] 	points = {}                   [Run3ModelGen.modelgen]
2024-08-28 19:57:14 [info     ] 	prior = fixed                 [Run3ModelGen.modelgen]
2024-08-28 19:57:14 [info     ] 	num_models = 1                [Run3ModelGen.modelgen]
2024-08-28 19:57:14 [info     ] 	isGMSB = False                [Run3ModelGen.modelgen]
2024-08-28 19:57:14 [info     ] 	parameters =                  [Run3ModelGen.modelgen]
2024-08-28 19:57:14 [info     ] 		tanb: [60]                   [Run3ModelGen.modelgen]
2024-08-28 19:57:14 [info     ] 		M_1: [-2000.0]               [Run3ModelGen.modelgen]
2024-08-28 19:57:14 [info     ] 		M_2: [2000]                  [Run3ModelGen.modelgen]
2024-08-28 19:57:14 [info     ] 		M_3: [4000]                  [Run3ModelGen.modelgen]
2024-08-28 19:57:14 [info     ] 		AT: [4000]                   [Run3ModelGen.modelgen]
2024-08-28 19:57:14 [info     ] 		Ab: [2000]                   [Run3ModelGen.modelgen]
2024-08-28 19:57:14 [info     ] 		Atau: [2000]                 [Run3ModelGen.modelgen]
2024-08-28 19:57:14 [info     ] 		mu: [2000]                   [Run3ModelGen.modelgen]
2024-08-28 19:57:14 [info     ] 		mA: [2000]                   [Run3ModelGen.modelgen]
2024-08-28 19:57:14 [info     ] 		meL: [2000]                  [Run3ModelGen.modelgen]
2024-08-28 19:57:14 [info     ] 		mtauL: [2000]                [Run3ModelGen.modelgen]
2024-08-28 19:57:14 [info     ] 		meR: [2000]                  [Run3ModelGen.modelgen]
2024-08-28 19:57:14 [info     ] 		mtauR: [2000]                [Run3ModelGen.modelgen]
2024-08-28 19:57:14 [info     ] 		mqL1: [4000]                 [Run3ModelGen.modelgen]
2024-08-28 19:57:14 [info     ] 		mqL3: [4000]                 [Run3ModelGen.modelgen]
2024-08-28 19:57:14 [info     ] 		muR: [4000]                  [Run3ModelGen.modelgen]
2024-08-28 19:57:14 [info     ] 		mtR: [4000]                  [Run3ModelGen.modelgen]
2024-08-28 19:57:14 [info     ] 		mdR: [4000]                  [Run3ModelGen.modelgen]
2024-08-28 19:57:14 [info     ] 		mbR: [4000]                  [Run3ModelGen.modelgen]
2024-08-28 19:57:14 [info     ] 	steps =                       [Run3ModelGen.modelgen]
2024-08-28 19:57:14 [info     ] 		{'name': 'prep_input', 'output_dir': 'input', 'prefix': 'IN'} [Run3ModelGen.modelgen]
2024-08-28 19:57:14 [info     ] 		{'name': 'SPheno', 'input_dir': 'input', 'output_dir': 'SPheno', 'log_dir': 'SPheno_log', 'prefix': 'SP'} [Run3ModelGen.modelgen]
2024-08-28 19:57:14 [info     ] 		{'name': 'softsusy', 'input_dir': 'input', 'output_dir': 'softsusy', 'prefix': 'SS'} [Run3ModelGen.modelgen]
2024-08-28 19:57:14 [info     ] 		{'name': 'micromegas', 'input_dir': 'SPheno', 'output_dir': 'micromegas', 'prefix': 'MO'} [Run3ModelGen.modelgen]
2024-08-28 19:57:14 [info     ] 		{'name': 'superiso', 'input_dir': 'SPheno', 'output_dir': 'superiso', 'prefix': 'SI'} [Run3ModelGen.modelgen]
2024-08-28 19:57:14 [info     ] 		{'name': 'gm2calc', 'input_dir': 'SPheno', 'output_dir': 'gm2calc', 'prefix': 'GM2'} [Run3ModelGen.modelgen]
2024-08-28 19:57:14 [info     ] 		{'name': 'evade', 'input_dir': 'SPheno', 'output_dir': 'evade', 'prefix': 'EV'} [Run3ModelGen.modelgen]
2024-08-28 19:57:14 [info     ] Starting Model Generation...   [Run3ModelGen.modelgen]
2024-08-28 19:57:14 [info     ] Setting up output directories for scan [Run3ModelGen.modelgen]
2024-08-28 19:57:14 [info     ] Generating Model: 0.slha (1/1) [Run3ModelGen.modelgen]
2024-08-28 19:57:14 [info     ] 	running step: prep_input      [Run3ModelGen.modelgen]
2024-08-28 19:57:14 [info     ] 	running step: SPheno          [Run3ModelGen.modelgen]
2024-08-28 19:57:14 [info     ] 	running step: softsusy        [Run3ModelGen.modelgen]
2024-08-28 19:57:14 [info     ] 	running step: micromegas      [Run3ModelGen.modelgen]
2024-08-28 19:57:15 [info     ] 	running step: superiso        [Run3ModelGen.modelgen]
2024-08-28 19:57:16 [info     ] 	running step: gm2calc         [Run3ModelGen.modelgen]
2024-08-28 19:57:16 [info     ] 	running step: evade           [Run3ModelGen.modelgen]
2024-08-28 19:57:16 [info     ] Finished generating models! Stats for successful generations (successful/attempted): [Run3ModelGen.modelgen]
2024-08-28 19:57:16 [info     ] 	prep_input: 1/1.              [Run3ModelGen.modelgen]
2024-08-28 19:57:16 [info     ] 	SPheno: 1/1.                  [Run3ModelGen.modelgen]
2024-08-28 19:57:16 [info     ] 	softsusy: 1/1.                [Run3ModelGen.modelgen]
2024-08-28 19:57:16 [info     ] 	micromegas: 1/1.              [Run3ModelGen.modelgen]
2024-08-28 19:57:16 [info     ] 	superiso: 1/1.                [Run3ModelGen.modelgen]
2024-08-28 19:57:16 [info     ] 	gm2calc: 1/1.                 [Run3ModelGen.modelgen]
2024-08-28 19:57:16 [info     ] 	evade: 0/1.                   [Run3ModelGen.modelgen]
2024-08-28 19:57:16 [info     ] Will make NTuple for: /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/scans/scan_10, with 1 models [Run3ModelGen.ntupling]
2024-08-28 19:57:16 [info     ] 	Filling values for model: 0   [Run3ModelGen.ntupling]
2024-08-28 19:57:16 [info     ] Saving NTuple in /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/scans/scan_10/ntuple.0.0.root ... [Run3ModelGen.ntupling]
Completed iteration 10
Starting iteration 11
Initial Training points:  tensor([0.8528, 0.5812, 0.1617, 0.0046], device='cuda:0') torch.Size([4])
Training points after adding:  tensor([0.0000, 0.0140, 0.4324, 0.1617, 0.1091, 0.0046, 0.2132, 0.0561, 0.3373,
        0.2593, 0.4995, 0.2723, 0.5175, 0.2392, 0.5812, 0.0210, 0.8528],
       device='cuda:0') torch.Size([17])
These training_points are used in the GP tensor([0.0000, 0.0140, 0.4324, 0.1617, 0.1091, 0.0046, 0.2132, 0.0561, 0.3373,
        0.2593, 0.4995, 0.2723, 0.5175, 0.2392, 0.5812, 0.0210, 0.8528],
       device='cuda:0')
Point: tensor([0.0000, 0.0010, 0.0020, 0.0030, 0.0040, 0.0050, 0.0060, 0.0070, 0.0080,
        0.0090, 0.0100, 0.0110, 0.0120, 0.0130, 0.0140, 0.0150, 0.0160, 0.0170,
        0.0180, 0.0190, 0.0200, 0.0210, 0.0220, 0.0230, 0.0240, 0.0250, 0.0260,
        0.0270, 0.0280, 0.0290, 0.0300, 0.0310, 0.0320, 0.0330, 0.0340, 0.0350,
        0.0360, 0.0370, 0.0380, 0.0390, 0.0400, 0.0410, 0.0420, 0.0430, 0.0440,
        0.0450, 0.0460, 0.0470, 0.0480, 0.0490, 0.0501, 0.0511, 0.0521, 0.0531,
        0.0541, 0.0551, 0.0561, 0.0571, 0.0581, 0.0591, 0.0601, 0.0611, 0.0621,
        0.0631, 0.0641, 0.0651, 0.0661, 0.0671, 0.0681, 0.0691, 0.0701, 0.0711,
        0.0721, 0.0731, 0.0741, 0.0751, 0.0761, 0.0771, 0.0781, 0.0791, 0.0801,
        0.0811, 0.0821, 0.0831, 0.0841, 0.0851, 0.0861, 0.0871, 0.0881, 0.0891,
        0.0901, 0.0911, 0.0921, 0.0931, 0.0941, 0.0951, 0.0961, 0.0971, 0.0981,
        0.0991, 0.1001, 0.1011, 0.1021, 0.1031, 0.1041, 0.1051, 0.1061, 0.1071,
        0.1081, 0.1091, 0.1101, 0.1111, 0.1121, 0.1131, 0.1141, 0.1151, 0.1161,
        0.1171, 0.1181, 0.1191, 0.1201, 0.1211, 0.1221, 0.1231, 0.1241, 0.1251,
        0.1261, 0.1271, 0.1281, 0.1291, 0.1301, 0.1311, 0.1321, 0.1331, 0.1341,
        0.1351, 0.1361, 0.1371, 0.1381, 0.1391, 0.1401, 0.1411, 0.1421, 0.1431,
        0.1441, 0.1451, 0.1461, 0.1471, 0.1481, 0.1491, 0.1502, 0.1512, 0.1522,
        0.1532, 0.1542, 0.1552, 0.1562, 0.1572, 0.1582, 0.1592, 0.1602, 0.1612,
        0.1622, 0.1632, 0.1642, 0.1652, 0.1662, 0.1672, 0.1682, 0.1692, 0.1702,
        0.1712, 0.1722, 0.1732, 0.1742, 0.1752, 0.1762, 0.1772, 0.1782, 0.1792,
        0.1802, 0.1812, 0.1822, 0.1832, 0.1842, 0.1852, 0.1862, 0.1872, 0.1882,
        0.1892, 0.1902, 0.1912, 0.1922, 0.1932, 0.1942, 0.1952, 0.1962, 0.1972,
        0.1982, 0.1992, 0.2002, 0.2012, 0.2022, 0.2032, 0.2042, 0.2052, 0.2062,
        0.2072, 0.2082, 0.2092, 0.2102, 0.2112, 0.2122, 0.2132, 0.2142, 0.2152,
        0.2162, 0.2172, 0.2182, 0.2192, 0.2202, 0.2212, 0.2222, 0.2232, 0.2242,
        0.2252, 0.2262, 0.2272, 0.2282, 0.2292, 0.2302, 0.2312, 0.2322, 0.2332,
        0.2342, 0.2352, 0.2362, 0.2372, 0.2382, 0.2392, 0.2402, 0.2412, 0.2422,
        0.2432, 0.2442, 0.2452, 0.2462, 0.2472, 0.2482, 0.2492, 0.2503, 0.2513,
        0.2523, 0.2533, 0.2543, 0.2553, 0.2563, 0.2573, 0.2583, 0.2593, 0.2603,
        0.2613, 0.2623, 0.2633, 0.2643, 0.2653, 0.2663, 0.2673, 0.2683, 0.2693,
        0.2703, 0.2713, 0.2723, 0.2733, 0.2743, 0.2753, 0.2763, 0.2773, 0.2783,
        0.2793, 0.2803, 0.2813, 0.2823, 0.2833, 0.2843, 0.2853, 0.2863, 0.2873,
        0.2883, 0.2893, 0.2903, 0.2913, 0.2923, 0.2933, 0.2943, 0.2953, 0.2963,
        0.2973, 0.2983, 0.2993, 0.3003, 0.3013, 0.3023, 0.3033, 0.3043, 0.3053,
        0.3063, 0.3073, 0.3083, 0.3093, 0.3103, 0.3113, 0.3123, 0.3133, 0.3143,
        0.3153, 0.3163, 0.3173, 0.3183, 0.3193, 0.3203, 0.3213, 0.3223, 0.3233,
        0.3243, 0.3253, 0.3263, 0.3273, 0.3283, 0.3293, 0.3303, 0.3313, 0.3323,
        0.3333, 0.3343, 0.3353, 0.3363, 0.3373, 0.3383, 0.3393, 0.3403, 0.3413,
        0.3423, 0.3433, 0.3443, 0.3453, 0.3463, 0.3473, 0.3483, 0.3493, 0.3504,
        0.3514, 0.3524, 0.3534, 0.3544, 0.3554, 0.3564, 0.3574, 0.3584, 0.3594,
        0.3604, 0.3614, 0.3624, 0.3634, 0.3644, 0.3654, 0.3664, 0.3674, 0.3684,
        0.3694, 0.3704, 0.3714, 0.3724, 0.3734, 0.3744, 0.3754, 0.3764, 0.3774,
        0.3784, 0.3794, 0.3804, 0.3814, 0.3824, 0.3834, 0.3844, 0.3854, 0.3864,
        0.3874, 0.3884, 0.3894, 0.3904, 0.3914, 0.3924, 0.3934, 0.3944, 0.3954,
        0.3964, 0.3974, 0.3984, 0.3994, 0.4004, 0.4014, 0.4024, 0.4034, 0.4044,
        0.4054, 0.4064, 0.4074, 0.4084, 0.4094, 0.4104, 0.4114, 0.4124, 0.4134,
        0.4144, 0.4154, 0.4164, 0.4174, 0.4184, 0.4194, 0.4204, 0.4214, 0.4224,
        0.4234, 0.4244, 0.4254, 0.4264, 0.4274, 0.4284, 0.4294, 0.4304, 0.4314,
        0.4324, 0.4334, 0.4344, 0.4354, 0.4364, 0.4374, 0.4384, 0.4394, 0.4404,
        0.4414, 0.4424, 0.4434, 0.4444, 0.4454, 0.4464, 0.4474, 0.4484, 0.4494,
        0.4505, 0.4515, 0.4525, 0.4535, 0.4545, 0.4555, 0.4565, 0.4575, 0.4585,
        0.4595, 0.4605, 0.4615, 0.4625, 0.4635, 0.4645, 0.4655, 0.4665, 0.4675,
        0.4685, 0.4695, 0.4705, 0.4715, 0.4725, 0.4735, 0.4745, 0.4755, 0.4765,
        0.4775, 0.4785, 0.4795, 0.4805, 0.4815, 0.4825, 0.4835, 0.4845, 0.4855,
        0.4865, 0.4875, 0.4885, 0.4895, 0.4905, 0.4915, 0.4925, 0.4935, 0.4945,
        0.4955, 0.4965, 0.4975, 0.4985, 0.4995, 0.5005, 0.5015, 0.5025, 0.5035,
        0.5045, 0.5055, 0.5065, 0.5075, 0.5085, 0.5095, 0.5105, 0.5115, 0.5125,
        0.5135, 0.5145, 0.5155, 0.5165, 0.5175, 0.5185, 0.5195, 0.5205, 0.5215,
        0.5225, 0.5235, 0.5245, 0.5255, 0.5265, 0.5275, 0.5285, 0.5295, 0.5305,
        0.5315, 0.5325, 0.5335, 0.5345, 0.5355, 0.5365, 0.5375, 0.5385, 0.5395,
        0.5405, 0.5415, 0.5425, 0.5435, 0.5445, 0.5455, 0.5465, 0.5475, 0.5485,
        0.5495, 0.5506, 0.5516, 0.5526, 0.5536, 0.5546, 0.5556, 0.5566, 0.5576,
        0.5586, 0.5596, 0.5606, 0.5616, 0.5626, 0.5636, 0.5646, 0.5656, 0.5666,
        0.5676, 0.5686, 0.5696, 0.5706, 0.5716, 0.5726, 0.5736, 0.5746, 0.5756,
        0.5766, 0.5776, 0.5786, 0.5796, 0.5806, 0.5816, 0.5826, 0.5836, 0.5846,
        0.5856, 0.5866, 0.5876, 0.5886, 0.5896, 0.5906, 0.5916, 0.5926, 0.5936,
        0.5946, 0.5956, 0.5966, 0.5976, 0.5986, 0.5996, 0.6006, 0.6016, 0.6026,
        0.6036, 0.6046, 0.6056, 0.6066, 0.6076, 0.6086, 0.6096, 0.6106, 0.6116,
        0.6126, 0.6136, 0.6146, 0.6156, 0.6166, 0.6176, 0.6186, 0.6196, 0.6206,
        0.6216, 0.6226, 0.6236, 0.6246, 0.6256, 0.6266, 0.6276, 0.6286, 0.6296,
        0.6306, 0.6316, 0.6326, 0.6336, 0.6346, 0.6356, 0.6366, 0.6376, 0.6386,
        0.6396, 0.6406, 0.6416, 0.6426, 0.6436, 0.6446, 0.6456, 0.6466, 0.6476,
        0.6486, 0.6496, 0.6507, 0.6517, 0.6527, 0.6537, 0.6547, 0.6557, 0.6567,
        0.6577, 0.6587, 0.6597, 0.6607, 0.6617, 0.6627, 0.6637, 0.6647, 0.6657,
        0.6667, 0.6677, 0.6687, 0.6697, 0.6707, 0.6717, 0.6727, 0.6737, 0.6747,
        0.6757, 0.6767, 0.6777, 0.6787, 0.6797, 0.6807, 0.6817, 0.6827, 0.6837,
        0.6847, 0.6857, 0.6867, 0.6877, 0.6887, 0.6897, 0.6907, 0.6917, 0.6927,
        0.6937, 0.6947, 0.6957, 0.6967, 0.6977, 0.6987, 0.6997, 0.7007, 0.7017,
        0.7027, 0.7037, 0.7047, 0.7057, 0.7067, 0.7077, 0.7087, 0.7097, 0.7107,
        0.7117, 0.7127, 0.7137, 0.7147, 0.7157, 0.7167, 0.7177, 0.7187, 0.7197,
        0.7207, 0.7217, 0.7227, 0.7237, 0.7247, 0.7257, 0.7267, 0.7277, 0.7287,
        0.7297, 0.7307, 0.7317, 0.7327, 0.7337, 0.7347, 0.7357, 0.7367, 0.7377,
        0.7387, 0.7397, 0.7407, 0.7417, 0.7427, 0.7437, 0.7447, 0.7457, 0.7467,
        0.7477, 0.7487, 0.7497, 0.7508, 0.7518, 0.7528, 0.7538, 0.7548, 0.7558,
        0.7568, 0.7578, 0.7588, 0.7598, 0.7608, 0.7618, 0.7628, 0.7638, 0.7648,
        0.7658, 0.7668, 0.7678, 0.7688, 0.7698, 0.7708, 0.7718, 0.7728, 0.7738,
        0.7748, 0.7758, 0.7768, 0.7778, 0.7788, 0.7798, 0.7808, 0.7818, 0.7828,
        0.7838, 0.7848, 0.7858, 0.7868, 0.7878, 0.7888, 0.7898, 0.7908, 0.7918,
        0.7928, 0.7938, 0.7948, 0.7958, 0.7968, 0.7978, 0.7988, 0.7998, 0.8008,
        0.8018, 0.8028, 0.8038, 0.8048, 0.8058, 0.8068, 0.8078, 0.8088, 0.8098,
        0.8108, 0.8118, 0.8128, 0.8138, 0.8148, 0.8158, 0.8168, 0.8178, 0.8188,
        0.8198, 0.8208, 0.8218, 0.8228, 0.8238, 0.8248, 0.8258, 0.8268, 0.8278,
        0.8288, 0.8298, 0.8308, 0.8318, 0.8328, 0.8338, 0.8348, 0.8358, 0.8368,
        0.8378, 0.8388, 0.8398, 0.8408, 0.8418, 0.8428, 0.8438, 0.8448, 0.8458,
        0.8468, 0.8478, 0.8488, 0.8498, 0.8509, 0.8519, 0.8529, 0.8539, 0.8549,
        0.8559, 0.8569, 0.8579, 0.8589, 0.8599, 0.8609, 0.8619, 0.8629, 0.8639,
        0.8649, 0.8659, 0.8669, 0.8679, 0.8689, 0.8699, 0.8709, 0.8719, 0.8729,
        0.8739, 0.8749, 0.8759, 0.8769, 0.8779, 0.8789, 0.8799, 0.8809, 0.8819,
        0.8829, 0.8839, 0.8849, 0.8859, 0.8869, 0.8879, 0.8889, 0.8899, 0.8909,
        0.8919, 0.8929, 0.8939, 0.8949, 0.8959, 0.8969, 0.8979, 0.8989, 0.8999,
        0.9009, 0.9019, 0.9029, 0.9039, 0.9049, 0.9059, 0.9069, 0.9079, 0.9089,
        0.9099, 0.9109, 0.9119, 0.9129, 0.9139, 0.9149, 0.9159, 0.9169, 0.9179,
        0.9189, 0.9199, 0.9209, 0.9219, 0.9229, 0.9239, 0.9249, 0.9259, 0.9269,
        0.9279, 0.9289, 0.9299, 0.9309, 0.9319, 0.9329, 0.9339, 0.9349, 0.9359,
        0.9369, 0.9379, 0.9389, 0.9399, 0.9409, 0.9419, 0.9429, 0.9439, 0.9449,
        0.9459, 0.9469, 0.9479, 0.9489, 0.9499, 0.9510, 0.9520, 0.9530, 0.9540,
        0.9550, 0.9560, 0.9570, 0.9580, 0.9590, 0.9600, 0.9610, 0.9620, 0.9630,
        0.9640, 0.9650, 0.9660, 0.9670, 0.9680, 0.9690, 0.9700, 0.9710, 0.9720,
        0.9730, 0.9740, 0.9750, 0.9760, 0.9770, 0.9780, 0.9790, 0.9800, 0.9810,
        0.9820, 0.9830, 0.9840, 0.9850, 0.9860, 0.9870, 0.9880, 0.9890, 0.9900,
        0.9910, 0.9920, 0.9930, 0.9940, 0.9950, 0.9960, 0.9970, 0.9980, 0.9990,
        1.0000], device='cuda:0') - Variance: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0') 
Smoothed_batch_entropy:  tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan], device='cuda:0')
Selected points (indices): {0}
Selected new x values (normalized): tensor([0.], device='cuda:0')
Corresponding new x values (unnormalized): tensor([-2000.], device='cuda:0')
Plot saved to /raven/u/dvoss/al_pmssmwithgp/model/plots/Iter11/gp_plot.png
YAML file has been created: /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/data/new_config.yaml
Note: detected 144 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
Note: NumExpr detected 144 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
NumExpr defaulting to 8 threads.

  ___              ____ __  __          _       _   ___            
 | _ \ _  _  _ _  |__ /|  \/  | ___  __| | ___ | | / __| ___  _ _  
 |   /| || || ' \  |_ \| |\/| |/ _ \/ _` |/ -_)| || (_ |/ -_)| ' \ 
 |_|_\ \_,_||_||_||___/|_|  |_|\___/\__,_|\___||_| \___|\___||_||_|
                                                                   

2024-08-28 19:58:00 [info     ] Initialised ModelGenerator with: [Run3ModelGen.modelgen]
2024-08-28 19:58:00 [info     ] 	config_file = /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/data/new_config.yaml [Run3ModelGen.modelgen]
2024-08-28 19:58:00 [info     ] 	scan_dir = /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/scans/scan_11 [Run3ModelGen.modelgen]
2024-08-28 19:58:00 [info     ] 	rawfilen = /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/data/raw.slha [Run3ModelGen.modelgen]
2024-08-28 19:58:00 [info     ] 	seed = 123                    [Run3ModelGen.modelgen]
2024-08-28 19:58:00 [info     ] 	points = {}                   [Run3ModelGen.modelgen]
2024-08-28 19:58:00 [info     ] 	prior = fixed                 [Run3ModelGen.modelgen]
2024-08-28 19:58:00 [info     ] 	num_models = 1                [Run3ModelGen.modelgen]
2024-08-28 19:58:00 [info     ] 	isGMSB = False                [Run3ModelGen.modelgen]
2024-08-28 19:58:00 [info     ] 	parameters =                  [Run3ModelGen.modelgen]
2024-08-28 19:58:00 [info     ] 		tanb: [60]                   [Run3ModelGen.modelgen]
2024-08-28 19:58:00 [info     ] 		M_1: [-2000.0]               [Run3ModelGen.modelgen]
2024-08-28 19:58:00 [info     ] 		M_2: [2000]                  [Run3ModelGen.modelgen]
2024-08-28 19:58:00 [info     ] 		M_3: [4000]                  [Run3ModelGen.modelgen]
2024-08-28 19:58:00 [info     ] 		AT: [4000]                   [Run3ModelGen.modelgen]
2024-08-28 19:58:00 [info     ] 		Ab: [2000]                   [Run3ModelGen.modelgen]
2024-08-28 19:58:00 [info     ] 		Atau: [2000]                 [Run3ModelGen.modelgen]
2024-08-28 19:58:00 [info     ] 		mu: [2000]                   [Run3ModelGen.modelgen]
2024-08-28 19:58:00 [info     ] 		mA: [2000]                   [Run3ModelGen.modelgen]
2024-08-28 19:58:00 [info     ] 		meL: [2000]                  [Run3ModelGen.modelgen]
2024-08-28 19:58:00 [info     ] 		mtauL: [2000]                [Run3ModelGen.modelgen]
2024-08-28 19:58:00 [info     ] 		meR: [2000]                  [Run3ModelGen.modelgen]
2024-08-28 19:58:00 [info     ] 		mtauR: [2000]                [Run3ModelGen.modelgen]
2024-08-28 19:58:00 [info     ] 		mqL1: [4000]                 [Run3ModelGen.modelgen]
2024-08-28 19:58:00 [info     ] 		mqL3: [4000]                 [Run3ModelGen.modelgen]
2024-08-28 19:58:00 [info     ] 		muR: [4000]                  [Run3ModelGen.modelgen]
2024-08-28 19:58:00 [info     ] 		mtR: [4000]                  [Run3ModelGen.modelgen]
2024-08-28 19:58:00 [info     ] 		mdR: [4000]                  [Run3ModelGen.modelgen]
2024-08-28 19:58:00 [info     ] 		mbR: [4000]                  [Run3ModelGen.modelgen]
2024-08-28 19:58:00 [info     ] 	steps =                       [Run3ModelGen.modelgen]
2024-08-28 19:58:00 [info     ] 		{'name': 'prep_input', 'output_dir': 'input', 'prefix': 'IN'} [Run3ModelGen.modelgen]
2024-08-28 19:58:00 [info     ] 		{'name': 'SPheno', 'input_dir': 'input', 'output_dir': 'SPheno', 'log_dir': 'SPheno_log', 'prefix': 'SP'} [Run3ModelGen.modelgen]
2024-08-28 19:58:00 [info     ] 		{'name': 'softsusy', 'input_dir': 'input', 'output_dir': 'softsusy', 'prefix': 'SS'} [Run3ModelGen.modelgen]
2024-08-28 19:58:00 [info     ] 		{'name': 'micromegas', 'input_dir': 'SPheno', 'output_dir': 'micromegas', 'prefix': 'MO'} [Run3ModelGen.modelgen]
2024-08-28 19:58:00 [info     ] 		{'name': 'superiso', 'input_dir': 'SPheno', 'output_dir': 'superiso', 'prefix': 'SI'} [Run3ModelGen.modelgen]
2024-08-28 19:58:00 [info     ] 		{'name': 'gm2calc', 'input_dir': 'SPheno', 'output_dir': 'gm2calc', 'prefix': 'GM2'} [Run3ModelGen.modelgen]
2024-08-28 19:58:00 [info     ] 		{'name': 'evade', 'input_dir': 'SPheno', 'output_dir': 'evade', 'prefix': 'EV'} [Run3ModelGen.modelgen]
2024-08-28 19:58:00 [info     ] Starting Model Generation...   [Run3ModelGen.modelgen]
2024-08-28 19:58:00 [info     ] Setting up output directories for scan [Run3ModelGen.modelgen]
2024-08-28 19:58:00 [info     ] Generating Model: 0.slha (1/1) [Run3ModelGen.modelgen]
2024-08-28 19:58:00 [info     ] 	running step: prep_input      [Run3ModelGen.modelgen]
2024-08-28 19:58:00 [info     ] 	running step: SPheno          [Run3ModelGen.modelgen]
2024-08-28 19:58:01 [info     ] 	running step: softsusy        [Run3ModelGen.modelgen]
2024-08-28 19:58:01 [info     ] 	running step: micromegas      [Run3ModelGen.modelgen]
2024-08-28 19:58:02 [info     ] 	running step: superiso        [Run3ModelGen.modelgen]
2024-08-28 19:58:02 [info     ] 	running step: gm2calc         [Run3ModelGen.modelgen]
2024-08-28 19:58:02 [info     ] 	running step: evade           [Run3ModelGen.modelgen]
2024-08-28 19:58:02 [info     ] Finished generating models! Stats for successful generations (successful/attempted): [Run3ModelGen.modelgen]
2024-08-28 19:58:02 [info     ] 	prep_input: 1/1.              [Run3ModelGen.modelgen]
2024-08-28 19:58:02 [info     ] 	SPheno: 1/1.                  [Run3ModelGen.modelgen]
2024-08-28 19:58:02 [info     ] 	softsusy: 1/1.                [Run3ModelGen.modelgen]
2024-08-28 19:58:02 [info     ] 	micromegas: 1/1.              [Run3ModelGen.modelgen]
2024-08-28 19:58:02 [info     ] 	superiso: 1/1.                [Run3ModelGen.modelgen]
2024-08-28 19:58:02 [info     ] 	gm2calc: 1/1.                 [Run3ModelGen.modelgen]
2024-08-28 19:58:02 [info     ] 	evade: 0/1.                   [Run3ModelGen.modelgen]
2024-08-28 19:58:02 [info     ] Will make NTuple for: /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/scans/scan_11, with 1 models [Run3ModelGen.ntupling]
2024-08-28 19:58:02 [info     ] 	Filling values for model: 0   [Run3ModelGen.ntupling]
2024-08-28 19:58:02 [info     ] Saving NTuple in /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/scans/scan_11/ntuple.0.0.root ... [Run3ModelGen.ntupling]
Completed iteration 11
Starting iteration 12
Initial Training points:  tensor([0.8528, 0.5812, 0.1617, 0.0046], device='cuda:0') torch.Size([4])
Training points after adding:  tensor([0.0000, 0.0140, 0.4324, 0.1617, 0.1091, 0.0046, 0.2132, 0.0561, 0.3373,
        0.2593, 0.4995, 0.2723, 0.5175, 0.2392, 0.5812, 0.0210, 0.8528],
       device='cuda:0') torch.Size([17])
These training_points are used in the GP tensor([0.0000, 0.0140, 0.4324, 0.1617, 0.1091, 0.0046, 0.2132, 0.0561, 0.3373,
        0.2593, 0.4995, 0.2723, 0.5175, 0.2392, 0.5812, 0.0210, 0.8528],
       device='cuda:0')
Point: tensor([0.0000, 0.0010, 0.0020, 0.0030, 0.0040, 0.0050, 0.0060, 0.0070, 0.0080,
        0.0090, 0.0100, 0.0110, 0.0120, 0.0130, 0.0140, 0.0150, 0.0160, 0.0170,
        0.0180, 0.0190, 0.0200, 0.0210, 0.0220, 0.0230, 0.0240, 0.0250, 0.0260,
        0.0270, 0.0280, 0.0290, 0.0300, 0.0310, 0.0320, 0.0330, 0.0340, 0.0350,
        0.0360, 0.0370, 0.0380, 0.0390, 0.0400, 0.0410, 0.0420, 0.0430, 0.0440,
        0.0450, 0.0460, 0.0470, 0.0480, 0.0490, 0.0501, 0.0511, 0.0521, 0.0531,
        0.0541, 0.0551, 0.0561, 0.0571, 0.0581, 0.0591, 0.0601, 0.0611, 0.0621,
        0.0631, 0.0641, 0.0651, 0.0661, 0.0671, 0.0681, 0.0691, 0.0701, 0.0711,
        0.0721, 0.0731, 0.0741, 0.0751, 0.0761, 0.0771, 0.0781, 0.0791, 0.0801,
        0.0811, 0.0821, 0.0831, 0.0841, 0.0851, 0.0861, 0.0871, 0.0881, 0.0891,
        0.0901, 0.0911, 0.0921, 0.0931, 0.0941, 0.0951, 0.0961, 0.0971, 0.0981,
        0.0991, 0.1001, 0.1011, 0.1021, 0.1031, 0.1041, 0.1051, 0.1061, 0.1071,
        0.1081, 0.1091, 0.1101, 0.1111, 0.1121, 0.1131, 0.1141, 0.1151, 0.1161,
        0.1171, 0.1181, 0.1191, 0.1201, 0.1211, 0.1221, 0.1231, 0.1241, 0.1251,
        0.1261, 0.1271, 0.1281, 0.1291, 0.1301, 0.1311, 0.1321, 0.1331, 0.1341,
        0.1351, 0.1361, 0.1371, 0.1381, 0.1391, 0.1401, 0.1411, 0.1421, 0.1431,
        0.1441, 0.1451, 0.1461, 0.1471, 0.1481, 0.1491, 0.1502, 0.1512, 0.1522,
        0.1532, 0.1542, 0.1552, 0.1562, 0.1572, 0.1582, 0.1592, 0.1602, 0.1612,
        0.1622, 0.1632, 0.1642, 0.1652, 0.1662, 0.1672, 0.1682, 0.1692, 0.1702,
        0.1712, 0.1722, 0.1732, 0.1742, 0.1752, 0.1762, 0.1772, 0.1782, 0.1792,
        0.1802, 0.1812, 0.1822, 0.1832, 0.1842, 0.1852, 0.1862, 0.1872, 0.1882,
        0.1892, 0.1902, 0.1912, 0.1922, 0.1932, 0.1942, 0.1952, 0.1962, 0.1972,
        0.1982, 0.1992, 0.2002, 0.2012, 0.2022, 0.2032, 0.2042, 0.2052, 0.2062,
        0.2072, 0.2082, 0.2092, 0.2102, 0.2112, 0.2122, 0.2132, 0.2142, 0.2152,
        0.2162, 0.2172, 0.2182, 0.2192, 0.2202, 0.2212, 0.2222, 0.2232, 0.2242,
        0.2252, 0.2262, 0.2272, 0.2282, 0.2292, 0.2302, 0.2312, 0.2322, 0.2332,
        0.2342, 0.2352, 0.2362, 0.2372, 0.2382, 0.2392, 0.2402, 0.2412, 0.2422,
        0.2432, 0.2442, 0.2452, 0.2462, 0.2472, 0.2482, 0.2492, 0.2503, 0.2513,
        0.2523, 0.2533, 0.2543, 0.2553, 0.2563, 0.2573, 0.2583, 0.2593, 0.2603,
        0.2613, 0.2623, 0.2633, 0.2643, 0.2653, 0.2663, 0.2673, 0.2683, 0.2693,
        0.2703, 0.2713, 0.2723, 0.2733, 0.2743, 0.2753, 0.2763, 0.2773, 0.2783,
        0.2793, 0.2803, 0.2813, 0.2823, 0.2833, 0.2843, 0.2853, 0.2863, 0.2873,
        0.2883, 0.2893, 0.2903, 0.2913, 0.2923, 0.2933, 0.2943, 0.2953, 0.2963,
        0.2973, 0.2983, 0.2993, 0.3003, 0.3013, 0.3023, 0.3033, 0.3043, 0.3053,
        0.3063, 0.3073, 0.3083, 0.3093, 0.3103, 0.3113, 0.3123, 0.3133, 0.3143,
        0.3153, 0.3163, 0.3173, 0.3183, 0.3193, 0.3203, 0.3213, 0.3223, 0.3233,
        0.3243, 0.3253, 0.3263, 0.3273, 0.3283, 0.3293, 0.3303, 0.3313, 0.3323,
        0.3333, 0.3343, 0.3353, 0.3363, 0.3373, 0.3383, 0.3393, 0.3403, 0.3413,
        0.3423, 0.3433, 0.3443, 0.3453, 0.3463, 0.3473, 0.3483, 0.3493, 0.3504,
        0.3514, 0.3524, 0.3534, 0.3544, 0.3554, 0.3564, 0.3574, 0.3584, 0.3594,
        0.3604, 0.3614, 0.3624, 0.3634, 0.3644, 0.3654, 0.3664, 0.3674, 0.3684,
        0.3694, 0.3704, 0.3714, 0.3724, 0.3734, 0.3744, 0.3754, 0.3764, 0.3774,
        0.3784, 0.3794, 0.3804, 0.3814, 0.3824, 0.3834, 0.3844, 0.3854, 0.3864,
        0.3874, 0.3884, 0.3894, 0.3904, 0.3914, 0.3924, 0.3934, 0.3944, 0.3954,
        0.3964, 0.3974, 0.3984, 0.3994, 0.4004, 0.4014, 0.4024, 0.4034, 0.4044,
        0.4054, 0.4064, 0.4074, 0.4084, 0.4094, 0.4104, 0.4114, 0.4124, 0.4134,
        0.4144, 0.4154, 0.4164, 0.4174, 0.4184, 0.4194, 0.4204, 0.4214, 0.4224,
        0.4234, 0.4244, 0.4254, 0.4264, 0.4274, 0.4284, 0.4294, 0.4304, 0.4314,
        0.4324, 0.4334, 0.4344, 0.4354, 0.4364, 0.4374, 0.4384, 0.4394, 0.4404,
        0.4414, 0.4424, 0.4434, 0.4444, 0.4454, 0.4464, 0.4474, 0.4484, 0.4494,
        0.4505, 0.4515, 0.4525, 0.4535, 0.4545, 0.4555, 0.4565, 0.4575, 0.4585,
        0.4595, 0.4605, 0.4615, 0.4625, 0.4635, 0.4645, 0.4655, 0.4665, 0.4675,
        0.4685, 0.4695, 0.4705, 0.4715, 0.4725, 0.4735, 0.4745, 0.4755, 0.4765,
        0.4775, 0.4785, 0.4795, 0.4805, 0.4815, 0.4825, 0.4835, 0.4845, 0.4855,
        0.4865, 0.4875, 0.4885, 0.4895, 0.4905, 0.4915, 0.4925, 0.4935, 0.4945,
        0.4955, 0.4965, 0.4975, 0.4985, 0.4995, 0.5005, 0.5015, 0.5025, 0.5035,
        0.5045, 0.5055, 0.5065, 0.5075, 0.5085, 0.5095, 0.5105, 0.5115, 0.5125,
        0.5135, 0.5145, 0.5155, 0.5165, 0.5175, 0.5185, 0.5195, 0.5205, 0.5215,
        0.5225, 0.5235, 0.5245, 0.5255, 0.5265, 0.5275, 0.5285, 0.5295, 0.5305,
        0.5315, 0.5325, 0.5335, 0.5345, 0.5355, 0.5365, 0.5375, 0.5385, 0.5395,
        0.5405, 0.5415, 0.5425, 0.5435, 0.5445, 0.5455, 0.5465, 0.5475, 0.5485,
        0.5495, 0.5506, 0.5516, 0.5526, 0.5536, 0.5546, 0.5556, 0.5566, 0.5576,
        0.5586, 0.5596, 0.5606, 0.5616, 0.5626, 0.5636, 0.5646, 0.5656, 0.5666,
        0.5676, 0.5686, 0.5696, 0.5706, 0.5716, 0.5726, 0.5736, 0.5746, 0.5756,
        0.5766, 0.5776, 0.5786, 0.5796, 0.5806, 0.5816, 0.5826, 0.5836, 0.5846,
        0.5856, 0.5866, 0.5876, 0.5886, 0.5896, 0.5906, 0.5916, 0.5926, 0.5936,
        0.5946, 0.5956, 0.5966, 0.5976, 0.5986, 0.5996, 0.6006, 0.6016, 0.6026,
        0.6036, 0.6046, 0.6056, 0.6066, 0.6076, 0.6086, 0.6096, 0.6106, 0.6116,
        0.6126, 0.6136, 0.6146, 0.6156, 0.6166, 0.6176, 0.6186, 0.6196, 0.6206,
        0.6216, 0.6226, 0.6236, 0.6246, 0.6256, 0.6266, 0.6276, 0.6286, 0.6296,
        0.6306, 0.6316, 0.6326, 0.6336, 0.6346, 0.6356, 0.6366, 0.6376, 0.6386,
        0.6396, 0.6406, 0.6416, 0.6426, 0.6436, 0.6446, 0.6456, 0.6466, 0.6476,
        0.6486, 0.6496, 0.6507, 0.6517, 0.6527, 0.6537, 0.6547, 0.6557, 0.6567,
        0.6577, 0.6587, 0.6597, 0.6607, 0.6617, 0.6627, 0.6637, 0.6647, 0.6657,
        0.6667, 0.6677, 0.6687, 0.6697, 0.6707, 0.6717, 0.6727, 0.6737, 0.6747,
        0.6757, 0.6767, 0.6777, 0.6787, 0.6797, 0.6807, 0.6817, 0.6827, 0.6837,
        0.6847, 0.6857, 0.6867, 0.6877, 0.6887, 0.6897, 0.6907, 0.6917, 0.6927,
        0.6937, 0.6947, 0.6957, 0.6967, 0.6977, 0.6987, 0.6997, 0.7007, 0.7017,
        0.7027, 0.7037, 0.7047, 0.7057, 0.7067, 0.7077, 0.7087, 0.7097, 0.7107,
        0.7117, 0.7127, 0.7137, 0.7147, 0.7157, 0.7167, 0.7177, 0.7187, 0.7197,
        0.7207, 0.7217, 0.7227, 0.7237, 0.7247, 0.7257, 0.7267, 0.7277, 0.7287,
        0.7297, 0.7307, 0.7317, 0.7327, 0.7337, 0.7347, 0.7357, 0.7367, 0.7377,
        0.7387, 0.7397, 0.7407, 0.7417, 0.7427, 0.7437, 0.7447, 0.7457, 0.7467,
        0.7477, 0.7487, 0.7497, 0.7508, 0.7518, 0.7528, 0.7538, 0.7548, 0.7558,
        0.7568, 0.7578, 0.7588, 0.7598, 0.7608, 0.7618, 0.7628, 0.7638, 0.7648,
        0.7658, 0.7668, 0.7678, 0.7688, 0.7698, 0.7708, 0.7718, 0.7728, 0.7738,
        0.7748, 0.7758, 0.7768, 0.7778, 0.7788, 0.7798, 0.7808, 0.7818, 0.7828,
        0.7838, 0.7848, 0.7858, 0.7868, 0.7878, 0.7888, 0.7898, 0.7908, 0.7918,
        0.7928, 0.7938, 0.7948, 0.7958, 0.7968, 0.7978, 0.7988, 0.7998, 0.8008,
        0.8018, 0.8028, 0.8038, 0.8048, 0.8058, 0.8068, 0.8078, 0.8088, 0.8098,
        0.8108, 0.8118, 0.8128, 0.8138, 0.8148, 0.8158, 0.8168, 0.8178, 0.8188,
        0.8198, 0.8208, 0.8218, 0.8228, 0.8238, 0.8248, 0.8258, 0.8268, 0.8278,
        0.8288, 0.8298, 0.8308, 0.8318, 0.8328, 0.8338, 0.8348, 0.8358, 0.8368,
        0.8378, 0.8388, 0.8398, 0.8408, 0.8418, 0.8428, 0.8438, 0.8448, 0.8458,
        0.8468, 0.8478, 0.8488, 0.8498, 0.8509, 0.8519, 0.8529, 0.8539, 0.8549,
        0.8559, 0.8569, 0.8579, 0.8589, 0.8599, 0.8609, 0.8619, 0.8629, 0.8639,
        0.8649, 0.8659, 0.8669, 0.8679, 0.8689, 0.8699, 0.8709, 0.8719, 0.8729,
        0.8739, 0.8749, 0.8759, 0.8769, 0.8779, 0.8789, 0.8799, 0.8809, 0.8819,
        0.8829, 0.8839, 0.8849, 0.8859, 0.8869, 0.8879, 0.8889, 0.8899, 0.8909,
        0.8919, 0.8929, 0.8939, 0.8949, 0.8959, 0.8969, 0.8979, 0.8989, 0.8999,
        0.9009, 0.9019, 0.9029, 0.9039, 0.9049, 0.9059, 0.9069, 0.9079, 0.9089,
        0.9099, 0.9109, 0.9119, 0.9129, 0.9139, 0.9149, 0.9159, 0.9169, 0.9179,
        0.9189, 0.9199, 0.9209, 0.9219, 0.9229, 0.9239, 0.9249, 0.9259, 0.9269,
        0.9279, 0.9289, 0.9299, 0.9309, 0.9319, 0.9329, 0.9339, 0.9349, 0.9359,
        0.9369, 0.9379, 0.9389, 0.9399, 0.9409, 0.9419, 0.9429, 0.9439, 0.9449,
        0.9459, 0.9469, 0.9479, 0.9489, 0.9499, 0.9510, 0.9520, 0.9530, 0.9540,
        0.9550, 0.9560, 0.9570, 0.9580, 0.9590, 0.9600, 0.9610, 0.9620, 0.9630,
        0.9640, 0.9650, 0.9660, 0.9670, 0.9680, 0.9690, 0.9700, 0.9710, 0.9720,
        0.9730, 0.9740, 0.9750, 0.9760, 0.9770, 0.9780, 0.9790, 0.9800, 0.9810,
        0.9820, 0.9830, 0.9840, 0.9850, 0.9860, 0.9870, 0.9880, 0.9890, 0.9900,
        0.9910, 0.9920, 0.9930, 0.9940, 0.9950, 0.9960, 0.9970, 0.9980, 0.9990,
        1.0000], device='cuda:0') - Variance: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0') 
Smoothed_batch_entropy:  tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan], device='cuda:0')
Selected points (indices): {0}
Selected new x values (normalized): tensor([0.], device='cuda:0')
Corresponding new x values (unnormalized): tensor([-2000.], device='cuda:0')
Plot saved to /raven/u/dvoss/al_pmssmwithgp/model/plots/Iter12/gp_plot.png
YAML file has been created: /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/data/new_config.yaml
Note: detected 144 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
Note: NumExpr detected 144 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
NumExpr defaulting to 8 threads.

  ___              ____ __  __          _       _   ___            
 | _ \ _  _  _ _  |__ /|  \/  | ___  __| | ___ | | / __| ___  _ _  
 |   /| || || ' \  |_ \| |\/| |/ _ \/ _` |/ -_)| || (_ |/ -_)| ' \ 
 |_|_\ \_,_||_||_||___/|_|  |_|\___/\__,_|\___||_| \___|\___||_||_|
                                                                   

2024-08-28 19:58:47 [info     ] Initialised ModelGenerator with: [Run3ModelGen.modelgen]
2024-08-28 19:58:47 [info     ] 	config_file = /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/data/new_config.yaml [Run3ModelGen.modelgen]
2024-08-28 19:58:47 [info     ] 	scan_dir = /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/scans/scan_12 [Run3ModelGen.modelgen]
2024-08-28 19:58:47 [info     ] 	rawfilen = /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/data/raw.slha [Run3ModelGen.modelgen]
2024-08-28 19:58:47 [info     ] 	seed = 123                    [Run3ModelGen.modelgen]
2024-08-28 19:58:47 [info     ] 	points = {}                   [Run3ModelGen.modelgen]
2024-08-28 19:58:47 [info     ] 	prior = fixed                 [Run3ModelGen.modelgen]
2024-08-28 19:58:47 [info     ] 	num_models = 1                [Run3ModelGen.modelgen]
2024-08-28 19:58:47 [info     ] 	isGMSB = False                [Run3ModelGen.modelgen]
2024-08-28 19:58:47 [info     ] 	parameters =                  [Run3ModelGen.modelgen]
2024-08-28 19:58:47 [info     ] 		tanb: [60]                   [Run3ModelGen.modelgen]
2024-08-28 19:58:47 [info     ] 		M_1: [-2000.0]               [Run3ModelGen.modelgen]
2024-08-28 19:58:47 [info     ] 		M_2: [2000]                  [Run3ModelGen.modelgen]
2024-08-28 19:58:47 [info     ] 		M_3: [4000]                  [Run3ModelGen.modelgen]
2024-08-28 19:58:47 [info     ] 		AT: [4000]                   [Run3ModelGen.modelgen]
2024-08-28 19:58:47 [info     ] 		Ab: [2000]                   [Run3ModelGen.modelgen]
2024-08-28 19:58:47 [info     ] 		Atau: [2000]                 [Run3ModelGen.modelgen]
2024-08-28 19:58:47 [info     ] 		mu: [2000]                   [Run3ModelGen.modelgen]
2024-08-28 19:58:47 [info     ] 		mA: [2000]                   [Run3ModelGen.modelgen]
2024-08-28 19:58:47 [info     ] 		meL: [2000]                  [Run3ModelGen.modelgen]
2024-08-28 19:58:47 [info     ] 		mtauL: [2000]                [Run3ModelGen.modelgen]
2024-08-28 19:58:47 [info     ] 		meR: [2000]                  [Run3ModelGen.modelgen]
2024-08-28 19:58:47 [info     ] 		mtauR: [2000]                [Run3ModelGen.modelgen]
2024-08-28 19:58:47 [info     ] 		mqL1: [4000]                 [Run3ModelGen.modelgen]
2024-08-28 19:58:47 [info     ] 		mqL3: [4000]                 [Run3ModelGen.modelgen]
2024-08-28 19:58:47 [info     ] 		muR: [4000]                  [Run3ModelGen.modelgen]
2024-08-28 19:58:47 [info     ] 		mtR: [4000]                  [Run3ModelGen.modelgen]
2024-08-28 19:58:47 [info     ] 		mdR: [4000]                  [Run3ModelGen.modelgen]
2024-08-28 19:58:47 [info     ] 		mbR: [4000]                  [Run3ModelGen.modelgen]
2024-08-28 19:58:47 [info     ] 	steps =                       [Run3ModelGen.modelgen]
2024-08-28 19:58:47 [info     ] 		{'name': 'prep_input', 'output_dir': 'input', 'prefix': 'IN'} [Run3ModelGen.modelgen]
2024-08-28 19:58:47 [info     ] 		{'name': 'SPheno', 'input_dir': 'input', 'output_dir': 'SPheno', 'log_dir': 'SPheno_log', 'prefix': 'SP'} [Run3ModelGen.modelgen]
2024-08-28 19:58:47 [info     ] 		{'name': 'softsusy', 'input_dir': 'input', 'output_dir': 'softsusy', 'prefix': 'SS'} [Run3ModelGen.modelgen]
2024-08-28 19:58:47 [info     ] 		{'name': 'micromegas', 'input_dir': 'SPheno', 'output_dir': 'micromegas', 'prefix': 'MO'} [Run3ModelGen.modelgen]
2024-08-28 19:58:47 [info     ] 		{'name': 'superiso', 'input_dir': 'SPheno', 'output_dir': 'superiso', 'prefix': 'SI'} [Run3ModelGen.modelgen]
2024-08-28 19:58:47 [info     ] 		{'name': 'gm2calc', 'input_dir': 'SPheno', 'output_dir': 'gm2calc', 'prefix': 'GM2'} [Run3ModelGen.modelgen]
2024-08-28 19:58:47 [info     ] 		{'name': 'evade', 'input_dir': 'SPheno', 'output_dir': 'evade', 'prefix': 'EV'} [Run3ModelGen.modelgen]
2024-08-28 19:58:47 [info     ] Starting Model Generation...   [Run3ModelGen.modelgen]
2024-08-28 19:58:47 [info     ] Setting up output directories for scan [Run3ModelGen.modelgen]
2024-08-28 19:58:47 [info     ] Generating Model: 0.slha (1/1) [Run3ModelGen.modelgen]
2024-08-28 19:58:47 [info     ] 	running step: prep_input      [Run3ModelGen.modelgen]
2024-08-28 19:58:47 [info     ] 	running step: SPheno          [Run3ModelGen.modelgen]
2024-08-28 19:58:47 [info     ] 	running step: softsusy        [Run3ModelGen.modelgen]
2024-08-28 19:58:47 [info     ] 	running step: micromegas      [Run3ModelGen.modelgen]
2024-08-28 19:58:48 [info     ] 	running step: superiso        [Run3ModelGen.modelgen]
2024-08-28 19:58:49 [info     ] 	running step: gm2calc         [Run3ModelGen.modelgen]
2024-08-28 19:58:49 [info     ] 	running step: evade           [Run3ModelGen.modelgen]
2024-08-28 19:58:49 [info     ] Finished generating models! Stats for successful generations (successful/attempted): [Run3ModelGen.modelgen]
2024-08-28 19:58:49 [info     ] 	prep_input: 1/1.              [Run3ModelGen.modelgen]
2024-08-28 19:58:49 [info     ] 	SPheno: 1/1.                  [Run3ModelGen.modelgen]
2024-08-28 19:58:49 [info     ] 	softsusy: 1/1.                [Run3ModelGen.modelgen]
2024-08-28 19:58:49 [info     ] 	micromegas: 1/1.              [Run3ModelGen.modelgen]
2024-08-28 19:58:49 [info     ] 	superiso: 1/1.                [Run3ModelGen.modelgen]
2024-08-28 19:58:49 [info     ] 	gm2calc: 1/1.                 [Run3ModelGen.modelgen]
2024-08-28 19:58:49 [info     ] 	evade: 0/1.                   [Run3ModelGen.modelgen]
2024-08-28 19:58:49 [info     ] Will make NTuple for: /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/scans/scan_12, with 1 models [Run3ModelGen.ntupling]
2024-08-28 19:58:49 [info     ] 	Filling values for model: 0   [Run3ModelGen.ntupling]
2024-08-28 19:58:49 [info     ] Saving NTuple in /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/scans/scan_12/ntuple.0.0.root ... [Run3ModelGen.ntupling]
Completed iteration 12
Starting iteration 13
Initial Training points:  tensor([0.8528, 0.5812, 0.1617, 0.0046], device='cuda:0') torch.Size([4])
Training points after adding:  tensor([0.0000, 0.0140, 0.4324, 0.1617, 0.1091, 0.0046, 0.2132, 0.0561, 0.3373,
        0.2593, 0.4995, 0.2723, 0.5175, 0.2392, 0.5812, 0.0210, 0.8528],
       device='cuda:0') torch.Size([17])
These training_points are used in the GP tensor([0.0000, 0.0140, 0.4324, 0.1617, 0.1091, 0.0046, 0.2132, 0.0561, 0.3373,
        0.2593, 0.4995, 0.2723, 0.5175, 0.2392, 0.5812, 0.0210, 0.8528],
       device='cuda:0')
Point: tensor([0.0000, 0.0010, 0.0020, 0.0030, 0.0040, 0.0050, 0.0060, 0.0070, 0.0080,
        0.0090, 0.0100, 0.0110, 0.0120, 0.0130, 0.0140, 0.0150, 0.0160, 0.0170,
        0.0180, 0.0190, 0.0200, 0.0210, 0.0220, 0.0230, 0.0240, 0.0250, 0.0260,
        0.0270, 0.0280, 0.0290, 0.0300, 0.0310, 0.0320, 0.0330, 0.0340, 0.0350,
        0.0360, 0.0370, 0.0380, 0.0390, 0.0400, 0.0410, 0.0420, 0.0430, 0.0440,
        0.0450, 0.0460, 0.0470, 0.0480, 0.0490, 0.0501, 0.0511, 0.0521, 0.0531,
        0.0541, 0.0551, 0.0561, 0.0571, 0.0581, 0.0591, 0.0601, 0.0611, 0.0621,
        0.0631, 0.0641, 0.0651, 0.0661, 0.0671, 0.0681, 0.0691, 0.0701, 0.0711,
        0.0721, 0.0731, 0.0741, 0.0751, 0.0761, 0.0771, 0.0781, 0.0791, 0.0801,
        0.0811, 0.0821, 0.0831, 0.0841, 0.0851, 0.0861, 0.0871, 0.0881, 0.0891,
        0.0901, 0.0911, 0.0921, 0.0931, 0.0941, 0.0951, 0.0961, 0.0971, 0.0981,
        0.0991, 0.1001, 0.1011, 0.1021, 0.1031, 0.1041, 0.1051, 0.1061, 0.1071,
        0.1081, 0.1091, 0.1101, 0.1111, 0.1121, 0.1131, 0.1141, 0.1151, 0.1161,
        0.1171, 0.1181, 0.1191, 0.1201, 0.1211, 0.1221, 0.1231, 0.1241, 0.1251,
        0.1261, 0.1271, 0.1281, 0.1291, 0.1301, 0.1311, 0.1321, 0.1331, 0.1341,
        0.1351, 0.1361, 0.1371, 0.1381, 0.1391, 0.1401, 0.1411, 0.1421, 0.1431,
        0.1441, 0.1451, 0.1461, 0.1471, 0.1481, 0.1491, 0.1502, 0.1512, 0.1522,
        0.1532, 0.1542, 0.1552, 0.1562, 0.1572, 0.1582, 0.1592, 0.1602, 0.1612,
        0.1622, 0.1632, 0.1642, 0.1652, 0.1662, 0.1672, 0.1682, 0.1692, 0.1702,
        0.1712, 0.1722, 0.1732, 0.1742, 0.1752, 0.1762, 0.1772, 0.1782, 0.1792,
        0.1802, 0.1812, 0.1822, 0.1832, 0.1842, 0.1852, 0.1862, 0.1872, 0.1882,
        0.1892, 0.1902, 0.1912, 0.1922, 0.1932, 0.1942, 0.1952, 0.1962, 0.1972,
        0.1982, 0.1992, 0.2002, 0.2012, 0.2022, 0.2032, 0.2042, 0.2052, 0.2062,
        0.2072, 0.2082, 0.2092, 0.2102, 0.2112, 0.2122, 0.2132, 0.2142, 0.2152,
        0.2162, 0.2172, 0.2182, 0.2192, 0.2202, 0.2212, 0.2222, 0.2232, 0.2242,
        0.2252, 0.2262, 0.2272, 0.2282, 0.2292, 0.2302, 0.2312, 0.2322, 0.2332,
        0.2342, 0.2352, 0.2362, 0.2372, 0.2382, 0.2392, 0.2402, 0.2412, 0.2422,
        0.2432, 0.2442, 0.2452, 0.2462, 0.2472, 0.2482, 0.2492, 0.2503, 0.2513,
        0.2523, 0.2533, 0.2543, 0.2553, 0.2563, 0.2573, 0.2583, 0.2593, 0.2603,
        0.2613, 0.2623, 0.2633, 0.2643, 0.2653, 0.2663, 0.2673, 0.2683, 0.2693,
        0.2703, 0.2713, 0.2723, 0.2733, 0.2743, 0.2753, 0.2763, 0.2773, 0.2783,
        0.2793, 0.2803, 0.2813, 0.2823, 0.2833, 0.2843, 0.2853, 0.2863, 0.2873,
        0.2883, 0.2893, 0.2903, 0.2913, 0.2923, 0.2933, 0.2943, 0.2953, 0.2963,
        0.2973, 0.2983, 0.2993, 0.3003, 0.3013, 0.3023, 0.3033, 0.3043, 0.3053,
        0.3063, 0.3073, 0.3083, 0.3093, 0.3103, 0.3113, 0.3123, 0.3133, 0.3143,
        0.3153, 0.3163, 0.3173, 0.3183, 0.3193, 0.3203, 0.3213, 0.3223, 0.3233,
        0.3243, 0.3253, 0.3263, 0.3273, 0.3283, 0.3293, 0.3303, 0.3313, 0.3323,
        0.3333, 0.3343, 0.3353, 0.3363, 0.3373, 0.3383, 0.3393, 0.3403, 0.3413,
        0.3423, 0.3433, 0.3443, 0.3453, 0.3463, 0.3473, 0.3483, 0.3493, 0.3504,
        0.3514, 0.3524, 0.3534, 0.3544, 0.3554, 0.3564, 0.3574, 0.3584, 0.3594,
        0.3604, 0.3614, 0.3624, 0.3634, 0.3644, 0.3654, 0.3664, 0.3674, 0.3684,
        0.3694, 0.3704, 0.3714, 0.3724, 0.3734, 0.3744, 0.3754, 0.3764, 0.3774,
        0.3784, 0.3794, 0.3804, 0.3814, 0.3824, 0.3834, 0.3844, 0.3854, 0.3864,
        0.3874, 0.3884, 0.3894, 0.3904, 0.3914, 0.3924, 0.3934, 0.3944, 0.3954,
        0.3964, 0.3974, 0.3984, 0.3994, 0.4004, 0.4014, 0.4024, 0.4034, 0.4044,
        0.4054, 0.4064, 0.4074, 0.4084, 0.4094, 0.4104, 0.4114, 0.4124, 0.4134,
        0.4144, 0.4154, 0.4164, 0.4174, 0.4184, 0.4194, 0.4204, 0.4214, 0.4224,
        0.4234, 0.4244, 0.4254, 0.4264, 0.4274, 0.4284, 0.4294, 0.4304, 0.4314,
        0.4324, 0.4334, 0.4344, 0.4354, 0.4364, 0.4374, 0.4384, 0.4394, 0.4404,
        0.4414, 0.4424, 0.4434, 0.4444, 0.4454, 0.4464, 0.4474, 0.4484, 0.4494,
        0.4505, 0.4515, 0.4525, 0.4535, 0.4545, 0.4555, 0.4565, 0.4575, 0.4585,
        0.4595, 0.4605, 0.4615, 0.4625, 0.4635, 0.4645, 0.4655, 0.4665, 0.4675,
        0.4685, 0.4695, 0.4705, 0.4715, 0.4725, 0.4735, 0.4745, 0.4755, 0.4765,
        0.4775, 0.4785, 0.4795, 0.4805, 0.4815, 0.4825, 0.4835, 0.4845, 0.4855,
        0.4865, 0.4875, 0.4885, 0.4895, 0.4905, 0.4915, 0.4925, 0.4935, 0.4945,
        0.4955, 0.4965, 0.4975, 0.4985, 0.4995, 0.5005, 0.5015, 0.5025, 0.5035,
        0.5045, 0.5055, 0.5065, 0.5075, 0.5085, 0.5095, 0.5105, 0.5115, 0.5125,
        0.5135, 0.5145, 0.5155, 0.5165, 0.5175, 0.5185, 0.5195, 0.5205, 0.5215,
        0.5225, 0.5235, 0.5245, 0.5255, 0.5265, 0.5275, 0.5285, 0.5295, 0.5305,
        0.5315, 0.5325, 0.5335, 0.5345, 0.5355, 0.5365, 0.5375, 0.5385, 0.5395,
        0.5405, 0.5415, 0.5425, 0.5435, 0.5445, 0.5455, 0.5465, 0.5475, 0.5485,
        0.5495, 0.5506, 0.5516, 0.5526, 0.5536, 0.5546, 0.5556, 0.5566, 0.5576,
        0.5586, 0.5596, 0.5606, 0.5616, 0.5626, 0.5636, 0.5646, 0.5656, 0.5666,
        0.5676, 0.5686, 0.5696, 0.5706, 0.5716, 0.5726, 0.5736, 0.5746, 0.5756,
        0.5766, 0.5776, 0.5786, 0.5796, 0.5806, 0.5816, 0.5826, 0.5836, 0.5846,
        0.5856, 0.5866, 0.5876, 0.5886, 0.5896, 0.5906, 0.5916, 0.5926, 0.5936,
        0.5946, 0.5956, 0.5966, 0.5976, 0.5986, 0.5996, 0.6006, 0.6016, 0.6026,
        0.6036, 0.6046, 0.6056, 0.6066, 0.6076, 0.6086, 0.6096, 0.6106, 0.6116,
        0.6126, 0.6136, 0.6146, 0.6156, 0.6166, 0.6176, 0.6186, 0.6196, 0.6206,
        0.6216, 0.6226, 0.6236, 0.6246, 0.6256, 0.6266, 0.6276, 0.6286, 0.6296,
        0.6306, 0.6316, 0.6326, 0.6336, 0.6346, 0.6356, 0.6366, 0.6376, 0.6386,
        0.6396, 0.6406, 0.6416, 0.6426, 0.6436, 0.6446, 0.6456, 0.6466, 0.6476,
        0.6486, 0.6496, 0.6507, 0.6517, 0.6527, 0.6537, 0.6547, 0.6557, 0.6567,
        0.6577, 0.6587, 0.6597, 0.6607, 0.6617, 0.6627, 0.6637, 0.6647, 0.6657,
        0.6667, 0.6677, 0.6687, 0.6697, 0.6707, 0.6717, 0.6727, 0.6737, 0.6747,
        0.6757, 0.6767, 0.6777, 0.6787, 0.6797, 0.6807, 0.6817, 0.6827, 0.6837,
        0.6847, 0.6857, 0.6867, 0.6877, 0.6887, 0.6897, 0.6907, 0.6917, 0.6927,
        0.6937, 0.6947, 0.6957, 0.6967, 0.6977, 0.6987, 0.6997, 0.7007, 0.7017,
        0.7027, 0.7037, 0.7047, 0.7057, 0.7067, 0.7077, 0.7087, 0.7097, 0.7107,
        0.7117, 0.7127, 0.7137, 0.7147, 0.7157, 0.7167, 0.7177, 0.7187, 0.7197,
        0.7207, 0.7217, 0.7227, 0.7237, 0.7247, 0.7257, 0.7267, 0.7277, 0.7287,
        0.7297, 0.7307, 0.7317, 0.7327, 0.7337, 0.7347, 0.7357, 0.7367, 0.7377,
        0.7387, 0.7397, 0.7407, 0.7417, 0.7427, 0.7437, 0.7447, 0.7457, 0.7467,
        0.7477, 0.7487, 0.7497, 0.7508, 0.7518, 0.7528, 0.7538, 0.7548, 0.7558,
        0.7568, 0.7578, 0.7588, 0.7598, 0.7608, 0.7618, 0.7628, 0.7638, 0.7648,
        0.7658, 0.7668, 0.7678, 0.7688, 0.7698, 0.7708, 0.7718, 0.7728, 0.7738,
        0.7748, 0.7758, 0.7768, 0.7778, 0.7788, 0.7798, 0.7808, 0.7818, 0.7828,
        0.7838, 0.7848, 0.7858, 0.7868, 0.7878, 0.7888, 0.7898, 0.7908, 0.7918,
        0.7928, 0.7938, 0.7948, 0.7958, 0.7968, 0.7978, 0.7988, 0.7998, 0.8008,
        0.8018, 0.8028, 0.8038, 0.8048, 0.8058, 0.8068, 0.8078, 0.8088, 0.8098,
        0.8108, 0.8118, 0.8128, 0.8138, 0.8148, 0.8158, 0.8168, 0.8178, 0.8188,
        0.8198, 0.8208, 0.8218, 0.8228, 0.8238, 0.8248, 0.8258, 0.8268, 0.8278,
        0.8288, 0.8298, 0.8308, 0.8318, 0.8328, 0.8338, 0.8348, 0.8358, 0.8368,
        0.8378, 0.8388, 0.8398, 0.8408, 0.8418, 0.8428, 0.8438, 0.8448, 0.8458,
        0.8468, 0.8478, 0.8488, 0.8498, 0.8509, 0.8519, 0.8529, 0.8539, 0.8549,
        0.8559, 0.8569, 0.8579, 0.8589, 0.8599, 0.8609, 0.8619, 0.8629, 0.8639,
        0.8649, 0.8659, 0.8669, 0.8679, 0.8689, 0.8699, 0.8709, 0.8719, 0.8729,
        0.8739, 0.8749, 0.8759, 0.8769, 0.8779, 0.8789, 0.8799, 0.8809, 0.8819,
        0.8829, 0.8839, 0.8849, 0.8859, 0.8869, 0.8879, 0.8889, 0.8899, 0.8909,
        0.8919, 0.8929, 0.8939, 0.8949, 0.8959, 0.8969, 0.8979, 0.8989, 0.8999,
        0.9009, 0.9019, 0.9029, 0.9039, 0.9049, 0.9059, 0.9069, 0.9079, 0.9089,
        0.9099, 0.9109, 0.9119, 0.9129, 0.9139, 0.9149, 0.9159, 0.9169, 0.9179,
        0.9189, 0.9199, 0.9209, 0.9219, 0.9229, 0.9239, 0.9249, 0.9259, 0.9269,
        0.9279, 0.9289, 0.9299, 0.9309, 0.9319, 0.9329, 0.9339, 0.9349, 0.9359,
        0.9369, 0.9379, 0.9389, 0.9399, 0.9409, 0.9419, 0.9429, 0.9439, 0.9449,
        0.9459, 0.9469, 0.9479, 0.9489, 0.9499, 0.9510, 0.9520, 0.9530, 0.9540,
        0.9550, 0.9560, 0.9570, 0.9580, 0.9590, 0.9600, 0.9610, 0.9620, 0.9630,
        0.9640, 0.9650, 0.9660, 0.9670, 0.9680, 0.9690, 0.9700, 0.9710, 0.9720,
        0.9730, 0.9740, 0.9750, 0.9760, 0.9770, 0.9780, 0.9790, 0.9800, 0.9810,
        0.9820, 0.9830, 0.9840, 0.9850, 0.9860, 0.9870, 0.9880, 0.9890, 0.9900,
        0.9910, 0.9920, 0.9930, 0.9940, 0.9950, 0.9960, 0.9970, 0.9980, 0.9990,
        1.0000], device='cuda:0') - Variance: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0') 
Smoothed_batch_entropy:  tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan], device='cuda:0')
Selected points (indices): {0}
Selected new x values (normalized): tensor([0.], device='cuda:0')
Corresponding new x values (unnormalized): tensor([-2000.], device='cuda:0')
Plot saved to /raven/u/dvoss/al_pmssmwithgp/model/plots/Iter13/gp_plot.png
YAML file has been created: /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/data/new_config.yaml
Note: detected 144 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
Note: NumExpr detected 144 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
NumExpr defaulting to 8 threads.

  ___              ____ __  __          _       _   ___            
 | _ \ _  _  _ _  |__ /|  \/  | ___  __| | ___ | | / __| ___  _ _  
 |   /| || || ' \  |_ \| |\/| |/ _ \/ _` |/ -_)| || (_ |/ -_)| ' \ 
 |_|_\ \_,_||_||_||___/|_|  |_|\___/\__,_|\___||_| \___|\___||_||_|
                                                                   

2024-08-28 19:59:33 [info     ] Initialised ModelGenerator with: [Run3ModelGen.modelgen]
2024-08-28 19:59:33 [info     ] 	config_file = /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/data/new_config.yaml [Run3ModelGen.modelgen]
2024-08-28 19:59:33 [info     ] 	scan_dir = /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/scans/scan_13 [Run3ModelGen.modelgen]
2024-08-28 19:59:33 [info     ] 	rawfilen = /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/data/raw.slha [Run3ModelGen.modelgen]
2024-08-28 19:59:33 [info     ] 	seed = 123                    [Run3ModelGen.modelgen]
2024-08-28 19:59:33 [info     ] 	points = {}                   [Run3ModelGen.modelgen]
2024-08-28 19:59:33 [info     ] 	prior = fixed                 [Run3ModelGen.modelgen]
2024-08-28 19:59:33 [info     ] 	num_models = 1                [Run3ModelGen.modelgen]
2024-08-28 19:59:33 [info     ] 	isGMSB = False                [Run3ModelGen.modelgen]
2024-08-28 19:59:33 [info     ] 	parameters =                  [Run3ModelGen.modelgen]
2024-08-28 19:59:33 [info     ] 		tanb: [60]                   [Run3ModelGen.modelgen]
2024-08-28 19:59:33 [info     ] 		M_1: [-2000.0]               [Run3ModelGen.modelgen]
2024-08-28 19:59:33 [info     ] 		M_2: [2000]                  [Run3ModelGen.modelgen]
2024-08-28 19:59:33 [info     ] 		M_3: [4000]                  [Run3ModelGen.modelgen]
2024-08-28 19:59:33 [info     ] 		AT: [4000]                   [Run3ModelGen.modelgen]
2024-08-28 19:59:33 [info     ] 		Ab: [2000]                   [Run3ModelGen.modelgen]
2024-08-28 19:59:33 [info     ] 		Atau: [2000]                 [Run3ModelGen.modelgen]
2024-08-28 19:59:33 [info     ] 		mu: [2000]                   [Run3ModelGen.modelgen]
2024-08-28 19:59:33 [info     ] 		mA: [2000]                   [Run3ModelGen.modelgen]
2024-08-28 19:59:33 [info     ] 		meL: [2000]                  [Run3ModelGen.modelgen]
2024-08-28 19:59:33 [info     ] 		mtauL: [2000]                [Run3ModelGen.modelgen]
2024-08-28 19:59:33 [info     ] 		meR: [2000]                  [Run3ModelGen.modelgen]
2024-08-28 19:59:33 [info     ] 		mtauR: [2000]                [Run3ModelGen.modelgen]
2024-08-28 19:59:33 [info     ] 		mqL1: [4000]                 [Run3ModelGen.modelgen]
2024-08-28 19:59:33 [info     ] 		mqL3: [4000]                 [Run3ModelGen.modelgen]
2024-08-28 19:59:33 [info     ] 		muR: [4000]                  [Run3ModelGen.modelgen]
2024-08-28 19:59:33 [info     ] 		mtR: [4000]                  [Run3ModelGen.modelgen]
2024-08-28 19:59:33 [info     ] 		mdR: [4000]                  [Run3ModelGen.modelgen]
2024-08-28 19:59:33 [info     ] 		mbR: [4000]                  [Run3ModelGen.modelgen]
2024-08-28 19:59:33 [info     ] 	steps =                       [Run3ModelGen.modelgen]
2024-08-28 19:59:33 [info     ] 		{'name': 'prep_input', 'output_dir': 'input', 'prefix': 'IN'} [Run3ModelGen.modelgen]
2024-08-28 19:59:33 [info     ] 		{'name': 'SPheno', 'input_dir': 'input', 'output_dir': 'SPheno', 'log_dir': 'SPheno_log', 'prefix': 'SP'} [Run3ModelGen.modelgen]
2024-08-28 19:59:33 [info     ] 		{'name': 'softsusy', 'input_dir': 'input', 'output_dir': 'softsusy', 'prefix': 'SS'} [Run3ModelGen.modelgen]
2024-08-28 19:59:33 [info     ] 		{'name': 'micromegas', 'input_dir': 'SPheno', 'output_dir': 'micromegas', 'prefix': 'MO'} [Run3ModelGen.modelgen]
2024-08-28 19:59:33 [info     ] 		{'name': 'superiso', 'input_dir': 'SPheno', 'output_dir': 'superiso', 'prefix': 'SI'} [Run3ModelGen.modelgen]
2024-08-28 19:59:33 [info     ] 		{'name': 'gm2calc', 'input_dir': 'SPheno', 'output_dir': 'gm2calc', 'prefix': 'GM2'} [Run3ModelGen.modelgen]
2024-08-28 19:59:33 [info     ] 		{'name': 'evade', 'input_dir': 'SPheno', 'output_dir': 'evade', 'prefix': 'EV'} [Run3ModelGen.modelgen]
2024-08-28 19:59:33 [info     ] Starting Model Generation...   [Run3ModelGen.modelgen]
2024-08-28 19:59:33 [info     ] Setting up output directories for scan [Run3ModelGen.modelgen]
2024-08-28 19:59:33 [info     ] Generating Model: 0.slha (1/1) [Run3ModelGen.modelgen]
2024-08-28 19:59:33 [info     ] 	running step: prep_input      [Run3ModelGen.modelgen]
2024-08-28 19:59:33 [info     ] 	running step: SPheno          [Run3ModelGen.modelgen]
2024-08-28 19:59:34 [info     ] 	running step: softsusy        [Run3ModelGen.modelgen]
2024-08-28 19:59:34 [info     ] 	running step: micromegas      [Run3ModelGen.modelgen]
2024-08-28 19:59:35 [info     ] 	running step: superiso        [Run3ModelGen.modelgen]
2024-08-28 19:59:35 [info     ] 	running step: gm2calc         [Run3ModelGen.modelgen]
2024-08-28 19:59:35 [info     ] 	running step: evade           [Run3ModelGen.modelgen]
2024-08-28 19:59:35 [info     ] Finished generating models! Stats for successful generations (successful/attempted): [Run3ModelGen.modelgen]
2024-08-28 19:59:35 [info     ] 	prep_input: 1/1.              [Run3ModelGen.modelgen]
2024-08-28 19:59:35 [info     ] 	SPheno: 1/1.                  [Run3ModelGen.modelgen]
2024-08-28 19:59:35 [info     ] 	softsusy: 1/1.                [Run3ModelGen.modelgen]
2024-08-28 19:59:35 [info     ] 	micromegas: 1/1.              [Run3ModelGen.modelgen]
2024-08-28 19:59:35 [info     ] 	superiso: 1/1.                [Run3ModelGen.modelgen]
2024-08-28 19:59:35 [info     ] 	gm2calc: 1/1.                 [Run3ModelGen.modelgen]
2024-08-28 19:59:35 [info     ] 	evade: 0/1.                   [Run3ModelGen.modelgen]
2024-08-28 19:59:35 [info     ] Will make NTuple for: /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/scans/scan_13, with 1 models [Run3ModelGen.ntupling]
2024-08-28 19:59:35 [info     ] 	Filling values for model: 0   [Run3ModelGen.ntupling]
2024-08-28 19:59:35 [info     ] Saving NTuple in /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/scans/scan_13/ntuple.0.0.root ... [Run3ModelGen.ntupling]
Completed iteration 13
Starting iteration 14
Initial Training points:  tensor([0.8528, 0.5812, 0.1617, 0.0046], device='cuda:0') torch.Size([4])
Training points after adding:  tensor([0.0000, 0.0140, 0.4324, 0.1617, 0.1091, 0.0046, 0.2132, 0.0561, 0.3373,
        0.2593, 0.4995, 0.2723, 0.5175, 0.2392, 0.5812, 0.0210, 0.8528],
       device='cuda:0') torch.Size([17])
These training_points are used in the GP tensor([0.0000, 0.0140, 0.4324, 0.1617, 0.1091, 0.0046, 0.2132, 0.0561, 0.3373,
        0.2593, 0.4995, 0.2723, 0.5175, 0.2392, 0.5812, 0.0210, 0.8528],
       device='cuda:0')
Point: tensor([0.0000, 0.0010, 0.0020, 0.0030, 0.0040, 0.0050, 0.0060, 0.0070, 0.0080,
        0.0090, 0.0100, 0.0110, 0.0120, 0.0130, 0.0140, 0.0150, 0.0160, 0.0170,
        0.0180, 0.0190, 0.0200, 0.0210, 0.0220, 0.0230, 0.0240, 0.0250, 0.0260,
        0.0270, 0.0280, 0.0290, 0.0300, 0.0310, 0.0320, 0.0330, 0.0340, 0.0350,
        0.0360, 0.0370, 0.0380, 0.0390, 0.0400, 0.0410, 0.0420, 0.0430, 0.0440,
        0.0450, 0.0460, 0.0470, 0.0480, 0.0490, 0.0501, 0.0511, 0.0521, 0.0531,
        0.0541, 0.0551, 0.0561, 0.0571, 0.0581, 0.0591, 0.0601, 0.0611, 0.0621,
        0.0631, 0.0641, 0.0651, 0.0661, 0.0671, 0.0681, 0.0691, 0.0701, 0.0711,
        0.0721, 0.0731, 0.0741, 0.0751, 0.0761, 0.0771, 0.0781, 0.0791, 0.0801,
        0.0811, 0.0821, 0.0831, 0.0841, 0.0851, 0.0861, 0.0871, 0.0881, 0.0891,
        0.0901, 0.0911, 0.0921, 0.0931, 0.0941, 0.0951, 0.0961, 0.0971, 0.0981,
        0.0991, 0.1001, 0.1011, 0.1021, 0.1031, 0.1041, 0.1051, 0.1061, 0.1071,
        0.1081, 0.1091, 0.1101, 0.1111, 0.1121, 0.1131, 0.1141, 0.1151, 0.1161,
        0.1171, 0.1181, 0.1191, 0.1201, 0.1211, 0.1221, 0.1231, 0.1241, 0.1251,
        0.1261, 0.1271, 0.1281, 0.1291, 0.1301, 0.1311, 0.1321, 0.1331, 0.1341,
        0.1351, 0.1361, 0.1371, 0.1381, 0.1391, 0.1401, 0.1411, 0.1421, 0.1431,
        0.1441, 0.1451, 0.1461, 0.1471, 0.1481, 0.1491, 0.1502, 0.1512, 0.1522,
        0.1532, 0.1542, 0.1552, 0.1562, 0.1572, 0.1582, 0.1592, 0.1602, 0.1612,
        0.1622, 0.1632, 0.1642, 0.1652, 0.1662, 0.1672, 0.1682, 0.1692, 0.1702,
        0.1712, 0.1722, 0.1732, 0.1742, 0.1752, 0.1762, 0.1772, 0.1782, 0.1792,
        0.1802, 0.1812, 0.1822, 0.1832, 0.1842, 0.1852, 0.1862, 0.1872, 0.1882,
        0.1892, 0.1902, 0.1912, 0.1922, 0.1932, 0.1942, 0.1952, 0.1962, 0.1972,
        0.1982, 0.1992, 0.2002, 0.2012, 0.2022, 0.2032, 0.2042, 0.2052, 0.2062,
        0.2072, 0.2082, 0.2092, 0.2102, 0.2112, 0.2122, 0.2132, 0.2142, 0.2152,
        0.2162, 0.2172, 0.2182, 0.2192, 0.2202, 0.2212, 0.2222, 0.2232, 0.2242,
        0.2252, 0.2262, 0.2272, 0.2282, 0.2292, 0.2302, 0.2312, 0.2322, 0.2332,
        0.2342, 0.2352, 0.2362, 0.2372, 0.2382, 0.2392, 0.2402, 0.2412, 0.2422,
        0.2432, 0.2442, 0.2452, 0.2462, 0.2472, 0.2482, 0.2492, 0.2503, 0.2513,
        0.2523, 0.2533, 0.2543, 0.2553, 0.2563, 0.2573, 0.2583, 0.2593, 0.2603,
        0.2613, 0.2623, 0.2633, 0.2643, 0.2653, 0.2663, 0.2673, 0.2683, 0.2693,
        0.2703, 0.2713, 0.2723, 0.2733, 0.2743, 0.2753, 0.2763, 0.2773, 0.2783,
        0.2793, 0.2803, 0.2813, 0.2823, 0.2833, 0.2843, 0.2853, 0.2863, 0.2873,
        0.2883, 0.2893, 0.2903, 0.2913, 0.2923, 0.2933, 0.2943, 0.2953, 0.2963,
        0.2973, 0.2983, 0.2993, 0.3003, 0.3013, 0.3023, 0.3033, 0.3043, 0.3053,
        0.3063, 0.3073, 0.3083, 0.3093, 0.3103, 0.3113, 0.3123, 0.3133, 0.3143,
        0.3153, 0.3163, 0.3173, 0.3183, 0.3193, 0.3203, 0.3213, 0.3223, 0.3233,
        0.3243, 0.3253, 0.3263, 0.3273, 0.3283, 0.3293, 0.3303, 0.3313, 0.3323,
        0.3333, 0.3343, 0.3353, 0.3363, 0.3373, 0.3383, 0.3393, 0.3403, 0.3413,
        0.3423, 0.3433, 0.3443, 0.3453, 0.3463, 0.3473, 0.3483, 0.3493, 0.3504,
        0.3514, 0.3524, 0.3534, 0.3544, 0.3554, 0.3564, 0.3574, 0.3584, 0.3594,
        0.3604, 0.3614, 0.3624, 0.3634, 0.3644, 0.3654, 0.3664, 0.3674, 0.3684,
        0.3694, 0.3704, 0.3714, 0.3724, 0.3734, 0.3744, 0.3754, 0.3764, 0.3774,
        0.3784, 0.3794, 0.3804, 0.3814, 0.3824, 0.3834, 0.3844, 0.3854, 0.3864,
        0.3874, 0.3884, 0.3894, 0.3904, 0.3914, 0.3924, 0.3934, 0.3944, 0.3954,
        0.3964, 0.3974, 0.3984, 0.3994, 0.4004, 0.4014, 0.4024, 0.4034, 0.4044,
        0.4054, 0.4064, 0.4074, 0.4084, 0.4094, 0.4104, 0.4114, 0.4124, 0.4134,
        0.4144, 0.4154, 0.4164, 0.4174, 0.4184, 0.4194, 0.4204, 0.4214, 0.4224,
        0.4234, 0.4244, 0.4254, 0.4264, 0.4274, 0.4284, 0.4294, 0.4304, 0.4314,
        0.4324, 0.4334, 0.4344, 0.4354, 0.4364, 0.4374, 0.4384, 0.4394, 0.4404,
        0.4414, 0.4424, 0.4434, 0.4444, 0.4454, 0.4464, 0.4474, 0.4484, 0.4494,
        0.4505, 0.4515, 0.4525, 0.4535, 0.4545, 0.4555, 0.4565, 0.4575, 0.4585,
        0.4595, 0.4605, 0.4615, 0.4625, 0.4635, 0.4645, 0.4655, 0.4665, 0.4675,
        0.4685, 0.4695, 0.4705, 0.4715, 0.4725, 0.4735, 0.4745, 0.4755, 0.4765,
        0.4775, 0.4785, 0.4795, 0.4805, 0.4815, 0.4825, 0.4835, 0.4845, 0.4855,
        0.4865, 0.4875, 0.4885, 0.4895, 0.4905, 0.4915, 0.4925, 0.4935, 0.4945,
        0.4955, 0.4965, 0.4975, 0.4985, 0.4995, 0.5005, 0.5015, 0.5025, 0.5035,
        0.5045, 0.5055, 0.5065, 0.5075, 0.5085, 0.5095, 0.5105, 0.5115, 0.5125,
        0.5135, 0.5145, 0.5155, 0.5165, 0.5175, 0.5185, 0.5195, 0.5205, 0.5215,
        0.5225, 0.5235, 0.5245, 0.5255, 0.5265, 0.5275, 0.5285, 0.5295, 0.5305,
        0.5315, 0.5325, 0.5335, 0.5345, 0.5355, 0.5365, 0.5375, 0.5385, 0.5395,
        0.5405, 0.5415, 0.5425, 0.5435, 0.5445, 0.5455, 0.5465, 0.5475, 0.5485,
        0.5495, 0.5506, 0.5516, 0.5526, 0.5536, 0.5546, 0.5556, 0.5566, 0.5576,
        0.5586, 0.5596, 0.5606, 0.5616, 0.5626, 0.5636, 0.5646, 0.5656, 0.5666,
        0.5676, 0.5686, 0.5696, 0.5706, 0.5716, 0.5726, 0.5736, 0.5746, 0.5756,
        0.5766, 0.5776, 0.5786, 0.5796, 0.5806, 0.5816, 0.5826, 0.5836, 0.5846,
        0.5856, 0.5866, 0.5876, 0.5886, 0.5896, 0.5906, 0.5916, 0.5926, 0.5936,
        0.5946, 0.5956, 0.5966, 0.5976, 0.5986, 0.5996, 0.6006, 0.6016, 0.6026,
        0.6036, 0.6046, 0.6056, 0.6066, 0.6076, 0.6086, 0.6096, 0.6106, 0.6116,
        0.6126, 0.6136, 0.6146, 0.6156, 0.6166, 0.6176, 0.6186, 0.6196, 0.6206,
        0.6216, 0.6226, 0.6236, 0.6246, 0.6256, 0.6266, 0.6276, 0.6286, 0.6296,
        0.6306, 0.6316, 0.6326, 0.6336, 0.6346, 0.6356, 0.6366, 0.6376, 0.6386,
        0.6396, 0.6406, 0.6416, 0.6426, 0.6436, 0.6446, 0.6456, 0.6466, 0.6476,
        0.6486, 0.6496, 0.6507, 0.6517, 0.6527, 0.6537, 0.6547, 0.6557, 0.6567,
        0.6577, 0.6587, 0.6597, 0.6607, 0.6617, 0.6627, 0.6637, 0.6647, 0.6657,
        0.6667, 0.6677, 0.6687, 0.6697, 0.6707, 0.6717, 0.6727, 0.6737, 0.6747,
        0.6757, 0.6767, 0.6777, 0.6787, 0.6797, 0.6807, 0.6817, 0.6827, 0.6837,
        0.6847, 0.6857, 0.6867, 0.6877, 0.6887, 0.6897, 0.6907, 0.6917, 0.6927,
        0.6937, 0.6947, 0.6957, 0.6967, 0.6977, 0.6987, 0.6997, 0.7007, 0.7017,
        0.7027, 0.7037, 0.7047, 0.7057, 0.7067, 0.7077, 0.7087, 0.7097, 0.7107,
        0.7117, 0.7127, 0.7137, 0.7147, 0.7157, 0.7167, 0.7177, 0.7187, 0.7197,
        0.7207, 0.7217, 0.7227, 0.7237, 0.7247, 0.7257, 0.7267, 0.7277, 0.7287,
        0.7297, 0.7307, 0.7317, 0.7327, 0.7337, 0.7347, 0.7357, 0.7367, 0.7377,
        0.7387, 0.7397, 0.7407, 0.7417, 0.7427, 0.7437, 0.7447, 0.7457, 0.7467,
        0.7477, 0.7487, 0.7497, 0.7508, 0.7518, 0.7528, 0.7538, 0.7548, 0.7558,
        0.7568, 0.7578, 0.7588, 0.7598, 0.7608, 0.7618, 0.7628, 0.7638, 0.7648,
        0.7658, 0.7668, 0.7678, 0.7688, 0.7698, 0.7708, 0.7718, 0.7728, 0.7738,
        0.7748, 0.7758, 0.7768, 0.7778, 0.7788, 0.7798, 0.7808, 0.7818, 0.7828,
        0.7838, 0.7848, 0.7858, 0.7868, 0.7878, 0.7888, 0.7898, 0.7908, 0.7918,
        0.7928, 0.7938, 0.7948, 0.7958, 0.7968, 0.7978, 0.7988, 0.7998, 0.8008,
        0.8018, 0.8028, 0.8038, 0.8048, 0.8058, 0.8068, 0.8078, 0.8088, 0.8098,
        0.8108, 0.8118, 0.8128, 0.8138, 0.8148, 0.8158, 0.8168, 0.8178, 0.8188,
        0.8198, 0.8208, 0.8218, 0.8228, 0.8238, 0.8248, 0.8258, 0.8268, 0.8278,
        0.8288, 0.8298, 0.8308, 0.8318, 0.8328, 0.8338, 0.8348, 0.8358, 0.8368,
        0.8378, 0.8388, 0.8398, 0.8408, 0.8418, 0.8428, 0.8438, 0.8448, 0.8458,
        0.8468, 0.8478, 0.8488, 0.8498, 0.8509, 0.8519, 0.8529, 0.8539, 0.8549,
        0.8559, 0.8569, 0.8579, 0.8589, 0.8599, 0.8609, 0.8619, 0.8629, 0.8639,
        0.8649, 0.8659, 0.8669, 0.8679, 0.8689, 0.8699, 0.8709, 0.8719, 0.8729,
        0.8739, 0.8749, 0.8759, 0.8769, 0.8779, 0.8789, 0.8799, 0.8809, 0.8819,
        0.8829, 0.8839, 0.8849, 0.8859, 0.8869, 0.8879, 0.8889, 0.8899, 0.8909,
        0.8919, 0.8929, 0.8939, 0.8949, 0.8959, 0.8969, 0.8979, 0.8989, 0.8999,
        0.9009, 0.9019, 0.9029, 0.9039, 0.9049, 0.9059, 0.9069, 0.9079, 0.9089,
        0.9099, 0.9109, 0.9119, 0.9129, 0.9139, 0.9149, 0.9159, 0.9169, 0.9179,
        0.9189, 0.9199, 0.9209, 0.9219, 0.9229, 0.9239, 0.9249, 0.9259, 0.9269,
        0.9279, 0.9289, 0.9299, 0.9309, 0.9319, 0.9329, 0.9339, 0.9349, 0.9359,
        0.9369, 0.9379, 0.9389, 0.9399, 0.9409, 0.9419, 0.9429, 0.9439, 0.9449,
        0.9459, 0.9469, 0.9479, 0.9489, 0.9499, 0.9510, 0.9520, 0.9530, 0.9540,
        0.9550, 0.9560, 0.9570, 0.9580, 0.9590, 0.9600, 0.9610, 0.9620, 0.9630,
        0.9640, 0.9650, 0.9660, 0.9670, 0.9680, 0.9690, 0.9700, 0.9710, 0.9720,
        0.9730, 0.9740, 0.9750, 0.9760, 0.9770, 0.9780, 0.9790, 0.9800, 0.9810,
        0.9820, 0.9830, 0.9840, 0.9850, 0.9860, 0.9870, 0.9880, 0.9890, 0.9900,
        0.9910, 0.9920, 0.9930, 0.9940, 0.9950, 0.9960, 0.9970, 0.9980, 0.9990,
        1.0000], device='cuda:0') - Variance: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0') 
Smoothed_batch_entropy:  tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan], device='cuda:0')
Selected points (indices): {0}
Selected new x values (normalized): tensor([0.], device='cuda:0')
Corresponding new x values (unnormalized): tensor([-2000.], device='cuda:0')
Plot saved to /raven/u/dvoss/al_pmssmwithgp/model/plots/Iter14/gp_plot.png
YAML file has been created: /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/data/new_config.yaml
Note: detected 144 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
Note: NumExpr detected 144 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
NumExpr defaulting to 8 threads.

  ___              ____ __  __          _       _   ___            
 | _ \ _  _  _ _  |__ /|  \/  | ___  __| | ___ | | / __| ___  _ _  
 |   /| || || ' \  |_ \| |\/| |/ _ \/ _` |/ -_)| || (_ |/ -_)| ' \ 
 |_|_\ \_,_||_||_||___/|_|  |_|\___/\__,_|\___||_| \___|\___||_||_|
                                                                   

2024-08-28 20:00:20 [info     ] Initialised ModelGenerator with: [Run3ModelGen.modelgen]
2024-08-28 20:00:20 [info     ] 	config_file = /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/data/new_config.yaml [Run3ModelGen.modelgen]
2024-08-28 20:00:20 [info     ] 	scan_dir = /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/scans/scan_14 [Run3ModelGen.modelgen]
2024-08-28 20:00:20 [info     ] 	rawfilen = /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/data/raw.slha [Run3ModelGen.modelgen]
2024-08-28 20:00:20 [info     ] 	seed = 123                    [Run3ModelGen.modelgen]
2024-08-28 20:00:20 [info     ] 	points = {}                   [Run3ModelGen.modelgen]
2024-08-28 20:00:20 [info     ] 	prior = fixed                 [Run3ModelGen.modelgen]
2024-08-28 20:00:20 [info     ] 	num_models = 1                [Run3ModelGen.modelgen]
2024-08-28 20:00:20 [info     ] 	isGMSB = False                [Run3ModelGen.modelgen]
2024-08-28 20:00:20 [info     ] 	parameters =                  [Run3ModelGen.modelgen]
2024-08-28 20:00:20 [info     ] 		tanb: [60]                   [Run3ModelGen.modelgen]
2024-08-28 20:00:20 [info     ] 		M_1: [-2000.0]               [Run3ModelGen.modelgen]
2024-08-28 20:00:20 [info     ] 		M_2: [2000]                  [Run3ModelGen.modelgen]
2024-08-28 20:00:20 [info     ] 		M_3: [4000]                  [Run3ModelGen.modelgen]
2024-08-28 20:00:20 [info     ] 		AT: [4000]                   [Run3ModelGen.modelgen]
2024-08-28 20:00:20 [info     ] 		Ab: [2000]                   [Run3ModelGen.modelgen]
2024-08-28 20:00:20 [info     ] 		Atau: [2000]                 [Run3ModelGen.modelgen]
2024-08-28 20:00:20 [info     ] 		mu: [2000]                   [Run3ModelGen.modelgen]
2024-08-28 20:00:20 [info     ] 		mA: [2000]                   [Run3ModelGen.modelgen]
2024-08-28 20:00:20 [info     ] 		meL: [2000]                  [Run3ModelGen.modelgen]
2024-08-28 20:00:20 [info     ] 		mtauL: [2000]                [Run3ModelGen.modelgen]
2024-08-28 20:00:20 [info     ] 		meR: [2000]                  [Run3ModelGen.modelgen]
2024-08-28 20:00:20 [info     ] 		mtauR: [2000]                [Run3ModelGen.modelgen]
2024-08-28 20:00:20 [info     ] 		mqL1: [4000]                 [Run3ModelGen.modelgen]
2024-08-28 20:00:20 [info     ] 		mqL3: [4000]                 [Run3ModelGen.modelgen]
2024-08-28 20:00:20 [info     ] 		muR: [4000]                  [Run3ModelGen.modelgen]
2024-08-28 20:00:20 [info     ] 		mtR: [4000]                  [Run3ModelGen.modelgen]
2024-08-28 20:00:20 [info     ] 		mdR: [4000]                  [Run3ModelGen.modelgen]
2024-08-28 20:00:20 [info     ] 		mbR: [4000]                  [Run3ModelGen.modelgen]
2024-08-28 20:00:20 [info     ] 	steps =                       [Run3ModelGen.modelgen]
2024-08-28 20:00:20 [info     ] 		{'name': 'prep_input', 'output_dir': 'input', 'prefix': 'IN'} [Run3ModelGen.modelgen]
2024-08-28 20:00:20 [info     ] 		{'name': 'SPheno', 'input_dir': 'input', 'output_dir': 'SPheno', 'log_dir': 'SPheno_log', 'prefix': 'SP'} [Run3ModelGen.modelgen]
2024-08-28 20:00:20 [info     ] 		{'name': 'softsusy', 'input_dir': 'input', 'output_dir': 'softsusy', 'prefix': 'SS'} [Run3ModelGen.modelgen]
2024-08-28 20:00:20 [info     ] 		{'name': 'micromegas', 'input_dir': 'SPheno', 'output_dir': 'micromegas', 'prefix': 'MO'} [Run3ModelGen.modelgen]
2024-08-28 20:00:20 [info     ] 		{'name': 'superiso', 'input_dir': 'SPheno', 'output_dir': 'superiso', 'prefix': 'SI'} [Run3ModelGen.modelgen]
2024-08-28 20:00:20 [info     ] 		{'name': 'gm2calc', 'input_dir': 'SPheno', 'output_dir': 'gm2calc', 'prefix': 'GM2'} [Run3ModelGen.modelgen]
2024-08-28 20:00:20 [info     ] 		{'name': 'evade', 'input_dir': 'SPheno', 'output_dir': 'evade', 'prefix': 'EV'} [Run3ModelGen.modelgen]
2024-08-28 20:00:20 [info     ] Starting Model Generation...   [Run3ModelGen.modelgen]
2024-08-28 20:00:20 [info     ] Setting up output directories for scan [Run3ModelGen.modelgen]
2024-08-28 20:00:20 [info     ] Generating Model: 0.slha (1/1) [Run3ModelGen.modelgen]
2024-08-28 20:00:20 [info     ] 	running step: prep_input      [Run3ModelGen.modelgen]
2024-08-28 20:00:20 [info     ] 	running step: SPheno          [Run3ModelGen.modelgen]
2024-08-28 20:00:20 [info     ] 	running step: softsusy        [Run3ModelGen.modelgen]
2024-08-28 20:00:20 [info     ] 	running step: micromegas      [Run3ModelGen.modelgen]
2024-08-28 20:00:21 [info     ] 	running step: superiso        [Run3ModelGen.modelgen]
2024-08-28 20:00:21 [info     ] 	running step: gm2calc         [Run3ModelGen.modelgen]
2024-08-28 20:00:21 [info     ] 	running step: evade           [Run3ModelGen.modelgen]
2024-08-28 20:00:21 [info     ] Finished generating models! Stats for successful generations (successful/attempted): [Run3ModelGen.modelgen]
2024-08-28 20:00:21 [info     ] 	prep_input: 1/1.              [Run3ModelGen.modelgen]
2024-08-28 20:00:21 [info     ] 	SPheno: 1/1.                  [Run3ModelGen.modelgen]
2024-08-28 20:00:21 [info     ] 	softsusy: 1/1.                [Run3ModelGen.modelgen]
2024-08-28 20:00:21 [info     ] 	micromegas: 1/1.              [Run3ModelGen.modelgen]
2024-08-28 20:00:21 [info     ] 	superiso: 1/1.                [Run3ModelGen.modelgen]
2024-08-28 20:00:21 [info     ] 	gm2calc: 1/1.                 [Run3ModelGen.modelgen]
2024-08-28 20:00:21 [info     ] 	evade: 0/1.                   [Run3ModelGen.modelgen]
2024-08-28 20:00:21 [info     ] Will make NTuple for: /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/scans/scan_14, with 1 models [Run3ModelGen.ntupling]
2024-08-28 20:00:21 [info     ] 	Filling values for model: 0   [Run3ModelGen.ntupling]
2024-08-28 20:00:21 [info     ] Saving NTuple in /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/scans/scan_14/ntuple.0.0.root ... [Run3ModelGen.ntupling]
Completed iteration 14
Starting iteration 15
Initial Training points:  tensor([0.8528, 0.5812, 0.1617, 0.0046], device='cuda:0') torch.Size([4])
Training points after adding:  tensor([0.0000, 0.0140, 0.4324, 0.1617, 0.1091, 0.0046, 0.2132, 0.0561, 0.3373,
        0.2593, 0.4995, 0.2723, 0.5175, 0.2392, 0.5812, 0.0210, 0.8528],
       device='cuda:0') torch.Size([17])
These training_points are used in the GP tensor([0.0000, 0.0140, 0.4324, 0.1617, 0.1091, 0.0046, 0.2132, 0.0561, 0.3373,
        0.2593, 0.4995, 0.2723, 0.5175, 0.2392, 0.5812, 0.0210, 0.8528],
       device='cuda:0')
Point: tensor([0.0000, 0.0010, 0.0020, 0.0030, 0.0040, 0.0050, 0.0060, 0.0070, 0.0080,
        0.0090, 0.0100, 0.0110, 0.0120, 0.0130, 0.0140, 0.0150, 0.0160, 0.0170,
        0.0180, 0.0190, 0.0200, 0.0210, 0.0220, 0.0230, 0.0240, 0.0250, 0.0260,
        0.0270, 0.0280, 0.0290, 0.0300, 0.0310, 0.0320, 0.0330, 0.0340, 0.0350,
        0.0360, 0.0370, 0.0380, 0.0390, 0.0400, 0.0410, 0.0420, 0.0430, 0.0440,
        0.0450, 0.0460, 0.0470, 0.0480, 0.0490, 0.0501, 0.0511, 0.0521, 0.0531,
        0.0541, 0.0551, 0.0561, 0.0571, 0.0581, 0.0591, 0.0601, 0.0611, 0.0621,
        0.0631, 0.0641, 0.0651, 0.0661, 0.0671, 0.0681, 0.0691, 0.0701, 0.0711,
        0.0721, 0.0731, 0.0741, 0.0751, 0.0761, 0.0771, 0.0781, 0.0791, 0.0801,
        0.0811, 0.0821, 0.0831, 0.0841, 0.0851, 0.0861, 0.0871, 0.0881, 0.0891,
        0.0901, 0.0911, 0.0921, 0.0931, 0.0941, 0.0951, 0.0961, 0.0971, 0.0981,
        0.0991, 0.1001, 0.1011, 0.1021, 0.1031, 0.1041, 0.1051, 0.1061, 0.1071,
        0.1081, 0.1091, 0.1101, 0.1111, 0.1121, 0.1131, 0.1141, 0.1151, 0.1161,
        0.1171, 0.1181, 0.1191, 0.1201, 0.1211, 0.1221, 0.1231, 0.1241, 0.1251,
        0.1261, 0.1271, 0.1281, 0.1291, 0.1301, 0.1311, 0.1321, 0.1331, 0.1341,
        0.1351, 0.1361, 0.1371, 0.1381, 0.1391, 0.1401, 0.1411, 0.1421, 0.1431,
        0.1441, 0.1451, 0.1461, 0.1471, 0.1481, 0.1491, 0.1502, 0.1512, 0.1522,
        0.1532, 0.1542, 0.1552, 0.1562, 0.1572, 0.1582, 0.1592, 0.1602, 0.1612,
        0.1622, 0.1632, 0.1642, 0.1652, 0.1662, 0.1672, 0.1682, 0.1692, 0.1702,
        0.1712, 0.1722, 0.1732, 0.1742, 0.1752, 0.1762, 0.1772, 0.1782, 0.1792,
        0.1802, 0.1812, 0.1822, 0.1832, 0.1842, 0.1852, 0.1862, 0.1872, 0.1882,
        0.1892, 0.1902, 0.1912, 0.1922, 0.1932, 0.1942, 0.1952, 0.1962, 0.1972,
        0.1982, 0.1992, 0.2002, 0.2012, 0.2022, 0.2032, 0.2042, 0.2052, 0.2062,
        0.2072, 0.2082, 0.2092, 0.2102, 0.2112, 0.2122, 0.2132, 0.2142, 0.2152,
        0.2162, 0.2172, 0.2182, 0.2192, 0.2202, 0.2212, 0.2222, 0.2232, 0.2242,
        0.2252, 0.2262, 0.2272, 0.2282, 0.2292, 0.2302, 0.2312, 0.2322, 0.2332,
        0.2342, 0.2352, 0.2362, 0.2372, 0.2382, 0.2392, 0.2402, 0.2412, 0.2422,
        0.2432, 0.2442, 0.2452, 0.2462, 0.2472, 0.2482, 0.2492, 0.2503, 0.2513,
        0.2523, 0.2533, 0.2543, 0.2553, 0.2563, 0.2573, 0.2583, 0.2593, 0.2603,
        0.2613, 0.2623, 0.2633, 0.2643, 0.2653, 0.2663, 0.2673, 0.2683, 0.2693,
        0.2703, 0.2713, 0.2723, 0.2733, 0.2743, 0.2753, 0.2763, 0.2773, 0.2783,
        0.2793, 0.2803, 0.2813, 0.2823, 0.2833, 0.2843, 0.2853, 0.2863, 0.2873,
        0.2883, 0.2893, 0.2903, 0.2913, 0.2923, 0.2933, 0.2943, 0.2953, 0.2963,
        0.2973, 0.2983, 0.2993, 0.3003, 0.3013, 0.3023, 0.3033, 0.3043, 0.3053,
        0.3063, 0.3073, 0.3083, 0.3093, 0.3103, 0.3113, 0.3123, 0.3133, 0.3143,
        0.3153, 0.3163, 0.3173, 0.3183, 0.3193, 0.3203, 0.3213, 0.3223, 0.3233,
        0.3243, 0.3253, 0.3263, 0.3273, 0.3283, 0.3293, 0.3303, 0.3313, 0.3323,
        0.3333, 0.3343, 0.3353, 0.3363, 0.3373, 0.3383, 0.3393, 0.3403, 0.3413,
        0.3423, 0.3433, 0.3443, 0.3453, 0.3463, 0.3473, 0.3483, 0.3493, 0.3504,
        0.3514, 0.3524, 0.3534, 0.3544, 0.3554, 0.3564, 0.3574, 0.3584, 0.3594,
        0.3604, 0.3614, 0.3624, 0.3634, 0.3644, 0.3654, 0.3664, 0.3674, 0.3684,
        0.3694, 0.3704, 0.3714, 0.3724, 0.3734, 0.3744, 0.3754, 0.3764, 0.3774,
        0.3784, 0.3794, 0.3804, 0.3814, 0.3824, 0.3834, 0.3844, 0.3854, 0.3864,
        0.3874, 0.3884, 0.3894, 0.3904, 0.3914, 0.3924, 0.3934, 0.3944, 0.3954,
        0.3964, 0.3974, 0.3984, 0.3994, 0.4004, 0.4014, 0.4024, 0.4034, 0.4044,
        0.4054, 0.4064, 0.4074, 0.4084, 0.4094, 0.4104, 0.4114, 0.4124, 0.4134,
        0.4144, 0.4154, 0.4164, 0.4174, 0.4184, 0.4194, 0.4204, 0.4214, 0.4224,
        0.4234, 0.4244, 0.4254, 0.4264, 0.4274, 0.4284, 0.4294, 0.4304, 0.4314,
        0.4324, 0.4334, 0.4344, 0.4354, 0.4364, 0.4374, 0.4384, 0.4394, 0.4404,
        0.4414, 0.4424, 0.4434, 0.4444, 0.4454, 0.4464, 0.4474, 0.4484, 0.4494,
        0.4505, 0.4515, 0.4525, 0.4535, 0.4545, 0.4555, 0.4565, 0.4575, 0.4585,
        0.4595, 0.4605, 0.4615, 0.4625, 0.4635, 0.4645, 0.4655, 0.4665, 0.4675,
        0.4685, 0.4695, 0.4705, 0.4715, 0.4725, 0.4735, 0.4745, 0.4755, 0.4765,
        0.4775, 0.4785, 0.4795, 0.4805, 0.4815, 0.4825, 0.4835, 0.4845, 0.4855,
        0.4865, 0.4875, 0.4885, 0.4895, 0.4905, 0.4915, 0.4925, 0.4935, 0.4945,
        0.4955, 0.4965, 0.4975, 0.4985, 0.4995, 0.5005, 0.5015, 0.5025, 0.5035,
        0.5045, 0.5055, 0.5065, 0.5075, 0.5085, 0.5095, 0.5105, 0.5115, 0.5125,
        0.5135, 0.5145, 0.5155, 0.5165, 0.5175, 0.5185, 0.5195, 0.5205, 0.5215,
        0.5225, 0.5235, 0.5245, 0.5255, 0.5265, 0.5275, 0.5285, 0.5295, 0.5305,
        0.5315, 0.5325, 0.5335, 0.5345, 0.5355, 0.5365, 0.5375, 0.5385, 0.5395,
        0.5405, 0.5415, 0.5425, 0.5435, 0.5445, 0.5455, 0.5465, 0.5475, 0.5485,
        0.5495, 0.5506, 0.5516, 0.5526, 0.5536, 0.5546, 0.5556, 0.5566, 0.5576,
        0.5586, 0.5596, 0.5606, 0.5616, 0.5626, 0.5636, 0.5646, 0.5656, 0.5666,
        0.5676, 0.5686, 0.5696, 0.5706, 0.5716, 0.5726, 0.5736, 0.5746, 0.5756,
        0.5766, 0.5776, 0.5786, 0.5796, 0.5806, 0.5816, 0.5826, 0.5836, 0.5846,
        0.5856, 0.5866, 0.5876, 0.5886, 0.5896, 0.5906, 0.5916, 0.5926, 0.5936,
        0.5946, 0.5956, 0.5966, 0.5976, 0.5986, 0.5996, 0.6006, 0.6016, 0.6026,
        0.6036, 0.6046, 0.6056, 0.6066, 0.6076, 0.6086, 0.6096, 0.6106, 0.6116,
        0.6126, 0.6136, 0.6146, 0.6156, 0.6166, 0.6176, 0.6186, 0.6196, 0.6206,
        0.6216, 0.6226, 0.6236, 0.6246, 0.6256, 0.6266, 0.6276, 0.6286, 0.6296,
        0.6306, 0.6316, 0.6326, 0.6336, 0.6346, 0.6356, 0.6366, 0.6376, 0.6386,
        0.6396, 0.6406, 0.6416, 0.6426, 0.6436, 0.6446, 0.6456, 0.6466, 0.6476,
        0.6486, 0.6496, 0.6507, 0.6517, 0.6527, 0.6537, 0.6547, 0.6557, 0.6567,
        0.6577, 0.6587, 0.6597, 0.6607, 0.6617, 0.6627, 0.6637, 0.6647, 0.6657,
        0.6667, 0.6677, 0.6687, 0.6697, 0.6707, 0.6717, 0.6727, 0.6737, 0.6747,
        0.6757, 0.6767, 0.6777, 0.6787, 0.6797, 0.6807, 0.6817, 0.6827, 0.6837,
        0.6847, 0.6857, 0.6867, 0.6877, 0.6887, 0.6897, 0.6907, 0.6917, 0.6927,
        0.6937, 0.6947, 0.6957, 0.6967, 0.6977, 0.6987, 0.6997, 0.7007, 0.7017,
        0.7027, 0.7037, 0.7047, 0.7057, 0.7067, 0.7077, 0.7087, 0.7097, 0.7107,
        0.7117, 0.7127, 0.7137, 0.7147, 0.7157, 0.7167, 0.7177, 0.7187, 0.7197,
        0.7207, 0.7217, 0.7227, 0.7237, 0.7247, 0.7257, 0.7267, 0.7277, 0.7287,
        0.7297, 0.7307, 0.7317, 0.7327, 0.7337, 0.7347, 0.7357, 0.7367, 0.7377,
        0.7387, 0.7397, 0.7407, 0.7417, 0.7427, 0.7437, 0.7447, 0.7457, 0.7467,
        0.7477, 0.7487, 0.7497, 0.7508, 0.7518, 0.7528, 0.7538, 0.7548, 0.7558,
        0.7568, 0.7578, 0.7588, 0.7598, 0.7608, 0.7618, 0.7628, 0.7638, 0.7648,
        0.7658, 0.7668, 0.7678, 0.7688, 0.7698, 0.7708, 0.7718, 0.7728, 0.7738,
        0.7748, 0.7758, 0.7768, 0.7778, 0.7788, 0.7798, 0.7808, 0.7818, 0.7828,
        0.7838, 0.7848, 0.7858, 0.7868, 0.7878, 0.7888, 0.7898, 0.7908, 0.7918,
        0.7928, 0.7938, 0.7948, 0.7958, 0.7968, 0.7978, 0.7988, 0.7998, 0.8008,
        0.8018, 0.8028, 0.8038, 0.8048, 0.8058, 0.8068, 0.8078, 0.8088, 0.8098,
        0.8108, 0.8118, 0.8128, 0.8138, 0.8148, 0.8158, 0.8168, 0.8178, 0.8188,
        0.8198, 0.8208, 0.8218, 0.8228, 0.8238, 0.8248, 0.8258, 0.8268, 0.8278,
        0.8288, 0.8298, 0.8308, 0.8318, 0.8328, 0.8338, 0.8348, 0.8358, 0.8368,
        0.8378, 0.8388, 0.8398, 0.8408, 0.8418, 0.8428, 0.8438, 0.8448, 0.8458,
        0.8468, 0.8478, 0.8488, 0.8498, 0.8509, 0.8519, 0.8529, 0.8539, 0.8549,
        0.8559, 0.8569, 0.8579, 0.8589, 0.8599, 0.8609, 0.8619, 0.8629, 0.8639,
        0.8649, 0.8659, 0.8669, 0.8679, 0.8689, 0.8699, 0.8709, 0.8719, 0.8729,
        0.8739, 0.8749, 0.8759, 0.8769, 0.8779, 0.8789, 0.8799, 0.8809, 0.8819,
        0.8829, 0.8839, 0.8849, 0.8859, 0.8869, 0.8879, 0.8889, 0.8899, 0.8909,
        0.8919, 0.8929, 0.8939, 0.8949, 0.8959, 0.8969, 0.8979, 0.8989, 0.8999,
        0.9009, 0.9019, 0.9029, 0.9039, 0.9049, 0.9059, 0.9069, 0.9079, 0.9089,
        0.9099, 0.9109, 0.9119, 0.9129, 0.9139, 0.9149, 0.9159, 0.9169, 0.9179,
        0.9189, 0.9199, 0.9209, 0.9219, 0.9229, 0.9239, 0.9249, 0.9259, 0.9269,
        0.9279, 0.9289, 0.9299, 0.9309, 0.9319, 0.9329, 0.9339, 0.9349, 0.9359,
        0.9369, 0.9379, 0.9389, 0.9399, 0.9409, 0.9419, 0.9429, 0.9439, 0.9449,
        0.9459, 0.9469, 0.9479, 0.9489, 0.9499, 0.9510, 0.9520, 0.9530, 0.9540,
        0.9550, 0.9560, 0.9570, 0.9580, 0.9590, 0.9600, 0.9610, 0.9620, 0.9630,
        0.9640, 0.9650, 0.9660, 0.9670, 0.9680, 0.9690, 0.9700, 0.9710, 0.9720,
        0.9730, 0.9740, 0.9750, 0.9760, 0.9770, 0.9780, 0.9790, 0.9800, 0.9810,
        0.9820, 0.9830, 0.9840, 0.9850, 0.9860, 0.9870, 0.9880, 0.9890, 0.9900,
        0.9910, 0.9920, 0.9930, 0.9940, 0.9950, 0.9960, 0.9970, 0.9980, 0.9990,
        1.0000], device='cuda:0') - Variance: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0') 
Smoothed_batch_entropy:  tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan], device='cuda:0')
Selected points (indices): {0}
Selected new x values (normalized): tensor([0.], device='cuda:0')
Corresponding new x values (unnormalized): tensor([-2000.], device='cuda:0')
Plot saved to /raven/u/dvoss/al_pmssmwithgp/model/plots/Iter15/gp_plot.png
YAML file has been created: /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/data/new_config.yaml
Note: detected 144 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
Note: NumExpr detected 144 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
NumExpr defaulting to 8 threads.

  ___              ____ __  __          _       _   ___            
 | _ \ _  _  _ _  |__ /|  \/  | ___  __| | ___ | | / __| ___  _ _  
 |   /| || || ' \  |_ \| |\/| |/ _ \/ _` |/ -_)| || (_ |/ -_)| ' \ 
 |_|_\ \_,_||_||_||___/|_|  |_|\___/\__,_|\___||_| \___|\___||_||_|
                                                                   

2024-08-28 20:01:06 [info     ] Initialised ModelGenerator with: [Run3ModelGen.modelgen]
2024-08-28 20:01:06 [info     ] 	config_file = /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/data/new_config.yaml [Run3ModelGen.modelgen]
2024-08-28 20:01:06 [info     ] 	scan_dir = /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/scans/scan_15 [Run3ModelGen.modelgen]
2024-08-28 20:01:06 [info     ] 	rawfilen = /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/data/raw.slha [Run3ModelGen.modelgen]
2024-08-28 20:01:06 [info     ] 	seed = 123                    [Run3ModelGen.modelgen]
2024-08-28 20:01:06 [info     ] 	points = {}                   [Run3ModelGen.modelgen]
2024-08-28 20:01:06 [info     ] 	prior = fixed                 [Run3ModelGen.modelgen]
2024-08-28 20:01:06 [info     ] 	num_models = 1                [Run3ModelGen.modelgen]
2024-08-28 20:01:06 [info     ] 	isGMSB = False                [Run3ModelGen.modelgen]
2024-08-28 20:01:06 [info     ] 	parameters =                  [Run3ModelGen.modelgen]
2024-08-28 20:01:06 [info     ] 		tanb: [60]                   [Run3ModelGen.modelgen]
2024-08-28 20:01:06 [info     ] 		M_1: [-2000.0]               [Run3ModelGen.modelgen]
2024-08-28 20:01:06 [info     ] 		M_2: [2000]                  [Run3ModelGen.modelgen]
2024-08-28 20:01:06 [info     ] 		M_3: [4000]                  [Run3ModelGen.modelgen]
2024-08-28 20:01:06 [info     ] 		AT: [4000]                   [Run3ModelGen.modelgen]
2024-08-28 20:01:06 [info     ] 		Ab: [2000]                   [Run3ModelGen.modelgen]
2024-08-28 20:01:06 [info     ] 		Atau: [2000]                 [Run3ModelGen.modelgen]
2024-08-28 20:01:06 [info     ] 		mu: [2000]                   [Run3ModelGen.modelgen]
2024-08-28 20:01:06 [info     ] 		mA: [2000]                   [Run3ModelGen.modelgen]
2024-08-28 20:01:06 [info     ] 		meL: [2000]                  [Run3ModelGen.modelgen]
2024-08-28 20:01:06 [info     ] 		mtauL: [2000]                [Run3ModelGen.modelgen]
2024-08-28 20:01:06 [info     ] 		meR: [2000]                  [Run3ModelGen.modelgen]
2024-08-28 20:01:06 [info     ] 		mtauR: [2000]                [Run3ModelGen.modelgen]
2024-08-28 20:01:06 [info     ] 		mqL1: [4000]                 [Run3ModelGen.modelgen]
2024-08-28 20:01:06 [info     ] 		mqL3: [4000]                 [Run3ModelGen.modelgen]
2024-08-28 20:01:06 [info     ] 		muR: [4000]                  [Run3ModelGen.modelgen]
2024-08-28 20:01:06 [info     ] 		mtR: [4000]                  [Run3ModelGen.modelgen]
2024-08-28 20:01:06 [info     ] 		mdR: [4000]                  [Run3ModelGen.modelgen]
2024-08-28 20:01:06 [info     ] 		mbR: [4000]                  [Run3ModelGen.modelgen]
2024-08-28 20:01:06 [info     ] 	steps =                       [Run3ModelGen.modelgen]
2024-08-28 20:01:06 [info     ] 		{'name': 'prep_input', 'output_dir': 'input', 'prefix': 'IN'} [Run3ModelGen.modelgen]
2024-08-28 20:01:06 [info     ] 		{'name': 'SPheno', 'input_dir': 'input', 'output_dir': 'SPheno', 'log_dir': 'SPheno_log', 'prefix': 'SP'} [Run3ModelGen.modelgen]
2024-08-28 20:01:06 [info     ] 		{'name': 'softsusy', 'input_dir': 'input', 'output_dir': 'softsusy', 'prefix': 'SS'} [Run3ModelGen.modelgen]
2024-08-28 20:01:06 [info     ] 		{'name': 'micromegas', 'input_dir': 'SPheno', 'output_dir': 'micromegas', 'prefix': 'MO'} [Run3ModelGen.modelgen]
2024-08-28 20:01:06 [info     ] 		{'name': 'superiso', 'input_dir': 'SPheno', 'output_dir': 'superiso', 'prefix': 'SI'} [Run3ModelGen.modelgen]
2024-08-28 20:01:06 [info     ] 		{'name': 'gm2calc', 'input_dir': 'SPheno', 'output_dir': 'gm2calc', 'prefix': 'GM2'} [Run3ModelGen.modelgen]
2024-08-28 20:01:06 [info     ] 		{'name': 'evade', 'input_dir': 'SPheno', 'output_dir': 'evade', 'prefix': 'EV'} [Run3ModelGen.modelgen]
2024-08-28 20:01:06 [info     ] Starting Model Generation...   [Run3ModelGen.modelgen]
2024-08-28 20:01:06 [info     ] Setting up output directories for scan [Run3ModelGen.modelgen]
2024-08-28 20:01:06 [info     ] Generating Model: 0.slha (1/1) [Run3ModelGen.modelgen]
2024-08-28 20:01:06 [info     ] 	running step: prep_input      [Run3ModelGen.modelgen]
2024-08-28 20:01:06 [info     ] 	running step: SPheno          [Run3ModelGen.modelgen]
2024-08-28 20:01:07 [info     ] 	running step: softsusy        [Run3ModelGen.modelgen]
2024-08-28 20:01:07 [info     ] 	running step: micromegas      [Run3ModelGen.modelgen]
2024-08-28 20:01:08 [info     ] 	running step: superiso        [Run3ModelGen.modelgen]
2024-08-28 20:01:08 [info     ] 	running step: gm2calc         [Run3ModelGen.modelgen]
2024-08-28 20:01:08 [info     ] 	running step: evade           [Run3ModelGen.modelgen]
2024-08-28 20:01:08 [info     ] Finished generating models! Stats for successful generations (successful/attempted): [Run3ModelGen.modelgen]
2024-08-28 20:01:08 [info     ] 	prep_input: 1/1.              [Run3ModelGen.modelgen]
2024-08-28 20:01:08 [info     ] 	SPheno: 1/1.                  [Run3ModelGen.modelgen]
2024-08-28 20:01:08 [info     ] 	softsusy: 1/1.                [Run3ModelGen.modelgen]
2024-08-28 20:01:08 [info     ] 	micromegas: 1/1.              [Run3ModelGen.modelgen]
2024-08-28 20:01:08 [info     ] 	superiso: 1/1.                [Run3ModelGen.modelgen]
2024-08-28 20:01:08 [info     ] 	gm2calc: 1/1.                 [Run3ModelGen.modelgen]
2024-08-28 20:01:08 [info     ] 	evade: 0/1.                   [Run3ModelGen.modelgen]
2024-08-28 20:01:08 [info     ] Will make NTuple for: /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/scans/scan_15, with 1 models [Run3ModelGen.ntupling]
2024-08-28 20:01:08 [info     ] 	Filling values for model: 0   [Run3ModelGen.ntupling]
2024-08-28 20:01:08 [info     ] Saving NTuple in /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/scans/scan_15/ntuple.0.0.root ... [Run3ModelGen.ntupling]
Completed iteration 15
Starting iteration 16
Initial Training points:  tensor([0.8528, 0.5812, 0.1617, 0.0046], device='cuda:0') torch.Size([4])
Training points after adding:  tensor([0.0000, 0.0140, 0.4324, 0.1617, 0.1091, 0.0046, 0.2132, 0.0561, 0.3373,
        0.2593, 0.4995, 0.2723, 0.5175, 0.2392, 0.5812, 0.0210, 0.8528],
       device='cuda:0') torch.Size([17])
These training_points are used in the GP tensor([0.0000, 0.0140, 0.4324, 0.1617, 0.1091, 0.0046, 0.2132, 0.0561, 0.3373,
        0.2593, 0.4995, 0.2723, 0.5175, 0.2392, 0.5812, 0.0210, 0.8528],
       device='cuda:0')
Point: tensor([0.0000, 0.0010, 0.0020, 0.0030, 0.0040, 0.0050, 0.0060, 0.0070, 0.0080,
        0.0090, 0.0100, 0.0110, 0.0120, 0.0130, 0.0140, 0.0150, 0.0160, 0.0170,
        0.0180, 0.0190, 0.0200, 0.0210, 0.0220, 0.0230, 0.0240, 0.0250, 0.0260,
        0.0270, 0.0280, 0.0290, 0.0300, 0.0310, 0.0320, 0.0330, 0.0340, 0.0350,
        0.0360, 0.0370, 0.0380, 0.0390, 0.0400, 0.0410, 0.0420, 0.0430, 0.0440,
        0.0450, 0.0460, 0.0470, 0.0480, 0.0490, 0.0501, 0.0511, 0.0521, 0.0531,
        0.0541, 0.0551, 0.0561, 0.0571, 0.0581, 0.0591, 0.0601, 0.0611, 0.0621,
        0.0631, 0.0641, 0.0651, 0.0661, 0.0671, 0.0681, 0.0691, 0.0701, 0.0711,
        0.0721, 0.0731, 0.0741, 0.0751, 0.0761, 0.0771, 0.0781, 0.0791, 0.0801,
        0.0811, 0.0821, 0.0831, 0.0841, 0.0851, 0.0861, 0.0871, 0.0881, 0.0891,
        0.0901, 0.0911, 0.0921, 0.0931, 0.0941, 0.0951, 0.0961, 0.0971, 0.0981,
        0.0991, 0.1001, 0.1011, 0.1021, 0.1031, 0.1041, 0.1051, 0.1061, 0.1071,
        0.1081, 0.1091, 0.1101, 0.1111, 0.1121, 0.1131, 0.1141, 0.1151, 0.1161,
        0.1171, 0.1181, 0.1191, 0.1201, 0.1211, 0.1221, 0.1231, 0.1241, 0.1251,
        0.1261, 0.1271, 0.1281, 0.1291, 0.1301, 0.1311, 0.1321, 0.1331, 0.1341,
        0.1351, 0.1361, 0.1371, 0.1381, 0.1391, 0.1401, 0.1411, 0.1421, 0.1431,
        0.1441, 0.1451, 0.1461, 0.1471, 0.1481, 0.1491, 0.1502, 0.1512, 0.1522,
        0.1532, 0.1542, 0.1552, 0.1562, 0.1572, 0.1582, 0.1592, 0.1602, 0.1612,
        0.1622, 0.1632, 0.1642, 0.1652, 0.1662, 0.1672, 0.1682, 0.1692, 0.1702,
        0.1712, 0.1722, 0.1732, 0.1742, 0.1752, 0.1762, 0.1772, 0.1782, 0.1792,
        0.1802, 0.1812, 0.1822, 0.1832, 0.1842, 0.1852, 0.1862, 0.1872, 0.1882,
        0.1892, 0.1902, 0.1912, 0.1922, 0.1932, 0.1942, 0.1952, 0.1962, 0.1972,
        0.1982, 0.1992, 0.2002, 0.2012, 0.2022, 0.2032, 0.2042, 0.2052, 0.2062,
        0.2072, 0.2082, 0.2092, 0.2102, 0.2112, 0.2122, 0.2132, 0.2142, 0.2152,
        0.2162, 0.2172, 0.2182, 0.2192, 0.2202, 0.2212, 0.2222, 0.2232, 0.2242,
        0.2252, 0.2262, 0.2272, 0.2282, 0.2292, 0.2302, 0.2312, 0.2322, 0.2332,
        0.2342, 0.2352, 0.2362, 0.2372, 0.2382, 0.2392, 0.2402, 0.2412, 0.2422,
        0.2432, 0.2442, 0.2452, 0.2462, 0.2472, 0.2482, 0.2492, 0.2503, 0.2513,
        0.2523, 0.2533, 0.2543, 0.2553, 0.2563, 0.2573, 0.2583, 0.2593, 0.2603,
        0.2613, 0.2623, 0.2633, 0.2643, 0.2653, 0.2663, 0.2673, 0.2683, 0.2693,
        0.2703, 0.2713, 0.2723, 0.2733, 0.2743, 0.2753, 0.2763, 0.2773, 0.2783,
        0.2793, 0.2803, 0.2813, 0.2823, 0.2833, 0.2843, 0.2853, 0.2863, 0.2873,
        0.2883, 0.2893, 0.2903, 0.2913, 0.2923, 0.2933, 0.2943, 0.2953, 0.2963,
        0.2973, 0.2983, 0.2993, 0.3003, 0.3013, 0.3023, 0.3033, 0.3043, 0.3053,
        0.3063, 0.3073, 0.3083, 0.3093, 0.3103, 0.3113, 0.3123, 0.3133, 0.3143,
        0.3153, 0.3163, 0.3173, 0.3183, 0.3193, 0.3203, 0.3213, 0.3223, 0.3233,
        0.3243, 0.3253, 0.3263, 0.3273, 0.3283, 0.3293, 0.3303, 0.3313, 0.3323,
        0.3333, 0.3343, 0.3353, 0.3363, 0.3373, 0.3383, 0.3393, 0.3403, 0.3413,
        0.3423, 0.3433, 0.3443, 0.3453, 0.3463, 0.3473, 0.3483, 0.3493, 0.3504,
        0.3514, 0.3524, 0.3534, 0.3544, 0.3554, 0.3564, 0.3574, 0.3584, 0.3594,
        0.3604, 0.3614, 0.3624, 0.3634, 0.3644, 0.3654, 0.3664, 0.3674, 0.3684,
        0.3694, 0.3704, 0.3714, 0.3724, 0.3734, 0.3744, 0.3754, 0.3764, 0.3774,
        0.3784, 0.3794, 0.3804, 0.3814, 0.3824, 0.3834, 0.3844, 0.3854, 0.3864,
        0.3874, 0.3884, 0.3894, 0.3904, 0.3914, 0.3924, 0.3934, 0.3944, 0.3954,
        0.3964, 0.3974, 0.3984, 0.3994, 0.4004, 0.4014, 0.4024, 0.4034, 0.4044,
        0.4054, 0.4064, 0.4074, 0.4084, 0.4094, 0.4104, 0.4114, 0.4124, 0.4134,
        0.4144, 0.4154, 0.4164, 0.4174, 0.4184, 0.4194, 0.4204, 0.4214, 0.4224,
        0.4234, 0.4244, 0.4254, 0.4264, 0.4274, 0.4284, 0.4294, 0.4304, 0.4314,
        0.4324, 0.4334, 0.4344, 0.4354, 0.4364, 0.4374, 0.4384, 0.4394, 0.4404,
        0.4414, 0.4424, 0.4434, 0.4444, 0.4454, 0.4464, 0.4474, 0.4484, 0.4494,
        0.4505, 0.4515, 0.4525, 0.4535, 0.4545, 0.4555, 0.4565, 0.4575, 0.4585,
        0.4595, 0.4605, 0.4615, 0.4625, 0.4635, 0.4645, 0.4655, 0.4665, 0.4675,
        0.4685, 0.4695, 0.4705, 0.4715, 0.4725, 0.4735, 0.4745, 0.4755, 0.4765,
        0.4775, 0.4785, 0.4795, 0.4805, 0.4815, 0.4825, 0.4835, 0.4845, 0.4855,
        0.4865, 0.4875, 0.4885, 0.4895, 0.4905, 0.4915, 0.4925, 0.4935, 0.4945,
        0.4955, 0.4965, 0.4975, 0.4985, 0.4995, 0.5005, 0.5015, 0.5025, 0.5035,
        0.5045, 0.5055, 0.5065, 0.5075, 0.5085, 0.5095, 0.5105, 0.5115, 0.5125,
        0.5135, 0.5145, 0.5155, 0.5165, 0.5175, 0.5185, 0.5195, 0.5205, 0.5215,
        0.5225, 0.5235, 0.5245, 0.5255, 0.5265, 0.5275, 0.5285, 0.5295, 0.5305,
        0.5315, 0.5325, 0.5335, 0.5345, 0.5355, 0.5365, 0.5375, 0.5385, 0.5395,
        0.5405, 0.5415, 0.5425, 0.5435, 0.5445, 0.5455, 0.5465, 0.5475, 0.5485,
        0.5495, 0.5506, 0.5516, 0.5526, 0.5536, 0.5546, 0.5556, 0.5566, 0.5576,
        0.5586, 0.5596, 0.5606, 0.5616, 0.5626, 0.5636, 0.5646, 0.5656, 0.5666,
        0.5676, 0.5686, 0.5696, 0.5706, 0.5716, 0.5726, 0.5736, 0.5746, 0.5756,
        0.5766, 0.5776, 0.5786, 0.5796, 0.5806, 0.5816, 0.5826, 0.5836, 0.5846,
        0.5856, 0.5866, 0.5876, 0.5886, 0.5896, 0.5906, 0.5916, 0.5926, 0.5936,
        0.5946, 0.5956, 0.5966, 0.5976, 0.5986, 0.5996, 0.6006, 0.6016, 0.6026,
        0.6036, 0.6046, 0.6056, 0.6066, 0.6076, 0.6086, 0.6096, 0.6106, 0.6116,
        0.6126, 0.6136, 0.6146, 0.6156, 0.6166, 0.6176, 0.6186, 0.6196, 0.6206,
        0.6216, 0.6226, 0.6236, 0.6246, 0.6256, 0.6266, 0.6276, 0.6286, 0.6296,
        0.6306, 0.6316, 0.6326, 0.6336, 0.6346, 0.6356, 0.6366, 0.6376, 0.6386,
        0.6396, 0.6406, 0.6416, 0.6426, 0.6436, 0.6446, 0.6456, 0.6466, 0.6476,
        0.6486, 0.6496, 0.6507, 0.6517, 0.6527, 0.6537, 0.6547, 0.6557, 0.6567,
        0.6577, 0.6587, 0.6597, 0.6607, 0.6617, 0.6627, 0.6637, 0.6647, 0.6657,
        0.6667, 0.6677, 0.6687, 0.6697, 0.6707, 0.6717, 0.6727, 0.6737, 0.6747,
        0.6757, 0.6767, 0.6777, 0.6787, 0.6797, 0.6807, 0.6817, 0.6827, 0.6837,
        0.6847, 0.6857, 0.6867, 0.6877, 0.6887, 0.6897, 0.6907, 0.6917, 0.6927,
        0.6937, 0.6947, 0.6957, 0.6967, 0.6977, 0.6987, 0.6997, 0.7007, 0.7017,
        0.7027, 0.7037, 0.7047, 0.7057, 0.7067, 0.7077, 0.7087, 0.7097, 0.7107,
        0.7117, 0.7127, 0.7137, 0.7147, 0.7157, 0.7167, 0.7177, 0.7187, 0.7197,
        0.7207, 0.7217, 0.7227, 0.7237, 0.7247, 0.7257, 0.7267, 0.7277, 0.7287,
        0.7297, 0.7307, 0.7317, 0.7327, 0.7337, 0.7347, 0.7357, 0.7367, 0.7377,
        0.7387, 0.7397, 0.7407, 0.7417, 0.7427, 0.7437, 0.7447, 0.7457, 0.7467,
        0.7477, 0.7487, 0.7497, 0.7508, 0.7518, 0.7528, 0.7538, 0.7548, 0.7558,
        0.7568, 0.7578, 0.7588, 0.7598, 0.7608, 0.7618, 0.7628, 0.7638, 0.7648,
        0.7658, 0.7668, 0.7678, 0.7688, 0.7698, 0.7708, 0.7718, 0.7728, 0.7738,
        0.7748, 0.7758, 0.7768, 0.7778, 0.7788, 0.7798, 0.7808, 0.7818, 0.7828,
        0.7838, 0.7848, 0.7858, 0.7868, 0.7878, 0.7888, 0.7898, 0.7908, 0.7918,
        0.7928, 0.7938, 0.7948, 0.7958, 0.7968, 0.7978, 0.7988, 0.7998, 0.8008,
        0.8018, 0.8028, 0.8038, 0.8048, 0.8058, 0.8068, 0.8078, 0.8088, 0.8098,
        0.8108, 0.8118, 0.8128, 0.8138, 0.8148, 0.8158, 0.8168, 0.8178, 0.8188,
        0.8198, 0.8208, 0.8218, 0.8228, 0.8238, 0.8248, 0.8258, 0.8268, 0.8278,
        0.8288, 0.8298, 0.8308, 0.8318, 0.8328, 0.8338, 0.8348, 0.8358, 0.8368,
        0.8378, 0.8388, 0.8398, 0.8408, 0.8418, 0.8428, 0.8438, 0.8448, 0.8458,
        0.8468, 0.8478, 0.8488, 0.8498, 0.8509, 0.8519, 0.8529, 0.8539, 0.8549,
        0.8559, 0.8569, 0.8579, 0.8589, 0.8599, 0.8609, 0.8619, 0.8629, 0.8639,
        0.8649, 0.8659, 0.8669, 0.8679, 0.8689, 0.8699, 0.8709, 0.8719, 0.8729,
        0.8739, 0.8749, 0.8759, 0.8769, 0.8779, 0.8789, 0.8799, 0.8809, 0.8819,
        0.8829, 0.8839, 0.8849, 0.8859, 0.8869, 0.8879, 0.8889, 0.8899, 0.8909,
        0.8919, 0.8929, 0.8939, 0.8949, 0.8959, 0.8969, 0.8979, 0.8989, 0.8999,
        0.9009, 0.9019, 0.9029, 0.9039, 0.9049, 0.9059, 0.9069, 0.9079, 0.9089,
        0.9099, 0.9109, 0.9119, 0.9129, 0.9139, 0.9149, 0.9159, 0.9169, 0.9179,
        0.9189, 0.9199, 0.9209, 0.9219, 0.9229, 0.9239, 0.9249, 0.9259, 0.9269,
        0.9279, 0.9289, 0.9299, 0.9309, 0.9319, 0.9329, 0.9339, 0.9349, 0.9359,
        0.9369, 0.9379, 0.9389, 0.9399, 0.9409, 0.9419, 0.9429, 0.9439, 0.9449,
        0.9459, 0.9469, 0.9479, 0.9489, 0.9499, 0.9510, 0.9520, 0.9530, 0.9540,
        0.9550, 0.9560, 0.9570, 0.9580, 0.9590, 0.9600, 0.9610, 0.9620, 0.9630,
        0.9640, 0.9650, 0.9660, 0.9670, 0.9680, 0.9690, 0.9700, 0.9710, 0.9720,
        0.9730, 0.9740, 0.9750, 0.9760, 0.9770, 0.9780, 0.9790, 0.9800, 0.9810,
        0.9820, 0.9830, 0.9840, 0.9850, 0.9860, 0.9870, 0.9880, 0.9890, 0.9900,
        0.9910, 0.9920, 0.9930, 0.9940, 0.9950, 0.9960, 0.9970, 0.9980, 0.9990,
        1.0000], device='cuda:0') - Variance: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0') 
Smoothed_batch_entropy:  tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan], device='cuda:0')
Selected points (indices): {0}
Selected new x values (normalized): tensor([0.], device='cuda:0')
Corresponding new x values (unnormalized): tensor([-2000.], device='cuda:0')
Plot saved to /raven/u/dvoss/al_pmssmwithgp/model/plots/Iter16/gp_plot.png
YAML file has been created: /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/data/new_config.yaml
Note: detected 144 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
Note: NumExpr detected 144 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
NumExpr defaulting to 8 threads.

  ___              ____ __  __          _       _   ___            
 | _ \ _  _  _ _  |__ /|  \/  | ___  __| | ___ | | / __| ___  _ _  
 |   /| || || ' \  |_ \| |\/| |/ _ \/ _` |/ -_)| || (_ |/ -_)| ' \ 
 |_|_\ \_,_||_||_||___/|_|  |_|\___/\__,_|\___||_| \___|\___||_||_|
                                                                   

2024-08-28 20:01:53 [info     ] Initialised ModelGenerator with: [Run3ModelGen.modelgen]
2024-08-28 20:01:53 [info     ] 	config_file = /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/data/new_config.yaml [Run3ModelGen.modelgen]
2024-08-28 20:01:53 [info     ] 	scan_dir = /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/scans/scan_16 [Run3ModelGen.modelgen]
2024-08-28 20:01:53 [info     ] 	rawfilen = /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/data/raw.slha [Run3ModelGen.modelgen]
2024-08-28 20:01:53 [info     ] 	seed = 123                    [Run3ModelGen.modelgen]
2024-08-28 20:01:53 [info     ] 	points = {}                   [Run3ModelGen.modelgen]
2024-08-28 20:01:53 [info     ] 	prior = fixed                 [Run3ModelGen.modelgen]
2024-08-28 20:01:53 [info     ] 	num_models = 1                [Run3ModelGen.modelgen]
2024-08-28 20:01:53 [info     ] 	isGMSB = False                [Run3ModelGen.modelgen]
2024-08-28 20:01:53 [info     ] 	parameters =                  [Run3ModelGen.modelgen]
2024-08-28 20:01:53 [info     ] 		tanb: [60]                   [Run3ModelGen.modelgen]
2024-08-28 20:01:53 [info     ] 		M_1: [-2000.0]               [Run3ModelGen.modelgen]
2024-08-28 20:01:53 [info     ] 		M_2: [2000]                  [Run3ModelGen.modelgen]
2024-08-28 20:01:53 [info     ] 		M_3: [4000]                  [Run3ModelGen.modelgen]
2024-08-28 20:01:53 [info     ] 		AT: [4000]                   [Run3ModelGen.modelgen]
2024-08-28 20:01:53 [info     ] 		Ab: [2000]                   [Run3ModelGen.modelgen]
2024-08-28 20:01:53 [info     ] 		Atau: [2000]                 [Run3ModelGen.modelgen]
2024-08-28 20:01:53 [info     ] 		mu: [2000]                   [Run3ModelGen.modelgen]
2024-08-28 20:01:53 [info     ] 		mA: [2000]                   [Run3ModelGen.modelgen]
2024-08-28 20:01:53 [info     ] 		meL: [2000]                  [Run3ModelGen.modelgen]
2024-08-28 20:01:53 [info     ] 		mtauL: [2000]                [Run3ModelGen.modelgen]
2024-08-28 20:01:53 [info     ] 		meR: [2000]                  [Run3ModelGen.modelgen]
2024-08-28 20:01:53 [info     ] 		mtauR: [2000]                [Run3ModelGen.modelgen]
2024-08-28 20:01:53 [info     ] 		mqL1: [4000]                 [Run3ModelGen.modelgen]
2024-08-28 20:01:53 [info     ] 		mqL3: [4000]                 [Run3ModelGen.modelgen]
2024-08-28 20:01:53 [info     ] 		muR: [4000]                  [Run3ModelGen.modelgen]
2024-08-28 20:01:53 [info     ] 		mtR: [4000]                  [Run3ModelGen.modelgen]
2024-08-28 20:01:53 [info     ] 		mdR: [4000]                  [Run3ModelGen.modelgen]
2024-08-28 20:01:53 [info     ] 		mbR: [4000]                  [Run3ModelGen.modelgen]
2024-08-28 20:01:53 [info     ] 	steps =                       [Run3ModelGen.modelgen]
2024-08-28 20:01:53 [info     ] 		{'name': 'prep_input', 'output_dir': 'input', 'prefix': 'IN'} [Run3ModelGen.modelgen]
2024-08-28 20:01:53 [info     ] 		{'name': 'SPheno', 'input_dir': 'input', 'output_dir': 'SPheno', 'log_dir': 'SPheno_log', 'prefix': 'SP'} [Run3ModelGen.modelgen]
2024-08-28 20:01:53 [info     ] 		{'name': 'softsusy', 'input_dir': 'input', 'output_dir': 'softsusy', 'prefix': 'SS'} [Run3ModelGen.modelgen]
2024-08-28 20:01:53 [info     ] 		{'name': 'micromegas', 'input_dir': 'SPheno', 'output_dir': 'micromegas', 'prefix': 'MO'} [Run3ModelGen.modelgen]
2024-08-28 20:01:53 [info     ] 		{'name': 'superiso', 'input_dir': 'SPheno', 'output_dir': 'superiso', 'prefix': 'SI'} [Run3ModelGen.modelgen]
2024-08-28 20:01:53 [info     ] 		{'name': 'gm2calc', 'input_dir': 'SPheno', 'output_dir': 'gm2calc', 'prefix': 'GM2'} [Run3ModelGen.modelgen]
2024-08-28 20:01:53 [info     ] 		{'name': 'evade', 'input_dir': 'SPheno', 'output_dir': 'evade', 'prefix': 'EV'} [Run3ModelGen.modelgen]
2024-08-28 20:01:53 [info     ] Starting Model Generation...   [Run3ModelGen.modelgen]
2024-08-28 20:01:53 [info     ] Setting up output directories for scan [Run3ModelGen.modelgen]
2024-08-28 20:01:53 [info     ] Generating Model: 0.slha (1/1) [Run3ModelGen.modelgen]
2024-08-28 20:01:53 [info     ] 	running step: prep_input      [Run3ModelGen.modelgen]
2024-08-28 20:01:53 [info     ] 	running step: SPheno          [Run3ModelGen.modelgen]
2024-08-28 20:01:53 [info     ] 	running step: softsusy        [Run3ModelGen.modelgen]
2024-08-28 20:01:53 [info     ] 	running step: micromegas      [Run3ModelGen.modelgen]
2024-08-28 20:01:54 [info     ] 	running step: superiso        [Run3ModelGen.modelgen]
2024-08-28 20:01:54 [info     ] 	running step: gm2calc         [Run3ModelGen.modelgen]
2024-08-28 20:01:54 [info     ] 	running step: evade           [Run3ModelGen.modelgen]
2024-08-28 20:01:54 [info     ] Finished generating models! Stats for successful generations (successful/attempted): [Run3ModelGen.modelgen]
2024-08-28 20:01:54 [info     ] 	prep_input: 1/1.              [Run3ModelGen.modelgen]
2024-08-28 20:01:54 [info     ] 	SPheno: 1/1.                  [Run3ModelGen.modelgen]
2024-08-28 20:01:54 [info     ] 	softsusy: 1/1.                [Run3ModelGen.modelgen]
2024-08-28 20:01:54 [info     ] 	micromegas: 1/1.              [Run3ModelGen.modelgen]
2024-08-28 20:01:54 [info     ] 	superiso: 1/1.                [Run3ModelGen.modelgen]
2024-08-28 20:01:54 [info     ] 	gm2calc: 1/1.                 [Run3ModelGen.modelgen]
2024-08-28 20:01:54 [info     ] 	evade: 0/1.                   [Run3ModelGen.modelgen]
2024-08-28 20:01:54 [info     ] Will make NTuple for: /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/scans/scan_16, with 1 models [Run3ModelGen.ntupling]
2024-08-28 20:01:54 [info     ] 	Filling values for model: 0   [Run3ModelGen.ntupling]
2024-08-28 20:01:54 [info     ] Saving NTuple in /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/scans/scan_16/ntuple.0.0.root ... [Run3ModelGen.ntupling]
Completed iteration 16
Starting iteration 17
Initial Training points:  tensor([0.8528, 0.5812, 0.1617, 0.0046], device='cuda:0') torch.Size([4])
Training points after adding:  tensor([0.0000, 0.0140, 0.4324, 0.1617, 0.1091, 0.0046, 0.2132, 0.0561, 0.3373,
        0.2593, 0.4995, 0.2723, 0.5175, 0.2392, 0.5812, 0.0210, 0.8528],
       device='cuda:0') torch.Size([17])
These training_points are used in the GP tensor([0.0000, 0.0140, 0.4324, 0.1617, 0.1091, 0.0046, 0.2132, 0.0561, 0.3373,
        0.2593, 0.4995, 0.2723, 0.5175, 0.2392, 0.5812, 0.0210, 0.8528],
       device='cuda:0')
Point: tensor([0.0000, 0.0010, 0.0020, 0.0030, 0.0040, 0.0050, 0.0060, 0.0070, 0.0080,
        0.0090, 0.0100, 0.0110, 0.0120, 0.0130, 0.0140, 0.0150, 0.0160, 0.0170,
        0.0180, 0.0190, 0.0200, 0.0210, 0.0220, 0.0230, 0.0240, 0.0250, 0.0260,
        0.0270, 0.0280, 0.0290, 0.0300, 0.0310, 0.0320, 0.0330, 0.0340, 0.0350,
        0.0360, 0.0370, 0.0380, 0.0390, 0.0400, 0.0410, 0.0420, 0.0430, 0.0440,
        0.0450, 0.0460, 0.0470, 0.0480, 0.0490, 0.0501, 0.0511, 0.0521, 0.0531,
        0.0541, 0.0551, 0.0561, 0.0571, 0.0581, 0.0591, 0.0601, 0.0611, 0.0621,
        0.0631, 0.0641, 0.0651, 0.0661, 0.0671, 0.0681, 0.0691, 0.0701, 0.0711,
        0.0721, 0.0731, 0.0741, 0.0751, 0.0761, 0.0771, 0.0781, 0.0791, 0.0801,
        0.0811, 0.0821, 0.0831, 0.0841, 0.0851, 0.0861, 0.0871, 0.0881, 0.0891,
        0.0901, 0.0911, 0.0921, 0.0931, 0.0941, 0.0951, 0.0961, 0.0971, 0.0981,
        0.0991, 0.1001, 0.1011, 0.1021, 0.1031, 0.1041, 0.1051, 0.1061, 0.1071,
        0.1081, 0.1091, 0.1101, 0.1111, 0.1121, 0.1131, 0.1141, 0.1151, 0.1161,
        0.1171, 0.1181, 0.1191, 0.1201, 0.1211, 0.1221, 0.1231, 0.1241, 0.1251,
        0.1261, 0.1271, 0.1281, 0.1291, 0.1301, 0.1311, 0.1321, 0.1331, 0.1341,
        0.1351, 0.1361, 0.1371, 0.1381, 0.1391, 0.1401, 0.1411, 0.1421, 0.1431,
        0.1441, 0.1451, 0.1461, 0.1471, 0.1481, 0.1491, 0.1502, 0.1512, 0.1522,
        0.1532, 0.1542, 0.1552, 0.1562, 0.1572, 0.1582, 0.1592, 0.1602, 0.1612,
        0.1622, 0.1632, 0.1642, 0.1652, 0.1662, 0.1672, 0.1682, 0.1692, 0.1702,
        0.1712, 0.1722, 0.1732, 0.1742, 0.1752, 0.1762, 0.1772, 0.1782, 0.1792,
        0.1802, 0.1812, 0.1822, 0.1832, 0.1842, 0.1852, 0.1862, 0.1872, 0.1882,
        0.1892, 0.1902, 0.1912, 0.1922, 0.1932, 0.1942, 0.1952, 0.1962, 0.1972,
        0.1982, 0.1992, 0.2002, 0.2012, 0.2022, 0.2032, 0.2042, 0.2052, 0.2062,
        0.2072, 0.2082, 0.2092, 0.2102, 0.2112, 0.2122, 0.2132, 0.2142, 0.2152,
        0.2162, 0.2172, 0.2182, 0.2192, 0.2202, 0.2212, 0.2222, 0.2232, 0.2242,
        0.2252, 0.2262, 0.2272, 0.2282, 0.2292, 0.2302, 0.2312, 0.2322, 0.2332,
        0.2342, 0.2352, 0.2362, 0.2372, 0.2382, 0.2392, 0.2402, 0.2412, 0.2422,
        0.2432, 0.2442, 0.2452, 0.2462, 0.2472, 0.2482, 0.2492, 0.2503, 0.2513,
        0.2523, 0.2533, 0.2543, 0.2553, 0.2563, 0.2573, 0.2583, 0.2593, 0.2603,
        0.2613, 0.2623, 0.2633, 0.2643, 0.2653, 0.2663, 0.2673, 0.2683, 0.2693,
        0.2703, 0.2713, 0.2723, 0.2733, 0.2743, 0.2753, 0.2763, 0.2773, 0.2783,
        0.2793, 0.2803, 0.2813, 0.2823, 0.2833, 0.2843, 0.2853, 0.2863, 0.2873,
        0.2883, 0.2893, 0.2903, 0.2913, 0.2923, 0.2933, 0.2943, 0.2953, 0.2963,
        0.2973, 0.2983, 0.2993, 0.3003, 0.3013, 0.3023, 0.3033, 0.3043, 0.3053,
        0.3063, 0.3073, 0.3083, 0.3093, 0.3103, 0.3113, 0.3123, 0.3133, 0.3143,
        0.3153, 0.3163, 0.3173, 0.3183, 0.3193, 0.3203, 0.3213, 0.3223, 0.3233,
        0.3243, 0.3253, 0.3263, 0.3273, 0.3283, 0.3293, 0.3303, 0.3313, 0.3323,
        0.3333, 0.3343, 0.3353, 0.3363, 0.3373, 0.3383, 0.3393, 0.3403, 0.3413,
        0.3423, 0.3433, 0.3443, 0.3453, 0.3463, 0.3473, 0.3483, 0.3493, 0.3504,
        0.3514, 0.3524, 0.3534, 0.3544, 0.3554, 0.3564, 0.3574, 0.3584, 0.3594,
        0.3604, 0.3614, 0.3624, 0.3634, 0.3644, 0.3654, 0.3664, 0.3674, 0.3684,
        0.3694, 0.3704, 0.3714, 0.3724, 0.3734, 0.3744, 0.3754, 0.3764, 0.3774,
        0.3784, 0.3794, 0.3804, 0.3814, 0.3824, 0.3834, 0.3844, 0.3854, 0.3864,
        0.3874, 0.3884, 0.3894, 0.3904, 0.3914, 0.3924, 0.3934, 0.3944, 0.3954,
        0.3964, 0.3974, 0.3984, 0.3994, 0.4004, 0.4014, 0.4024, 0.4034, 0.4044,
        0.4054, 0.4064, 0.4074, 0.4084, 0.4094, 0.4104, 0.4114, 0.4124, 0.4134,
        0.4144, 0.4154, 0.4164, 0.4174, 0.4184, 0.4194, 0.4204, 0.4214, 0.4224,
        0.4234, 0.4244, 0.4254, 0.4264, 0.4274, 0.4284, 0.4294, 0.4304, 0.4314,
        0.4324, 0.4334, 0.4344, 0.4354, 0.4364, 0.4374, 0.4384, 0.4394, 0.4404,
        0.4414, 0.4424, 0.4434, 0.4444, 0.4454, 0.4464, 0.4474, 0.4484, 0.4494,
        0.4505, 0.4515, 0.4525, 0.4535, 0.4545, 0.4555, 0.4565, 0.4575, 0.4585,
        0.4595, 0.4605, 0.4615, 0.4625, 0.4635, 0.4645, 0.4655, 0.4665, 0.4675,
        0.4685, 0.4695, 0.4705, 0.4715, 0.4725, 0.4735, 0.4745, 0.4755, 0.4765,
        0.4775, 0.4785, 0.4795, 0.4805, 0.4815, 0.4825, 0.4835, 0.4845, 0.4855,
        0.4865, 0.4875, 0.4885, 0.4895, 0.4905, 0.4915, 0.4925, 0.4935, 0.4945,
        0.4955, 0.4965, 0.4975, 0.4985, 0.4995, 0.5005, 0.5015, 0.5025, 0.5035,
        0.5045, 0.5055, 0.5065, 0.5075, 0.5085, 0.5095, 0.5105, 0.5115, 0.5125,
        0.5135, 0.5145, 0.5155, 0.5165, 0.5175, 0.5185, 0.5195, 0.5205, 0.5215,
        0.5225, 0.5235, 0.5245, 0.5255, 0.5265, 0.5275, 0.5285, 0.5295, 0.5305,
        0.5315, 0.5325, 0.5335, 0.5345, 0.5355, 0.5365, 0.5375, 0.5385, 0.5395,
        0.5405, 0.5415, 0.5425, 0.5435, 0.5445, 0.5455, 0.5465, 0.5475, 0.5485,
        0.5495, 0.5506, 0.5516, 0.5526, 0.5536, 0.5546, 0.5556, 0.5566, 0.5576,
        0.5586, 0.5596, 0.5606, 0.5616, 0.5626, 0.5636, 0.5646, 0.5656, 0.5666,
        0.5676, 0.5686, 0.5696, 0.5706, 0.5716, 0.5726, 0.5736, 0.5746, 0.5756,
        0.5766, 0.5776, 0.5786, 0.5796, 0.5806, 0.5816, 0.5826, 0.5836, 0.5846,
        0.5856, 0.5866, 0.5876, 0.5886, 0.5896, 0.5906, 0.5916, 0.5926, 0.5936,
        0.5946, 0.5956, 0.5966, 0.5976, 0.5986, 0.5996, 0.6006, 0.6016, 0.6026,
        0.6036, 0.6046, 0.6056, 0.6066, 0.6076, 0.6086, 0.6096, 0.6106, 0.6116,
        0.6126, 0.6136, 0.6146, 0.6156, 0.6166, 0.6176, 0.6186, 0.6196, 0.6206,
        0.6216, 0.6226, 0.6236, 0.6246, 0.6256, 0.6266, 0.6276, 0.6286, 0.6296,
        0.6306, 0.6316, 0.6326, 0.6336, 0.6346, 0.6356, 0.6366, 0.6376, 0.6386,
        0.6396, 0.6406, 0.6416, 0.6426, 0.6436, 0.6446, 0.6456, 0.6466, 0.6476,
        0.6486, 0.6496, 0.6507, 0.6517, 0.6527, 0.6537, 0.6547, 0.6557, 0.6567,
        0.6577, 0.6587, 0.6597, 0.6607, 0.6617, 0.6627, 0.6637, 0.6647, 0.6657,
        0.6667, 0.6677, 0.6687, 0.6697, 0.6707, 0.6717, 0.6727, 0.6737, 0.6747,
        0.6757, 0.6767, 0.6777, 0.6787, 0.6797, 0.6807, 0.6817, 0.6827, 0.6837,
        0.6847, 0.6857, 0.6867, 0.6877, 0.6887, 0.6897, 0.6907, 0.6917, 0.6927,
        0.6937, 0.6947, 0.6957, 0.6967, 0.6977, 0.6987, 0.6997, 0.7007, 0.7017,
        0.7027, 0.7037, 0.7047, 0.7057, 0.7067, 0.7077, 0.7087, 0.7097, 0.7107,
        0.7117, 0.7127, 0.7137, 0.7147, 0.7157, 0.7167, 0.7177, 0.7187, 0.7197,
        0.7207, 0.7217, 0.7227, 0.7237, 0.7247, 0.7257, 0.7267, 0.7277, 0.7287,
        0.7297, 0.7307, 0.7317, 0.7327, 0.7337, 0.7347, 0.7357, 0.7367, 0.7377,
        0.7387, 0.7397, 0.7407, 0.7417, 0.7427, 0.7437, 0.7447, 0.7457, 0.7467,
        0.7477, 0.7487, 0.7497, 0.7508, 0.7518, 0.7528, 0.7538, 0.7548, 0.7558,
        0.7568, 0.7578, 0.7588, 0.7598, 0.7608, 0.7618, 0.7628, 0.7638, 0.7648,
        0.7658, 0.7668, 0.7678, 0.7688, 0.7698, 0.7708, 0.7718, 0.7728, 0.7738,
        0.7748, 0.7758, 0.7768, 0.7778, 0.7788, 0.7798, 0.7808, 0.7818, 0.7828,
        0.7838, 0.7848, 0.7858, 0.7868, 0.7878, 0.7888, 0.7898, 0.7908, 0.7918,
        0.7928, 0.7938, 0.7948, 0.7958, 0.7968, 0.7978, 0.7988, 0.7998, 0.8008,
        0.8018, 0.8028, 0.8038, 0.8048, 0.8058, 0.8068, 0.8078, 0.8088, 0.8098,
        0.8108, 0.8118, 0.8128, 0.8138, 0.8148, 0.8158, 0.8168, 0.8178, 0.8188,
        0.8198, 0.8208, 0.8218, 0.8228, 0.8238, 0.8248, 0.8258, 0.8268, 0.8278,
        0.8288, 0.8298, 0.8308, 0.8318, 0.8328, 0.8338, 0.8348, 0.8358, 0.8368,
        0.8378, 0.8388, 0.8398, 0.8408, 0.8418, 0.8428, 0.8438, 0.8448, 0.8458,
        0.8468, 0.8478, 0.8488, 0.8498, 0.8509, 0.8519, 0.8529, 0.8539, 0.8549,
        0.8559, 0.8569, 0.8579, 0.8589, 0.8599, 0.8609, 0.8619, 0.8629, 0.8639,
        0.8649, 0.8659, 0.8669, 0.8679, 0.8689, 0.8699, 0.8709, 0.8719, 0.8729,
        0.8739, 0.8749, 0.8759, 0.8769, 0.8779, 0.8789, 0.8799, 0.8809, 0.8819,
        0.8829, 0.8839, 0.8849, 0.8859, 0.8869, 0.8879, 0.8889, 0.8899, 0.8909,
        0.8919, 0.8929, 0.8939, 0.8949, 0.8959, 0.8969, 0.8979, 0.8989, 0.8999,
        0.9009, 0.9019, 0.9029, 0.9039, 0.9049, 0.9059, 0.9069, 0.9079, 0.9089,
        0.9099, 0.9109, 0.9119, 0.9129, 0.9139, 0.9149, 0.9159, 0.9169, 0.9179,
        0.9189, 0.9199, 0.9209, 0.9219, 0.9229, 0.9239, 0.9249, 0.9259, 0.9269,
        0.9279, 0.9289, 0.9299, 0.9309, 0.9319, 0.9329, 0.9339, 0.9349, 0.9359,
        0.9369, 0.9379, 0.9389, 0.9399, 0.9409, 0.9419, 0.9429, 0.9439, 0.9449,
        0.9459, 0.9469, 0.9479, 0.9489, 0.9499, 0.9510, 0.9520, 0.9530, 0.9540,
        0.9550, 0.9560, 0.9570, 0.9580, 0.9590, 0.9600, 0.9610, 0.9620, 0.9630,
        0.9640, 0.9650, 0.9660, 0.9670, 0.9680, 0.9690, 0.9700, 0.9710, 0.9720,
        0.9730, 0.9740, 0.9750, 0.9760, 0.9770, 0.9780, 0.9790, 0.9800, 0.9810,
        0.9820, 0.9830, 0.9840, 0.9850, 0.9860, 0.9870, 0.9880, 0.9890, 0.9900,
        0.9910, 0.9920, 0.9930, 0.9940, 0.9950, 0.9960, 0.9970, 0.9980, 0.9990,
        1.0000], device='cuda:0') - Variance: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0') 
Smoothed_batch_entropy:  tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan], device='cuda:0')
Selected points (indices): {0}
Selected new x values (normalized): tensor([0.], device='cuda:0')
Corresponding new x values (unnormalized): tensor([-2000.], device='cuda:0')
Plot saved to /raven/u/dvoss/al_pmssmwithgp/model/plots/Iter17/gp_plot.png
YAML file has been created: /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/data/new_config.yaml
Note: detected 144 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
Note: NumExpr detected 144 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
NumExpr defaulting to 8 threads.

  ___              ____ __  __          _       _   ___            
 | _ \ _  _  _ _  |__ /|  \/  | ___  __| | ___ | | / __| ___  _ _  
 |   /| || || ' \  |_ \| |\/| |/ _ \/ _` |/ -_)| || (_ |/ -_)| ' \ 
 |_|_\ \_,_||_||_||___/|_|  |_|\___/\__,_|\___||_| \___|\___||_||_|
                                                                   

2024-08-28 20:02:39 [info     ] Initialised ModelGenerator with: [Run3ModelGen.modelgen]
2024-08-28 20:02:39 [info     ] 	config_file = /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/data/new_config.yaml [Run3ModelGen.modelgen]
2024-08-28 20:02:39 [info     ] 	scan_dir = /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/scans/scan_17 [Run3ModelGen.modelgen]
2024-08-28 20:02:39 [info     ] 	rawfilen = /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/data/raw.slha [Run3ModelGen.modelgen]
2024-08-28 20:02:39 [info     ] 	seed = 123                    [Run3ModelGen.modelgen]
2024-08-28 20:02:39 [info     ] 	points = {}                   [Run3ModelGen.modelgen]
2024-08-28 20:02:39 [info     ] 	prior = fixed                 [Run3ModelGen.modelgen]
2024-08-28 20:02:39 [info     ] 	num_models = 1                [Run3ModelGen.modelgen]
2024-08-28 20:02:39 [info     ] 	isGMSB = False                [Run3ModelGen.modelgen]
2024-08-28 20:02:39 [info     ] 	parameters =                  [Run3ModelGen.modelgen]
2024-08-28 20:02:39 [info     ] 		tanb: [60]                   [Run3ModelGen.modelgen]
2024-08-28 20:02:39 [info     ] 		M_1: [-2000.0]               [Run3ModelGen.modelgen]
2024-08-28 20:02:39 [info     ] 		M_2: [2000]                  [Run3ModelGen.modelgen]
2024-08-28 20:02:39 [info     ] 		M_3: [4000]                  [Run3ModelGen.modelgen]
2024-08-28 20:02:39 [info     ] 		AT: [4000]                   [Run3ModelGen.modelgen]
2024-08-28 20:02:39 [info     ] 		Ab: [2000]                   [Run3ModelGen.modelgen]
2024-08-28 20:02:39 [info     ] 		Atau: [2000]                 [Run3ModelGen.modelgen]
2024-08-28 20:02:39 [info     ] 		mu: [2000]                   [Run3ModelGen.modelgen]
2024-08-28 20:02:39 [info     ] 		mA: [2000]                   [Run3ModelGen.modelgen]
2024-08-28 20:02:39 [info     ] 		meL: [2000]                  [Run3ModelGen.modelgen]
2024-08-28 20:02:39 [info     ] 		mtauL: [2000]                [Run3ModelGen.modelgen]
2024-08-28 20:02:39 [info     ] 		meR: [2000]                  [Run3ModelGen.modelgen]
2024-08-28 20:02:39 [info     ] 		mtauR: [2000]                [Run3ModelGen.modelgen]
2024-08-28 20:02:39 [info     ] 		mqL1: [4000]                 [Run3ModelGen.modelgen]
2024-08-28 20:02:39 [info     ] 		mqL3: [4000]                 [Run3ModelGen.modelgen]
2024-08-28 20:02:39 [info     ] 		muR: [4000]                  [Run3ModelGen.modelgen]
2024-08-28 20:02:39 [info     ] 		mtR: [4000]                  [Run3ModelGen.modelgen]
2024-08-28 20:02:39 [info     ] 		mdR: [4000]                  [Run3ModelGen.modelgen]
2024-08-28 20:02:39 [info     ] 		mbR: [4000]                  [Run3ModelGen.modelgen]
2024-08-28 20:02:39 [info     ] 	steps =                       [Run3ModelGen.modelgen]
2024-08-28 20:02:39 [info     ] 		{'name': 'prep_input', 'output_dir': 'input', 'prefix': 'IN'} [Run3ModelGen.modelgen]
2024-08-28 20:02:39 [info     ] 		{'name': 'SPheno', 'input_dir': 'input', 'output_dir': 'SPheno', 'log_dir': 'SPheno_log', 'prefix': 'SP'} [Run3ModelGen.modelgen]
2024-08-28 20:02:39 [info     ] 		{'name': 'softsusy', 'input_dir': 'input', 'output_dir': 'softsusy', 'prefix': 'SS'} [Run3ModelGen.modelgen]
2024-08-28 20:02:39 [info     ] 		{'name': 'micromegas', 'input_dir': 'SPheno', 'output_dir': 'micromegas', 'prefix': 'MO'} [Run3ModelGen.modelgen]
2024-08-28 20:02:39 [info     ] 		{'name': 'superiso', 'input_dir': 'SPheno', 'output_dir': 'superiso', 'prefix': 'SI'} [Run3ModelGen.modelgen]
2024-08-28 20:02:39 [info     ] 		{'name': 'gm2calc', 'input_dir': 'SPheno', 'output_dir': 'gm2calc', 'prefix': 'GM2'} [Run3ModelGen.modelgen]
2024-08-28 20:02:39 [info     ] 		{'name': 'evade', 'input_dir': 'SPheno', 'output_dir': 'evade', 'prefix': 'EV'} [Run3ModelGen.modelgen]
2024-08-28 20:02:39 [info     ] Starting Model Generation...   [Run3ModelGen.modelgen]
2024-08-28 20:02:39 [info     ] Setting up output directories for scan [Run3ModelGen.modelgen]
2024-08-28 20:02:39 [info     ] Generating Model: 0.slha (1/1) [Run3ModelGen.modelgen]
2024-08-28 20:02:39 [info     ] 	running step: prep_input      [Run3ModelGen.modelgen]
2024-08-28 20:02:39 [info     ] 	running step: SPheno          [Run3ModelGen.modelgen]
2024-08-28 20:02:39 [info     ] 	running step: softsusy        [Run3ModelGen.modelgen]
2024-08-28 20:02:39 [info     ] 	running step: micromegas      [Run3ModelGen.modelgen]
2024-08-28 20:02:40 [info     ] 	running step: superiso        [Run3ModelGen.modelgen]
2024-08-28 20:02:41 [info     ] 	running step: gm2calc         [Run3ModelGen.modelgen]
2024-08-28 20:02:41 [info     ] 	running step: evade           [Run3ModelGen.modelgen]
2024-08-28 20:02:41 [info     ] Finished generating models! Stats for successful generations (successful/attempted): [Run3ModelGen.modelgen]
2024-08-28 20:02:41 [info     ] 	prep_input: 1/1.              [Run3ModelGen.modelgen]
2024-08-28 20:02:41 [info     ] 	SPheno: 1/1.                  [Run3ModelGen.modelgen]
2024-08-28 20:02:41 [info     ] 	softsusy: 1/1.                [Run3ModelGen.modelgen]
2024-08-28 20:02:41 [info     ] 	micromegas: 1/1.              [Run3ModelGen.modelgen]
2024-08-28 20:02:41 [info     ] 	superiso: 1/1.                [Run3ModelGen.modelgen]
2024-08-28 20:02:41 [info     ] 	gm2calc: 1/1.                 [Run3ModelGen.modelgen]
2024-08-28 20:02:41 [info     ] 	evade: 0/1.                   [Run3ModelGen.modelgen]
2024-08-28 20:02:41 [info     ] Will make NTuple for: /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/scans/scan_17, with 1 models [Run3ModelGen.ntupling]
2024-08-28 20:02:41 [info     ] 	Filling values for model: 0   [Run3ModelGen.ntupling]
2024-08-28 20:02:41 [info     ] Saving NTuple in /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/scans/scan_17/ntuple.0.0.root ... [Run3ModelGen.ntupling]
Completed iteration 17
Starting iteration 18
Initial Training points:  tensor([0.8528, 0.5812, 0.1617, 0.0046], device='cuda:0') torch.Size([4])
Training points after adding:  tensor([0.0000, 0.0140, 0.4324, 0.1617, 0.1091, 0.0046, 0.2132, 0.0561, 0.3373,
        0.2593, 0.4995, 0.2723, 0.5175, 0.2392, 0.5812, 0.0210, 0.8528],
       device='cuda:0') torch.Size([17])
These training_points are used in the GP tensor([0.0000, 0.0140, 0.4324, 0.1617, 0.1091, 0.0046, 0.2132, 0.0561, 0.3373,
        0.2593, 0.4995, 0.2723, 0.5175, 0.2392, 0.5812, 0.0210, 0.8528],
       device='cuda:0')
Point: tensor([0.0000, 0.0010, 0.0020, 0.0030, 0.0040, 0.0050, 0.0060, 0.0070, 0.0080,
        0.0090, 0.0100, 0.0110, 0.0120, 0.0130, 0.0140, 0.0150, 0.0160, 0.0170,
        0.0180, 0.0190, 0.0200, 0.0210, 0.0220, 0.0230, 0.0240, 0.0250, 0.0260,
        0.0270, 0.0280, 0.0290, 0.0300, 0.0310, 0.0320, 0.0330, 0.0340, 0.0350,
        0.0360, 0.0370, 0.0380, 0.0390, 0.0400, 0.0410, 0.0420, 0.0430, 0.0440,
        0.0450, 0.0460, 0.0470, 0.0480, 0.0490, 0.0501, 0.0511, 0.0521, 0.0531,
        0.0541, 0.0551, 0.0561, 0.0571, 0.0581, 0.0591, 0.0601, 0.0611, 0.0621,
        0.0631, 0.0641, 0.0651, 0.0661, 0.0671, 0.0681, 0.0691, 0.0701, 0.0711,
        0.0721, 0.0731, 0.0741, 0.0751, 0.0761, 0.0771, 0.0781, 0.0791, 0.0801,
        0.0811, 0.0821, 0.0831, 0.0841, 0.0851, 0.0861, 0.0871, 0.0881, 0.0891,
        0.0901, 0.0911, 0.0921, 0.0931, 0.0941, 0.0951, 0.0961, 0.0971, 0.0981,
        0.0991, 0.1001, 0.1011, 0.1021, 0.1031, 0.1041, 0.1051, 0.1061, 0.1071,
        0.1081, 0.1091, 0.1101, 0.1111, 0.1121, 0.1131, 0.1141, 0.1151, 0.1161,
        0.1171, 0.1181, 0.1191, 0.1201, 0.1211, 0.1221, 0.1231, 0.1241, 0.1251,
        0.1261, 0.1271, 0.1281, 0.1291, 0.1301, 0.1311, 0.1321, 0.1331, 0.1341,
        0.1351, 0.1361, 0.1371, 0.1381, 0.1391, 0.1401, 0.1411, 0.1421, 0.1431,
        0.1441, 0.1451, 0.1461, 0.1471, 0.1481, 0.1491, 0.1502, 0.1512, 0.1522,
        0.1532, 0.1542, 0.1552, 0.1562, 0.1572, 0.1582, 0.1592, 0.1602, 0.1612,
        0.1622, 0.1632, 0.1642, 0.1652, 0.1662, 0.1672, 0.1682, 0.1692, 0.1702,
        0.1712, 0.1722, 0.1732, 0.1742, 0.1752, 0.1762, 0.1772, 0.1782, 0.1792,
        0.1802, 0.1812, 0.1822, 0.1832, 0.1842, 0.1852, 0.1862, 0.1872, 0.1882,
        0.1892, 0.1902, 0.1912, 0.1922, 0.1932, 0.1942, 0.1952, 0.1962, 0.1972,
        0.1982, 0.1992, 0.2002, 0.2012, 0.2022, 0.2032, 0.2042, 0.2052, 0.2062,
        0.2072, 0.2082, 0.2092, 0.2102, 0.2112, 0.2122, 0.2132, 0.2142, 0.2152,
        0.2162, 0.2172, 0.2182, 0.2192, 0.2202, 0.2212, 0.2222, 0.2232, 0.2242,
        0.2252, 0.2262, 0.2272, 0.2282, 0.2292, 0.2302, 0.2312, 0.2322, 0.2332,
        0.2342, 0.2352, 0.2362, 0.2372, 0.2382, 0.2392, 0.2402, 0.2412, 0.2422,
        0.2432, 0.2442, 0.2452, 0.2462, 0.2472, 0.2482, 0.2492, 0.2503, 0.2513,
        0.2523, 0.2533, 0.2543, 0.2553, 0.2563, 0.2573, 0.2583, 0.2593, 0.2603,
        0.2613, 0.2623, 0.2633, 0.2643, 0.2653, 0.2663, 0.2673, 0.2683, 0.2693,
        0.2703, 0.2713, 0.2723, 0.2733, 0.2743, 0.2753, 0.2763, 0.2773, 0.2783,
        0.2793, 0.2803, 0.2813, 0.2823, 0.2833, 0.2843, 0.2853, 0.2863, 0.2873,
        0.2883, 0.2893, 0.2903, 0.2913, 0.2923, 0.2933, 0.2943, 0.2953, 0.2963,
        0.2973, 0.2983, 0.2993, 0.3003, 0.3013, 0.3023, 0.3033, 0.3043, 0.3053,
        0.3063, 0.3073, 0.3083, 0.3093, 0.3103, 0.3113, 0.3123, 0.3133, 0.3143,
        0.3153, 0.3163, 0.3173, 0.3183, 0.3193, 0.3203, 0.3213, 0.3223, 0.3233,
        0.3243, 0.3253, 0.3263, 0.3273, 0.3283, 0.3293, 0.3303, 0.3313, 0.3323,
        0.3333, 0.3343, 0.3353, 0.3363, 0.3373, 0.3383, 0.3393, 0.3403, 0.3413,
        0.3423, 0.3433, 0.3443, 0.3453, 0.3463, 0.3473, 0.3483, 0.3493, 0.3504,
        0.3514, 0.3524, 0.3534, 0.3544, 0.3554, 0.3564, 0.3574, 0.3584, 0.3594,
        0.3604, 0.3614, 0.3624, 0.3634, 0.3644, 0.3654, 0.3664, 0.3674, 0.3684,
        0.3694, 0.3704, 0.3714, 0.3724, 0.3734, 0.3744, 0.3754, 0.3764, 0.3774,
        0.3784, 0.3794, 0.3804, 0.3814, 0.3824, 0.3834, 0.3844, 0.3854, 0.3864,
        0.3874, 0.3884, 0.3894, 0.3904, 0.3914, 0.3924, 0.3934, 0.3944, 0.3954,
        0.3964, 0.3974, 0.3984, 0.3994, 0.4004, 0.4014, 0.4024, 0.4034, 0.4044,
        0.4054, 0.4064, 0.4074, 0.4084, 0.4094, 0.4104, 0.4114, 0.4124, 0.4134,
        0.4144, 0.4154, 0.4164, 0.4174, 0.4184, 0.4194, 0.4204, 0.4214, 0.4224,
        0.4234, 0.4244, 0.4254, 0.4264, 0.4274, 0.4284, 0.4294, 0.4304, 0.4314,
        0.4324, 0.4334, 0.4344, 0.4354, 0.4364, 0.4374, 0.4384, 0.4394, 0.4404,
        0.4414, 0.4424, 0.4434, 0.4444, 0.4454, 0.4464, 0.4474, 0.4484, 0.4494,
        0.4505, 0.4515, 0.4525, 0.4535, 0.4545, 0.4555, 0.4565, 0.4575, 0.4585,
        0.4595, 0.4605, 0.4615, 0.4625, 0.4635, 0.4645, 0.4655, 0.4665, 0.4675,
        0.4685, 0.4695, 0.4705, 0.4715, 0.4725, 0.4735, 0.4745, 0.4755, 0.4765,
        0.4775, 0.4785, 0.4795, 0.4805, 0.4815, 0.4825, 0.4835, 0.4845, 0.4855,
        0.4865, 0.4875, 0.4885, 0.4895, 0.4905, 0.4915, 0.4925, 0.4935, 0.4945,
        0.4955, 0.4965, 0.4975, 0.4985, 0.4995, 0.5005, 0.5015, 0.5025, 0.5035,
        0.5045, 0.5055, 0.5065, 0.5075, 0.5085, 0.5095, 0.5105, 0.5115, 0.5125,
        0.5135, 0.5145, 0.5155, 0.5165, 0.5175, 0.5185, 0.5195, 0.5205, 0.5215,
        0.5225, 0.5235, 0.5245, 0.5255, 0.5265, 0.5275, 0.5285, 0.5295, 0.5305,
        0.5315, 0.5325, 0.5335, 0.5345, 0.5355, 0.5365, 0.5375, 0.5385, 0.5395,
        0.5405, 0.5415, 0.5425, 0.5435, 0.5445, 0.5455, 0.5465, 0.5475, 0.5485,
        0.5495, 0.5506, 0.5516, 0.5526, 0.5536, 0.5546, 0.5556, 0.5566, 0.5576,
        0.5586, 0.5596, 0.5606, 0.5616, 0.5626, 0.5636, 0.5646, 0.5656, 0.5666,
        0.5676, 0.5686, 0.5696, 0.5706, 0.5716, 0.5726, 0.5736, 0.5746, 0.5756,
        0.5766, 0.5776, 0.5786, 0.5796, 0.5806, 0.5816, 0.5826, 0.5836, 0.5846,
        0.5856, 0.5866, 0.5876, 0.5886, 0.5896, 0.5906, 0.5916, 0.5926, 0.5936,
        0.5946, 0.5956, 0.5966, 0.5976, 0.5986, 0.5996, 0.6006, 0.6016, 0.6026,
        0.6036, 0.6046, 0.6056, 0.6066, 0.6076, 0.6086, 0.6096, 0.6106, 0.6116,
        0.6126, 0.6136, 0.6146, 0.6156, 0.6166, 0.6176, 0.6186, 0.6196, 0.6206,
        0.6216, 0.6226, 0.6236, 0.6246, 0.6256, 0.6266, 0.6276, 0.6286, 0.6296,
        0.6306, 0.6316, 0.6326, 0.6336, 0.6346, 0.6356, 0.6366, 0.6376, 0.6386,
        0.6396, 0.6406, 0.6416, 0.6426, 0.6436, 0.6446, 0.6456, 0.6466, 0.6476,
        0.6486, 0.6496, 0.6507, 0.6517, 0.6527, 0.6537, 0.6547, 0.6557, 0.6567,
        0.6577, 0.6587, 0.6597, 0.6607, 0.6617, 0.6627, 0.6637, 0.6647, 0.6657,
        0.6667, 0.6677, 0.6687, 0.6697, 0.6707, 0.6717, 0.6727, 0.6737, 0.6747,
        0.6757, 0.6767, 0.6777, 0.6787, 0.6797, 0.6807, 0.6817, 0.6827, 0.6837,
        0.6847, 0.6857, 0.6867, 0.6877, 0.6887, 0.6897, 0.6907, 0.6917, 0.6927,
        0.6937, 0.6947, 0.6957, 0.6967, 0.6977, 0.6987, 0.6997, 0.7007, 0.7017,
        0.7027, 0.7037, 0.7047, 0.7057, 0.7067, 0.7077, 0.7087, 0.7097, 0.7107,
        0.7117, 0.7127, 0.7137, 0.7147, 0.7157, 0.7167, 0.7177, 0.7187, 0.7197,
        0.7207, 0.7217, 0.7227, 0.7237, 0.7247, 0.7257, 0.7267, 0.7277, 0.7287,
        0.7297, 0.7307, 0.7317, 0.7327, 0.7337, 0.7347, 0.7357, 0.7367, 0.7377,
        0.7387, 0.7397, 0.7407, 0.7417, 0.7427, 0.7437, 0.7447, 0.7457, 0.7467,
        0.7477, 0.7487, 0.7497, 0.7508, 0.7518, 0.7528, 0.7538, 0.7548, 0.7558,
        0.7568, 0.7578, 0.7588, 0.7598, 0.7608, 0.7618, 0.7628, 0.7638, 0.7648,
        0.7658, 0.7668, 0.7678, 0.7688, 0.7698, 0.7708, 0.7718, 0.7728, 0.7738,
        0.7748, 0.7758, 0.7768, 0.7778, 0.7788, 0.7798, 0.7808, 0.7818, 0.7828,
        0.7838, 0.7848, 0.7858, 0.7868, 0.7878, 0.7888, 0.7898, 0.7908, 0.7918,
        0.7928, 0.7938, 0.7948, 0.7958, 0.7968, 0.7978, 0.7988, 0.7998, 0.8008,
        0.8018, 0.8028, 0.8038, 0.8048, 0.8058, 0.8068, 0.8078, 0.8088, 0.8098,
        0.8108, 0.8118, 0.8128, 0.8138, 0.8148, 0.8158, 0.8168, 0.8178, 0.8188,
        0.8198, 0.8208, 0.8218, 0.8228, 0.8238, 0.8248, 0.8258, 0.8268, 0.8278,
        0.8288, 0.8298, 0.8308, 0.8318, 0.8328, 0.8338, 0.8348, 0.8358, 0.8368,
        0.8378, 0.8388, 0.8398, 0.8408, 0.8418, 0.8428, 0.8438, 0.8448, 0.8458,
        0.8468, 0.8478, 0.8488, 0.8498, 0.8509, 0.8519, 0.8529, 0.8539, 0.8549,
        0.8559, 0.8569, 0.8579, 0.8589, 0.8599, 0.8609, 0.8619, 0.8629, 0.8639,
        0.8649, 0.8659, 0.8669, 0.8679, 0.8689, 0.8699, 0.8709, 0.8719, 0.8729,
        0.8739, 0.8749, 0.8759, 0.8769, 0.8779, 0.8789, 0.8799, 0.8809, 0.8819,
        0.8829, 0.8839, 0.8849, 0.8859, 0.8869, 0.8879, 0.8889, 0.8899, 0.8909,
        0.8919, 0.8929, 0.8939, 0.8949, 0.8959, 0.8969, 0.8979, 0.8989, 0.8999,
        0.9009, 0.9019, 0.9029, 0.9039, 0.9049, 0.9059, 0.9069, 0.9079, 0.9089,
        0.9099, 0.9109, 0.9119, 0.9129, 0.9139, 0.9149, 0.9159, 0.9169, 0.9179,
        0.9189, 0.9199, 0.9209, 0.9219, 0.9229, 0.9239, 0.9249, 0.9259, 0.9269,
        0.9279, 0.9289, 0.9299, 0.9309, 0.9319, 0.9329, 0.9339, 0.9349, 0.9359,
        0.9369, 0.9379, 0.9389, 0.9399, 0.9409, 0.9419, 0.9429, 0.9439, 0.9449,
        0.9459, 0.9469, 0.9479, 0.9489, 0.9499, 0.9510, 0.9520, 0.9530, 0.9540,
        0.9550, 0.9560, 0.9570, 0.9580, 0.9590, 0.9600, 0.9610, 0.9620, 0.9630,
        0.9640, 0.9650, 0.9660, 0.9670, 0.9680, 0.9690, 0.9700, 0.9710, 0.9720,
        0.9730, 0.9740, 0.9750, 0.9760, 0.9770, 0.9780, 0.9790, 0.9800, 0.9810,
        0.9820, 0.9830, 0.9840, 0.9850, 0.9860, 0.9870, 0.9880, 0.9890, 0.9900,
        0.9910, 0.9920, 0.9930, 0.9940, 0.9950, 0.9960, 0.9970, 0.9980, 0.9990,
        1.0000], device='cuda:0') - Variance: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0') 
Smoothed_batch_entropy:  tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan], device='cuda:0')
Selected points (indices): {0}
Selected new x values (normalized): tensor([0.], device='cuda:0')
Corresponding new x values (unnormalized): tensor([-2000.], device='cuda:0')
Plot saved to /raven/u/dvoss/al_pmssmwithgp/model/plots/Iter18/gp_plot.png
YAML file has been created: /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/data/new_config.yaml
Note: detected 144 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
Note: NumExpr detected 144 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
NumExpr defaulting to 8 threads.

  ___              ____ __  __          _       _   ___            
 | _ \ _  _  _ _  |__ /|  \/  | ___  __| | ___ | | / __| ___  _ _  
 |   /| || || ' \  |_ \| |\/| |/ _ \/ _` |/ -_)| || (_ |/ -_)| ' \ 
 |_|_\ \_,_||_||_||___/|_|  |_|\___/\__,_|\___||_| \___|\___||_||_|
                                                                   

2024-08-28 20:03:25 [info     ] Initialised ModelGenerator with: [Run3ModelGen.modelgen]
2024-08-28 20:03:25 [info     ] 	config_file = /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/data/new_config.yaml [Run3ModelGen.modelgen]
2024-08-28 20:03:25 [info     ] 	scan_dir = /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/scans/scan_18 [Run3ModelGen.modelgen]
2024-08-28 20:03:25 [info     ] 	rawfilen = /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/data/raw.slha [Run3ModelGen.modelgen]
2024-08-28 20:03:25 [info     ] 	seed = 123                    [Run3ModelGen.modelgen]
2024-08-28 20:03:25 [info     ] 	points = {}                   [Run3ModelGen.modelgen]
2024-08-28 20:03:25 [info     ] 	prior = fixed                 [Run3ModelGen.modelgen]
2024-08-28 20:03:25 [info     ] 	num_models = 1                [Run3ModelGen.modelgen]
2024-08-28 20:03:25 [info     ] 	isGMSB = False                [Run3ModelGen.modelgen]
2024-08-28 20:03:25 [info     ] 	parameters =                  [Run3ModelGen.modelgen]
2024-08-28 20:03:25 [info     ] 		tanb: [60]                   [Run3ModelGen.modelgen]
2024-08-28 20:03:25 [info     ] 		M_1: [-2000.0]               [Run3ModelGen.modelgen]
2024-08-28 20:03:25 [info     ] 		M_2: [2000]                  [Run3ModelGen.modelgen]
2024-08-28 20:03:25 [info     ] 		M_3: [4000]                  [Run3ModelGen.modelgen]
2024-08-28 20:03:25 [info     ] 		AT: [4000]                   [Run3ModelGen.modelgen]
2024-08-28 20:03:25 [info     ] 		Ab: [2000]                   [Run3ModelGen.modelgen]
2024-08-28 20:03:25 [info     ] 		Atau: [2000]                 [Run3ModelGen.modelgen]
2024-08-28 20:03:25 [info     ] 		mu: [2000]                   [Run3ModelGen.modelgen]
2024-08-28 20:03:25 [info     ] 		mA: [2000]                   [Run3ModelGen.modelgen]
2024-08-28 20:03:25 [info     ] 		meL: [2000]                  [Run3ModelGen.modelgen]
2024-08-28 20:03:25 [info     ] 		mtauL: [2000]                [Run3ModelGen.modelgen]
2024-08-28 20:03:25 [info     ] 		meR: [2000]                  [Run3ModelGen.modelgen]
2024-08-28 20:03:25 [info     ] 		mtauR: [2000]                [Run3ModelGen.modelgen]
2024-08-28 20:03:25 [info     ] 		mqL1: [4000]                 [Run3ModelGen.modelgen]
2024-08-28 20:03:25 [info     ] 		mqL3: [4000]                 [Run3ModelGen.modelgen]
2024-08-28 20:03:25 [info     ] 		muR: [4000]                  [Run3ModelGen.modelgen]
2024-08-28 20:03:25 [info     ] 		mtR: [4000]                  [Run3ModelGen.modelgen]
2024-08-28 20:03:25 [info     ] 		mdR: [4000]                  [Run3ModelGen.modelgen]
2024-08-28 20:03:25 [info     ] 		mbR: [4000]                  [Run3ModelGen.modelgen]
2024-08-28 20:03:25 [info     ] 	steps =                       [Run3ModelGen.modelgen]
2024-08-28 20:03:25 [info     ] 		{'name': 'prep_input', 'output_dir': 'input', 'prefix': 'IN'} [Run3ModelGen.modelgen]
2024-08-28 20:03:25 [info     ] 		{'name': 'SPheno', 'input_dir': 'input', 'output_dir': 'SPheno', 'log_dir': 'SPheno_log', 'prefix': 'SP'} [Run3ModelGen.modelgen]
2024-08-28 20:03:25 [info     ] 		{'name': 'softsusy', 'input_dir': 'input', 'output_dir': 'softsusy', 'prefix': 'SS'} [Run3ModelGen.modelgen]
2024-08-28 20:03:25 [info     ] 		{'name': 'micromegas', 'input_dir': 'SPheno', 'output_dir': 'micromegas', 'prefix': 'MO'} [Run3ModelGen.modelgen]
2024-08-28 20:03:25 [info     ] 		{'name': 'superiso', 'input_dir': 'SPheno', 'output_dir': 'superiso', 'prefix': 'SI'} [Run3ModelGen.modelgen]
2024-08-28 20:03:25 [info     ] 		{'name': 'gm2calc', 'input_dir': 'SPheno', 'output_dir': 'gm2calc', 'prefix': 'GM2'} [Run3ModelGen.modelgen]
2024-08-28 20:03:25 [info     ] 		{'name': 'evade', 'input_dir': 'SPheno', 'output_dir': 'evade', 'prefix': 'EV'} [Run3ModelGen.modelgen]
2024-08-28 20:03:25 [info     ] Starting Model Generation...   [Run3ModelGen.modelgen]
2024-08-28 20:03:25 [info     ] Setting up output directories for scan [Run3ModelGen.modelgen]
2024-08-28 20:03:25 [info     ] Generating Model: 0.slha (1/1) [Run3ModelGen.modelgen]
2024-08-28 20:03:25 [info     ] 	running step: prep_input      [Run3ModelGen.modelgen]
2024-08-28 20:03:25 [info     ] 	running step: SPheno          [Run3ModelGen.modelgen]
2024-08-28 20:03:25 [info     ] 	running step: softsusy        [Run3ModelGen.modelgen]
2024-08-28 20:03:25 [info     ] 	running step: micromegas      [Run3ModelGen.modelgen]
2024-08-28 20:03:27 [info     ] 	running step: superiso        [Run3ModelGen.modelgen]
2024-08-28 20:03:27 [info     ] 	running step: gm2calc         [Run3ModelGen.modelgen]
2024-08-28 20:03:27 [info     ] 	running step: evade           [Run3ModelGen.modelgen]
2024-08-28 20:03:27 [info     ] Finished generating models! Stats for successful generations (successful/attempted): [Run3ModelGen.modelgen]
2024-08-28 20:03:27 [info     ] 	prep_input: 1/1.              [Run3ModelGen.modelgen]
2024-08-28 20:03:27 [info     ] 	SPheno: 1/1.                  [Run3ModelGen.modelgen]
2024-08-28 20:03:27 [info     ] 	softsusy: 1/1.                [Run3ModelGen.modelgen]
2024-08-28 20:03:27 [info     ] 	micromegas: 1/1.              [Run3ModelGen.modelgen]
2024-08-28 20:03:27 [info     ] 	superiso: 1/1.                [Run3ModelGen.modelgen]
2024-08-28 20:03:27 [info     ] 	gm2calc: 1/1.                 [Run3ModelGen.modelgen]
2024-08-28 20:03:27 [info     ] 	evade: 0/1.                   [Run3ModelGen.modelgen]
2024-08-28 20:03:27 [info     ] Will make NTuple for: /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/scans/scan_18, with 1 models [Run3ModelGen.ntupling]
2024-08-28 20:03:27 [info     ] 	Filling values for model: 0   [Run3ModelGen.ntupling]
2024-08-28 20:03:27 [info     ] Saving NTuple in /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/scans/scan_18/ntuple.0.0.root ... [Run3ModelGen.ntupling]
Completed iteration 18
Starting iteration 19
Initial Training points:  tensor([0.8528, 0.5812, 0.1617, 0.0046], device='cuda:0') torch.Size([4])
Training points after adding:  tensor([0.0000, 0.0140, 0.4324, 0.1617, 0.1091, 0.0046, 0.2132, 0.0561, 0.3373,
        0.2593, 0.4995, 0.2723, 0.5175, 0.2392, 0.5812, 0.0210, 0.8528],
       device='cuda:0') torch.Size([17])
These training_points are used in the GP tensor([0.0000, 0.0140, 0.4324, 0.1617, 0.1091, 0.0046, 0.2132, 0.0561, 0.3373,
        0.2593, 0.4995, 0.2723, 0.5175, 0.2392, 0.5812, 0.0210, 0.8528],
       device='cuda:0')
Point: tensor([0.0000, 0.0010, 0.0020, 0.0030, 0.0040, 0.0050, 0.0060, 0.0070, 0.0080,
        0.0090, 0.0100, 0.0110, 0.0120, 0.0130, 0.0140, 0.0150, 0.0160, 0.0170,
        0.0180, 0.0190, 0.0200, 0.0210, 0.0220, 0.0230, 0.0240, 0.0250, 0.0260,
        0.0270, 0.0280, 0.0290, 0.0300, 0.0310, 0.0320, 0.0330, 0.0340, 0.0350,
        0.0360, 0.0370, 0.0380, 0.0390, 0.0400, 0.0410, 0.0420, 0.0430, 0.0440,
        0.0450, 0.0460, 0.0470, 0.0480, 0.0490, 0.0501, 0.0511, 0.0521, 0.0531,
        0.0541, 0.0551, 0.0561, 0.0571, 0.0581, 0.0591, 0.0601, 0.0611, 0.0621,
        0.0631, 0.0641, 0.0651, 0.0661, 0.0671, 0.0681, 0.0691, 0.0701, 0.0711,
        0.0721, 0.0731, 0.0741, 0.0751, 0.0761, 0.0771, 0.0781, 0.0791, 0.0801,
        0.0811, 0.0821, 0.0831, 0.0841, 0.0851, 0.0861, 0.0871, 0.0881, 0.0891,
        0.0901, 0.0911, 0.0921, 0.0931, 0.0941, 0.0951, 0.0961, 0.0971, 0.0981,
        0.0991, 0.1001, 0.1011, 0.1021, 0.1031, 0.1041, 0.1051, 0.1061, 0.1071,
        0.1081, 0.1091, 0.1101, 0.1111, 0.1121, 0.1131, 0.1141, 0.1151, 0.1161,
        0.1171, 0.1181, 0.1191, 0.1201, 0.1211, 0.1221, 0.1231, 0.1241, 0.1251,
        0.1261, 0.1271, 0.1281, 0.1291, 0.1301, 0.1311, 0.1321, 0.1331, 0.1341,
        0.1351, 0.1361, 0.1371, 0.1381, 0.1391, 0.1401, 0.1411, 0.1421, 0.1431,
        0.1441, 0.1451, 0.1461, 0.1471, 0.1481, 0.1491, 0.1502, 0.1512, 0.1522,
        0.1532, 0.1542, 0.1552, 0.1562, 0.1572, 0.1582, 0.1592, 0.1602, 0.1612,
        0.1622, 0.1632, 0.1642, 0.1652, 0.1662, 0.1672, 0.1682, 0.1692, 0.1702,
        0.1712, 0.1722, 0.1732, 0.1742, 0.1752, 0.1762, 0.1772, 0.1782, 0.1792,
        0.1802, 0.1812, 0.1822, 0.1832, 0.1842, 0.1852, 0.1862, 0.1872, 0.1882,
        0.1892, 0.1902, 0.1912, 0.1922, 0.1932, 0.1942, 0.1952, 0.1962, 0.1972,
        0.1982, 0.1992, 0.2002, 0.2012, 0.2022, 0.2032, 0.2042, 0.2052, 0.2062,
        0.2072, 0.2082, 0.2092, 0.2102, 0.2112, 0.2122, 0.2132, 0.2142, 0.2152,
        0.2162, 0.2172, 0.2182, 0.2192, 0.2202, 0.2212, 0.2222, 0.2232, 0.2242,
        0.2252, 0.2262, 0.2272, 0.2282, 0.2292, 0.2302, 0.2312, 0.2322, 0.2332,
        0.2342, 0.2352, 0.2362, 0.2372, 0.2382, 0.2392, 0.2402, 0.2412, 0.2422,
        0.2432, 0.2442, 0.2452, 0.2462, 0.2472, 0.2482, 0.2492, 0.2503, 0.2513,
        0.2523, 0.2533, 0.2543, 0.2553, 0.2563, 0.2573, 0.2583, 0.2593, 0.2603,
        0.2613, 0.2623, 0.2633, 0.2643, 0.2653, 0.2663, 0.2673, 0.2683, 0.2693,
        0.2703, 0.2713, 0.2723, 0.2733, 0.2743, 0.2753, 0.2763, 0.2773, 0.2783,
        0.2793, 0.2803, 0.2813, 0.2823, 0.2833, 0.2843, 0.2853, 0.2863, 0.2873,
        0.2883, 0.2893, 0.2903, 0.2913, 0.2923, 0.2933, 0.2943, 0.2953, 0.2963,
        0.2973, 0.2983, 0.2993, 0.3003, 0.3013, 0.3023, 0.3033, 0.3043, 0.3053,
        0.3063, 0.3073, 0.3083, 0.3093, 0.3103, 0.3113, 0.3123, 0.3133, 0.3143,
        0.3153, 0.3163, 0.3173, 0.3183, 0.3193, 0.3203, 0.3213, 0.3223, 0.3233,
        0.3243, 0.3253, 0.3263, 0.3273, 0.3283, 0.3293, 0.3303, 0.3313, 0.3323,
        0.3333, 0.3343, 0.3353, 0.3363, 0.3373, 0.3383, 0.3393, 0.3403, 0.3413,
        0.3423, 0.3433, 0.3443, 0.3453, 0.3463, 0.3473, 0.3483, 0.3493, 0.3504,
        0.3514, 0.3524, 0.3534, 0.3544, 0.3554, 0.3564, 0.3574, 0.3584, 0.3594,
        0.3604, 0.3614, 0.3624, 0.3634, 0.3644, 0.3654, 0.3664, 0.3674, 0.3684,
        0.3694, 0.3704, 0.3714, 0.3724, 0.3734, 0.3744, 0.3754, 0.3764, 0.3774,
        0.3784, 0.3794, 0.3804, 0.3814, 0.3824, 0.3834, 0.3844, 0.3854, 0.3864,
        0.3874, 0.3884, 0.3894, 0.3904, 0.3914, 0.3924, 0.3934, 0.3944, 0.3954,
        0.3964, 0.3974, 0.3984, 0.3994, 0.4004, 0.4014, 0.4024, 0.4034, 0.4044,
        0.4054, 0.4064, 0.4074, 0.4084, 0.4094, 0.4104, 0.4114, 0.4124, 0.4134,
        0.4144, 0.4154, 0.4164, 0.4174, 0.4184, 0.4194, 0.4204, 0.4214, 0.4224,
        0.4234, 0.4244, 0.4254, 0.4264, 0.4274, 0.4284, 0.4294, 0.4304, 0.4314,
        0.4324, 0.4334, 0.4344, 0.4354, 0.4364, 0.4374, 0.4384, 0.4394, 0.4404,
        0.4414, 0.4424, 0.4434, 0.4444, 0.4454, 0.4464, 0.4474, 0.4484, 0.4494,
        0.4505, 0.4515, 0.4525, 0.4535, 0.4545, 0.4555, 0.4565, 0.4575, 0.4585,
        0.4595, 0.4605, 0.4615, 0.4625, 0.4635, 0.4645, 0.4655, 0.4665, 0.4675,
        0.4685, 0.4695, 0.4705, 0.4715, 0.4725, 0.4735, 0.4745, 0.4755, 0.4765,
        0.4775, 0.4785, 0.4795, 0.4805, 0.4815, 0.4825, 0.4835, 0.4845, 0.4855,
        0.4865, 0.4875, 0.4885, 0.4895, 0.4905, 0.4915, 0.4925, 0.4935, 0.4945,
        0.4955, 0.4965, 0.4975, 0.4985, 0.4995, 0.5005, 0.5015, 0.5025, 0.5035,
        0.5045, 0.5055, 0.5065, 0.5075, 0.5085, 0.5095, 0.5105, 0.5115, 0.5125,
        0.5135, 0.5145, 0.5155, 0.5165, 0.5175, 0.5185, 0.5195, 0.5205, 0.5215,
        0.5225, 0.5235, 0.5245, 0.5255, 0.5265, 0.5275, 0.5285, 0.5295, 0.5305,
        0.5315, 0.5325, 0.5335, 0.5345, 0.5355, 0.5365, 0.5375, 0.5385, 0.5395,
        0.5405, 0.5415, 0.5425, 0.5435, 0.5445, 0.5455, 0.5465, 0.5475, 0.5485,
        0.5495, 0.5506, 0.5516, 0.5526, 0.5536, 0.5546, 0.5556, 0.5566, 0.5576,
        0.5586, 0.5596, 0.5606, 0.5616, 0.5626, 0.5636, 0.5646, 0.5656, 0.5666,
        0.5676, 0.5686, 0.5696, 0.5706, 0.5716, 0.5726, 0.5736, 0.5746, 0.5756,
        0.5766, 0.5776, 0.5786, 0.5796, 0.5806, 0.5816, 0.5826, 0.5836, 0.5846,
        0.5856, 0.5866, 0.5876, 0.5886, 0.5896, 0.5906, 0.5916, 0.5926, 0.5936,
        0.5946, 0.5956, 0.5966, 0.5976, 0.5986, 0.5996, 0.6006, 0.6016, 0.6026,
        0.6036, 0.6046, 0.6056, 0.6066, 0.6076, 0.6086, 0.6096, 0.6106, 0.6116,
        0.6126, 0.6136, 0.6146, 0.6156, 0.6166, 0.6176, 0.6186, 0.6196, 0.6206,
        0.6216, 0.6226, 0.6236, 0.6246, 0.6256, 0.6266, 0.6276, 0.6286, 0.6296,
        0.6306, 0.6316, 0.6326, 0.6336, 0.6346, 0.6356, 0.6366, 0.6376, 0.6386,
        0.6396, 0.6406, 0.6416, 0.6426, 0.6436, 0.6446, 0.6456, 0.6466, 0.6476,
        0.6486, 0.6496, 0.6507, 0.6517, 0.6527, 0.6537, 0.6547, 0.6557, 0.6567,
        0.6577, 0.6587, 0.6597, 0.6607, 0.6617, 0.6627, 0.6637, 0.6647, 0.6657,
        0.6667, 0.6677, 0.6687, 0.6697, 0.6707, 0.6717, 0.6727, 0.6737, 0.6747,
        0.6757, 0.6767, 0.6777, 0.6787, 0.6797, 0.6807, 0.6817, 0.6827, 0.6837,
        0.6847, 0.6857, 0.6867, 0.6877, 0.6887, 0.6897, 0.6907, 0.6917, 0.6927,
        0.6937, 0.6947, 0.6957, 0.6967, 0.6977, 0.6987, 0.6997, 0.7007, 0.7017,
        0.7027, 0.7037, 0.7047, 0.7057, 0.7067, 0.7077, 0.7087, 0.7097, 0.7107,
        0.7117, 0.7127, 0.7137, 0.7147, 0.7157, 0.7167, 0.7177, 0.7187, 0.7197,
        0.7207, 0.7217, 0.7227, 0.7237, 0.7247, 0.7257, 0.7267, 0.7277, 0.7287,
        0.7297, 0.7307, 0.7317, 0.7327, 0.7337, 0.7347, 0.7357, 0.7367, 0.7377,
        0.7387, 0.7397, 0.7407, 0.7417, 0.7427, 0.7437, 0.7447, 0.7457, 0.7467,
        0.7477, 0.7487, 0.7497, 0.7508, 0.7518, 0.7528, 0.7538, 0.7548, 0.7558,
        0.7568, 0.7578, 0.7588, 0.7598, 0.7608, 0.7618, 0.7628, 0.7638, 0.7648,
        0.7658, 0.7668, 0.7678, 0.7688, 0.7698, 0.7708, 0.7718, 0.7728, 0.7738,
        0.7748, 0.7758, 0.7768, 0.7778, 0.7788, 0.7798, 0.7808, 0.7818, 0.7828,
        0.7838, 0.7848, 0.7858, 0.7868, 0.7878, 0.7888, 0.7898, 0.7908, 0.7918,
        0.7928, 0.7938, 0.7948, 0.7958, 0.7968, 0.7978, 0.7988, 0.7998, 0.8008,
        0.8018, 0.8028, 0.8038, 0.8048, 0.8058, 0.8068, 0.8078, 0.8088, 0.8098,
        0.8108, 0.8118, 0.8128, 0.8138, 0.8148, 0.8158, 0.8168, 0.8178, 0.8188,
        0.8198, 0.8208, 0.8218, 0.8228, 0.8238, 0.8248, 0.8258, 0.8268, 0.8278,
        0.8288, 0.8298, 0.8308, 0.8318, 0.8328, 0.8338, 0.8348, 0.8358, 0.8368,
        0.8378, 0.8388, 0.8398, 0.8408, 0.8418, 0.8428, 0.8438, 0.8448, 0.8458,
        0.8468, 0.8478, 0.8488, 0.8498, 0.8509, 0.8519, 0.8529, 0.8539, 0.8549,
        0.8559, 0.8569, 0.8579, 0.8589, 0.8599, 0.8609, 0.8619, 0.8629, 0.8639,
        0.8649, 0.8659, 0.8669, 0.8679, 0.8689, 0.8699, 0.8709, 0.8719, 0.8729,
        0.8739, 0.8749, 0.8759, 0.8769, 0.8779, 0.8789, 0.8799, 0.8809, 0.8819,
        0.8829, 0.8839, 0.8849, 0.8859, 0.8869, 0.8879, 0.8889, 0.8899, 0.8909,
        0.8919, 0.8929, 0.8939, 0.8949, 0.8959, 0.8969, 0.8979, 0.8989, 0.8999,
        0.9009, 0.9019, 0.9029, 0.9039, 0.9049, 0.9059, 0.9069, 0.9079, 0.9089,
        0.9099, 0.9109, 0.9119, 0.9129, 0.9139, 0.9149, 0.9159, 0.9169, 0.9179,
        0.9189, 0.9199, 0.9209, 0.9219, 0.9229, 0.9239, 0.9249, 0.9259, 0.9269,
        0.9279, 0.9289, 0.9299, 0.9309, 0.9319, 0.9329, 0.9339, 0.9349, 0.9359,
        0.9369, 0.9379, 0.9389, 0.9399, 0.9409, 0.9419, 0.9429, 0.9439, 0.9449,
        0.9459, 0.9469, 0.9479, 0.9489, 0.9499, 0.9510, 0.9520, 0.9530, 0.9540,
        0.9550, 0.9560, 0.9570, 0.9580, 0.9590, 0.9600, 0.9610, 0.9620, 0.9630,
        0.9640, 0.9650, 0.9660, 0.9670, 0.9680, 0.9690, 0.9700, 0.9710, 0.9720,
        0.9730, 0.9740, 0.9750, 0.9760, 0.9770, 0.9780, 0.9790, 0.9800, 0.9810,
        0.9820, 0.9830, 0.9840, 0.9850, 0.9860, 0.9870, 0.9880, 0.9890, 0.9900,
        0.9910, 0.9920, 0.9930, 0.9940, 0.9950, 0.9960, 0.9970, 0.9980, 0.9990,
        1.0000], device='cuda:0') - Variance: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0') 
Smoothed_batch_entropy:  tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan], device='cuda:0')
Selected points (indices): {0}
Selected new x values (normalized): tensor([0.], device='cuda:0')
Corresponding new x values (unnormalized): tensor([-2000.], device='cuda:0')
Plot saved to /raven/u/dvoss/al_pmssmwithgp/model/plots/Iter19/gp_plot.png
YAML file has been created: /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/data/new_config.yaml
Note: detected 144 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
Note: NumExpr detected 144 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
NumExpr defaulting to 8 threads.

  ___              ____ __  __          _       _   ___            
 | _ \ _  _  _ _  |__ /|  \/  | ___  __| | ___ | | / __| ___  _ _  
 |   /| || || ' \  |_ \| |\/| |/ _ \/ _` |/ -_)| || (_ |/ -_)| ' \ 
 |_|_\ \_,_||_||_||___/|_|  |_|\___/\__,_|\___||_| \___|\___||_||_|
                                                                   

2024-08-28 20:04:12 [info     ] Initialised ModelGenerator with: [Run3ModelGen.modelgen]
2024-08-28 20:04:12 [info     ] 	config_file = /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/data/new_config.yaml [Run3ModelGen.modelgen]
2024-08-28 20:04:12 [info     ] 	scan_dir = /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/scans/scan_19 [Run3ModelGen.modelgen]
2024-08-28 20:04:12 [info     ] 	rawfilen = /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/data/raw.slha [Run3ModelGen.modelgen]
2024-08-28 20:04:12 [info     ] 	seed = 123                    [Run3ModelGen.modelgen]
2024-08-28 20:04:12 [info     ] 	points = {}                   [Run3ModelGen.modelgen]
2024-08-28 20:04:12 [info     ] 	prior = fixed                 [Run3ModelGen.modelgen]
2024-08-28 20:04:12 [info     ] 	num_models = 1                [Run3ModelGen.modelgen]
2024-08-28 20:04:12 [info     ] 	isGMSB = False                [Run3ModelGen.modelgen]
2024-08-28 20:04:12 [info     ] 	parameters =                  [Run3ModelGen.modelgen]
2024-08-28 20:04:12 [info     ] 		tanb: [60]                   [Run3ModelGen.modelgen]
2024-08-28 20:04:12 [info     ] 		M_1: [-2000.0]               [Run3ModelGen.modelgen]
2024-08-28 20:04:12 [info     ] 		M_2: [2000]                  [Run3ModelGen.modelgen]
2024-08-28 20:04:12 [info     ] 		M_3: [4000]                  [Run3ModelGen.modelgen]
2024-08-28 20:04:12 [info     ] 		AT: [4000]                   [Run3ModelGen.modelgen]
2024-08-28 20:04:12 [info     ] 		Ab: [2000]                   [Run3ModelGen.modelgen]
2024-08-28 20:04:12 [info     ] 		Atau: [2000]                 [Run3ModelGen.modelgen]
2024-08-28 20:04:12 [info     ] 		mu: [2000]                   [Run3ModelGen.modelgen]
2024-08-28 20:04:12 [info     ] 		mA: [2000]                   [Run3ModelGen.modelgen]
2024-08-28 20:04:12 [info     ] 		meL: [2000]                  [Run3ModelGen.modelgen]
2024-08-28 20:04:12 [info     ] 		mtauL: [2000]                [Run3ModelGen.modelgen]
2024-08-28 20:04:12 [info     ] 		meR: [2000]                  [Run3ModelGen.modelgen]
2024-08-28 20:04:12 [info     ] 		mtauR: [2000]                [Run3ModelGen.modelgen]
2024-08-28 20:04:12 [info     ] 		mqL1: [4000]                 [Run3ModelGen.modelgen]
2024-08-28 20:04:12 [info     ] 		mqL3: [4000]                 [Run3ModelGen.modelgen]
2024-08-28 20:04:12 [info     ] 		muR: [4000]                  [Run3ModelGen.modelgen]
2024-08-28 20:04:12 [info     ] 		mtR: [4000]                  [Run3ModelGen.modelgen]
2024-08-28 20:04:12 [info     ] 		mdR: [4000]                  [Run3ModelGen.modelgen]
2024-08-28 20:04:12 [info     ] 		mbR: [4000]                  [Run3ModelGen.modelgen]
2024-08-28 20:04:12 [info     ] 	steps =                       [Run3ModelGen.modelgen]
2024-08-28 20:04:12 [info     ] 		{'name': 'prep_input', 'output_dir': 'input', 'prefix': 'IN'} [Run3ModelGen.modelgen]
2024-08-28 20:04:12 [info     ] 		{'name': 'SPheno', 'input_dir': 'input', 'output_dir': 'SPheno', 'log_dir': 'SPheno_log', 'prefix': 'SP'} [Run3ModelGen.modelgen]
2024-08-28 20:04:12 [info     ] 		{'name': 'softsusy', 'input_dir': 'input', 'output_dir': 'softsusy', 'prefix': 'SS'} [Run3ModelGen.modelgen]
2024-08-28 20:04:12 [info     ] 		{'name': 'micromegas', 'input_dir': 'SPheno', 'output_dir': 'micromegas', 'prefix': 'MO'} [Run3ModelGen.modelgen]
2024-08-28 20:04:12 [info     ] 		{'name': 'superiso', 'input_dir': 'SPheno', 'output_dir': 'superiso', 'prefix': 'SI'} [Run3ModelGen.modelgen]
2024-08-28 20:04:12 [info     ] 		{'name': 'gm2calc', 'input_dir': 'SPheno', 'output_dir': 'gm2calc', 'prefix': 'GM2'} [Run3ModelGen.modelgen]
2024-08-28 20:04:12 [info     ] 		{'name': 'evade', 'input_dir': 'SPheno', 'output_dir': 'evade', 'prefix': 'EV'} [Run3ModelGen.modelgen]
2024-08-28 20:04:12 [info     ] Starting Model Generation...   [Run3ModelGen.modelgen]
2024-08-28 20:04:12 [info     ] Setting up output directories for scan [Run3ModelGen.modelgen]
2024-08-28 20:04:12 [info     ] Generating Model: 0.slha (1/1) [Run3ModelGen.modelgen]
2024-08-28 20:04:12 [info     ] 	running step: prep_input      [Run3ModelGen.modelgen]
2024-08-28 20:04:12 [info     ] 	running step: SPheno          [Run3ModelGen.modelgen]
2024-08-28 20:04:12 [info     ] 	running step: softsusy        [Run3ModelGen.modelgen]
2024-08-28 20:04:12 [info     ] 	running step: micromegas      [Run3ModelGen.modelgen]
2024-08-28 20:04:13 [info     ] 	running step: superiso        [Run3ModelGen.modelgen]
2024-08-28 20:04:13 [info     ] 	running step: gm2calc         [Run3ModelGen.modelgen]
2024-08-28 20:04:13 [info     ] 	running step: evade           [Run3ModelGen.modelgen]
2024-08-28 20:04:13 [info     ] Finished generating models! Stats for successful generations (successful/attempted): [Run3ModelGen.modelgen]
2024-08-28 20:04:13 [info     ] 	prep_input: 1/1.              [Run3ModelGen.modelgen]
2024-08-28 20:04:13 [info     ] 	SPheno: 1/1.                  [Run3ModelGen.modelgen]
2024-08-28 20:04:13 [info     ] 	softsusy: 1/1.                [Run3ModelGen.modelgen]
2024-08-28 20:04:13 [info     ] 	micromegas: 1/1.              [Run3ModelGen.modelgen]
2024-08-28 20:04:13 [info     ] 	superiso: 1/1.                [Run3ModelGen.modelgen]
2024-08-28 20:04:13 [info     ] 	gm2calc: 1/1.                 [Run3ModelGen.modelgen]
2024-08-28 20:04:13 [info     ] 	evade: 0/1.                   [Run3ModelGen.modelgen]
2024-08-28 20:04:13 [info     ] Will make NTuple for: /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/scans/scan_19, with 1 models [Run3ModelGen.ntupling]
2024-08-28 20:04:13 [info     ] 	Filling values for model: 0   [Run3ModelGen.ntupling]
2024-08-28 20:04:13 [info     ] Saving NTuple in /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/scans/scan_19/ntuple.0.0.root ... [Run3ModelGen.ntupling]
Completed iteration 19
Starting iteration 20
Initial Training points:  tensor([0.8528, 0.5812, 0.1617, 0.0046], device='cuda:0') torch.Size([4])
Training points after adding:  tensor([0.0000, 0.0140, 0.4324, 0.1617, 0.1091, 0.0046, 0.2132, 0.0561, 0.3373,
        0.2593, 0.4995, 0.2723, 0.5175, 0.2392, 0.5812, 0.0210, 0.8528],
       device='cuda:0') torch.Size([17])
These training_points are used in the GP tensor([0.0000, 0.0140, 0.4324, 0.1617, 0.1091, 0.0046, 0.2132, 0.0561, 0.3373,
        0.2593, 0.4995, 0.2723, 0.5175, 0.2392, 0.5812, 0.0210, 0.8528],
       device='cuda:0')
Point: tensor([0.0000, 0.0010, 0.0020, 0.0030, 0.0040, 0.0050, 0.0060, 0.0070, 0.0080,
        0.0090, 0.0100, 0.0110, 0.0120, 0.0130, 0.0140, 0.0150, 0.0160, 0.0170,
        0.0180, 0.0190, 0.0200, 0.0210, 0.0220, 0.0230, 0.0240, 0.0250, 0.0260,
        0.0270, 0.0280, 0.0290, 0.0300, 0.0310, 0.0320, 0.0330, 0.0340, 0.0350,
        0.0360, 0.0370, 0.0380, 0.0390, 0.0400, 0.0410, 0.0420, 0.0430, 0.0440,
        0.0450, 0.0460, 0.0470, 0.0480, 0.0490, 0.0501, 0.0511, 0.0521, 0.0531,
        0.0541, 0.0551, 0.0561, 0.0571, 0.0581, 0.0591, 0.0601, 0.0611, 0.0621,
        0.0631, 0.0641, 0.0651, 0.0661, 0.0671, 0.0681, 0.0691, 0.0701, 0.0711,
        0.0721, 0.0731, 0.0741, 0.0751, 0.0761, 0.0771, 0.0781, 0.0791, 0.0801,
        0.0811, 0.0821, 0.0831, 0.0841, 0.0851, 0.0861, 0.0871, 0.0881, 0.0891,
        0.0901, 0.0911, 0.0921, 0.0931, 0.0941, 0.0951, 0.0961, 0.0971, 0.0981,
        0.0991, 0.1001, 0.1011, 0.1021, 0.1031, 0.1041, 0.1051, 0.1061, 0.1071,
        0.1081, 0.1091, 0.1101, 0.1111, 0.1121, 0.1131, 0.1141, 0.1151, 0.1161,
        0.1171, 0.1181, 0.1191, 0.1201, 0.1211, 0.1221, 0.1231, 0.1241, 0.1251,
        0.1261, 0.1271, 0.1281, 0.1291, 0.1301, 0.1311, 0.1321, 0.1331, 0.1341,
        0.1351, 0.1361, 0.1371, 0.1381, 0.1391, 0.1401, 0.1411, 0.1421, 0.1431,
        0.1441, 0.1451, 0.1461, 0.1471, 0.1481, 0.1491, 0.1502, 0.1512, 0.1522,
        0.1532, 0.1542, 0.1552, 0.1562, 0.1572, 0.1582, 0.1592, 0.1602, 0.1612,
        0.1622, 0.1632, 0.1642, 0.1652, 0.1662, 0.1672, 0.1682, 0.1692, 0.1702,
        0.1712, 0.1722, 0.1732, 0.1742, 0.1752, 0.1762, 0.1772, 0.1782, 0.1792,
        0.1802, 0.1812, 0.1822, 0.1832, 0.1842, 0.1852, 0.1862, 0.1872, 0.1882,
        0.1892, 0.1902, 0.1912, 0.1922, 0.1932, 0.1942, 0.1952, 0.1962, 0.1972,
        0.1982, 0.1992, 0.2002, 0.2012, 0.2022, 0.2032, 0.2042, 0.2052, 0.2062,
        0.2072, 0.2082, 0.2092, 0.2102, 0.2112, 0.2122, 0.2132, 0.2142, 0.2152,
        0.2162, 0.2172, 0.2182, 0.2192, 0.2202, 0.2212, 0.2222, 0.2232, 0.2242,
        0.2252, 0.2262, 0.2272, 0.2282, 0.2292, 0.2302, 0.2312, 0.2322, 0.2332,
        0.2342, 0.2352, 0.2362, 0.2372, 0.2382, 0.2392, 0.2402, 0.2412, 0.2422,
        0.2432, 0.2442, 0.2452, 0.2462, 0.2472, 0.2482, 0.2492, 0.2503, 0.2513,
        0.2523, 0.2533, 0.2543, 0.2553, 0.2563, 0.2573, 0.2583, 0.2593, 0.2603,
        0.2613, 0.2623, 0.2633, 0.2643, 0.2653, 0.2663, 0.2673, 0.2683, 0.2693,
        0.2703, 0.2713, 0.2723, 0.2733, 0.2743, 0.2753, 0.2763, 0.2773, 0.2783,
        0.2793, 0.2803, 0.2813, 0.2823, 0.2833, 0.2843, 0.2853, 0.2863, 0.2873,
        0.2883, 0.2893, 0.2903, 0.2913, 0.2923, 0.2933, 0.2943, 0.2953, 0.2963,
        0.2973, 0.2983, 0.2993, 0.3003, 0.3013, 0.3023, 0.3033, 0.3043, 0.3053,
        0.3063, 0.3073, 0.3083, 0.3093, 0.3103, 0.3113, 0.3123, 0.3133, 0.3143,
        0.3153, 0.3163, 0.3173, 0.3183, 0.3193, 0.3203, 0.3213, 0.3223, 0.3233,
        0.3243, 0.3253, 0.3263, 0.3273, 0.3283, 0.3293, 0.3303, 0.3313, 0.3323,
        0.3333, 0.3343, 0.3353, 0.3363, 0.3373, 0.3383, 0.3393, 0.3403, 0.3413,
        0.3423, 0.3433, 0.3443, 0.3453, 0.3463, 0.3473, 0.3483, 0.3493, 0.3504,
        0.3514, 0.3524, 0.3534, 0.3544, 0.3554, 0.3564, 0.3574, 0.3584, 0.3594,
        0.3604, 0.3614, 0.3624, 0.3634, 0.3644, 0.3654, 0.3664, 0.3674, 0.3684,
        0.3694, 0.3704, 0.3714, 0.3724, 0.3734, 0.3744, 0.3754, 0.3764, 0.3774,
        0.3784, 0.3794, 0.3804, 0.3814, 0.3824, 0.3834, 0.3844, 0.3854, 0.3864,
        0.3874, 0.3884, 0.3894, 0.3904, 0.3914, 0.3924, 0.3934, 0.3944, 0.3954,
        0.3964, 0.3974, 0.3984, 0.3994, 0.4004, 0.4014, 0.4024, 0.4034, 0.4044,
        0.4054, 0.4064, 0.4074, 0.4084, 0.4094, 0.4104, 0.4114, 0.4124, 0.4134,
        0.4144, 0.4154, 0.4164, 0.4174, 0.4184, 0.4194, 0.4204, 0.4214, 0.4224,
        0.4234, 0.4244, 0.4254, 0.4264, 0.4274, 0.4284, 0.4294, 0.4304, 0.4314,
        0.4324, 0.4334, 0.4344, 0.4354, 0.4364, 0.4374, 0.4384, 0.4394, 0.4404,
        0.4414, 0.4424, 0.4434, 0.4444, 0.4454, 0.4464, 0.4474, 0.4484, 0.4494,
        0.4505, 0.4515, 0.4525, 0.4535, 0.4545, 0.4555, 0.4565, 0.4575, 0.4585,
        0.4595, 0.4605, 0.4615, 0.4625, 0.4635, 0.4645, 0.4655, 0.4665, 0.4675,
        0.4685, 0.4695, 0.4705, 0.4715, 0.4725, 0.4735, 0.4745, 0.4755, 0.4765,
        0.4775, 0.4785, 0.4795, 0.4805, 0.4815, 0.4825, 0.4835, 0.4845, 0.4855,
        0.4865, 0.4875, 0.4885, 0.4895, 0.4905, 0.4915, 0.4925, 0.4935, 0.4945,
        0.4955, 0.4965, 0.4975, 0.4985, 0.4995, 0.5005, 0.5015, 0.5025, 0.5035,
        0.5045, 0.5055, 0.5065, 0.5075, 0.5085, 0.5095, 0.5105, 0.5115, 0.5125,
        0.5135, 0.5145, 0.5155, 0.5165, 0.5175, 0.5185, 0.5195, 0.5205, 0.5215,
        0.5225, 0.5235, 0.5245, 0.5255, 0.5265, 0.5275, 0.5285, 0.5295, 0.5305,
        0.5315, 0.5325, 0.5335, 0.5345, 0.5355, 0.5365, 0.5375, 0.5385, 0.5395,
        0.5405, 0.5415, 0.5425, 0.5435, 0.5445, 0.5455, 0.5465, 0.5475, 0.5485,
        0.5495, 0.5506, 0.5516, 0.5526, 0.5536, 0.5546, 0.5556, 0.5566, 0.5576,
        0.5586, 0.5596, 0.5606, 0.5616, 0.5626, 0.5636, 0.5646, 0.5656, 0.5666,
        0.5676, 0.5686, 0.5696, 0.5706, 0.5716, 0.5726, 0.5736, 0.5746, 0.5756,
        0.5766, 0.5776, 0.5786, 0.5796, 0.5806, 0.5816, 0.5826, 0.5836, 0.5846,
        0.5856, 0.5866, 0.5876, 0.5886, 0.5896, 0.5906, 0.5916, 0.5926, 0.5936,
        0.5946, 0.5956, 0.5966, 0.5976, 0.5986, 0.5996, 0.6006, 0.6016, 0.6026,
        0.6036, 0.6046, 0.6056, 0.6066, 0.6076, 0.6086, 0.6096, 0.6106, 0.6116,
        0.6126, 0.6136, 0.6146, 0.6156, 0.6166, 0.6176, 0.6186, 0.6196, 0.6206,
        0.6216, 0.6226, 0.6236, 0.6246, 0.6256, 0.6266, 0.6276, 0.6286, 0.6296,
        0.6306, 0.6316, 0.6326, 0.6336, 0.6346, 0.6356, 0.6366, 0.6376, 0.6386,
        0.6396, 0.6406, 0.6416, 0.6426, 0.6436, 0.6446, 0.6456, 0.6466, 0.6476,
        0.6486, 0.6496, 0.6507, 0.6517, 0.6527, 0.6537, 0.6547, 0.6557, 0.6567,
        0.6577, 0.6587, 0.6597, 0.6607, 0.6617, 0.6627, 0.6637, 0.6647, 0.6657,
        0.6667, 0.6677, 0.6687, 0.6697, 0.6707, 0.6717, 0.6727, 0.6737, 0.6747,
        0.6757, 0.6767, 0.6777, 0.6787, 0.6797, 0.6807, 0.6817, 0.6827, 0.6837,
        0.6847, 0.6857, 0.6867, 0.6877, 0.6887, 0.6897, 0.6907, 0.6917, 0.6927,
        0.6937, 0.6947, 0.6957, 0.6967, 0.6977, 0.6987, 0.6997, 0.7007, 0.7017,
        0.7027, 0.7037, 0.7047, 0.7057, 0.7067, 0.7077, 0.7087, 0.7097, 0.7107,
        0.7117, 0.7127, 0.7137, 0.7147, 0.7157, 0.7167, 0.7177, 0.7187, 0.7197,
        0.7207, 0.7217, 0.7227, 0.7237, 0.7247, 0.7257, 0.7267, 0.7277, 0.7287,
        0.7297, 0.7307, 0.7317, 0.7327, 0.7337, 0.7347, 0.7357, 0.7367, 0.7377,
        0.7387, 0.7397, 0.7407, 0.7417, 0.7427, 0.7437, 0.7447, 0.7457, 0.7467,
        0.7477, 0.7487, 0.7497, 0.7508, 0.7518, 0.7528, 0.7538, 0.7548, 0.7558,
        0.7568, 0.7578, 0.7588, 0.7598, 0.7608, 0.7618, 0.7628, 0.7638, 0.7648,
        0.7658, 0.7668, 0.7678, 0.7688, 0.7698, 0.7708, 0.7718, 0.7728, 0.7738,
        0.7748, 0.7758, 0.7768, 0.7778, 0.7788, 0.7798, 0.7808, 0.7818, 0.7828,
        0.7838, 0.7848, 0.7858, 0.7868, 0.7878, 0.7888, 0.7898, 0.7908, 0.7918,
        0.7928, 0.7938, 0.7948, 0.7958, 0.7968, 0.7978, 0.7988, 0.7998, 0.8008,
        0.8018, 0.8028, 0.8038, 0.8048, 0.8058, 0.8068, 0.8078, 0.8088, 0.8098,
        0.8108, 0.8118, 0.8128, 0.8138, 0.8148, 0.8158, 0.8168, 0.8178, 0.8188,
        0.8198, 0.8208, 0.8218, 0.8228, 0.8238, 0.8248, 0.8258, 0.8268, 0.8278,
        0.8288, 0.8298, 0.8308, 0.8318, 0.8328, 0.8338, 0.8348, 0.8358, 0.8368,
        0.8378, 0.8388, 0.8398, 0.8408, 0.8418, 0.8428, 0.8438, 0.8448, 0.8458,
        0.8468, 0.8478, 0.8488, 0.8498, 0.8509, 0.8519, 0.8529, 0.8539, 0.8549,
        0.8559, 0.8569, 0.8579, 0.8589, 0.8599, 0.8609, 0.8619, 0.8629, 0.8639,
        0.8649, 0.8659, 0.8669, 0.8679, 0.8689, 0.8699, 0.8709, 0.8719, 0.8729,
        0.8739, 0.8749, 0.8759, 0.8769, 0.8779, 0.8789, 0.8799, 0.8809, 0.8819,
        0.8829, 0.8839, 0.8849, 0.8859, 0.8869, 0.8879, 0.8889, 0.8899, 0.8909,
        0.8919, 0.8929, 0.8939, 0.8949, 0.8959, 0.8969, 0.8979, 0.8989, 0.8999,
        0.9009, 0.9019, 0.9029, 0.9039, 0.9049, 0.9059, 0.9069, 0.9079, 0.9089,
        0.9099, 0.9109, 0.9119, 0.9129, 0.9139, 0.9149, 0.9159, 0.9169, 0.9179,
        0.9189, 0.9199, 0.9209, 0.9219, 0.9229, 0.9239, 0.9249, 0.9259, 0.9269,
        0.9279, 0.9289, 0.9299, 0.9309, 0.9319, 0.9329, 0.9339, 0.9349, 0.9359,
        0.9369, 0.9379, 0.9389, 0.9399, 0.9409, 0.9419, 0.9429, 0.9439, 0.9449,
        0.9459, 0.9469, 0.9479, 0.9489, 0.9499, 0.9510, 0.9520, 0.9530, 0.9540,
        0.9550, 0.9560, 0.9570, 0.9580, 0.9590, 0.9600, 0.9610, 0.9620, 0.9630,
        0.9640, 0.9650, 0.9660, 0.9670, 0.9680, 0.9690, 0.9700, 0.9710, 0.9720,
        0.9730, 0.9740, 0.9750, 0.9760, 0.9770, 0.9780, 0.9790, 0.9800, 0.9810,
        0.9820, 0.9830, 0.9840, 0.9850, 0.9860, 0.9870, 0.9880, 0.9890, 0.9900,
        0.9910, 0.9920, 0.9930, 0.9940, 0.9950, 0.9960, 0.9970, 0.9980, 0.9990,
        1.0000], device='cuda:0') - Variance: tensor([[nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan],
        [nan]], device='cuda:0') 
Smoothed_batch_entropy:  tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan], device='cuda:0')
Selected points (indices): {0}
Selected new x values (normalized): tensor([0.], device='cuda:0')
Corresponding new x values (unnormalized): tensor([-2000.], device='cuda:0')
Plot saved to /raven/u/dvoss/al_pmssmwithgp/model/plots/Iter20/gp_plot.png
YAML file has been created: /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/data/new_config.yaml
Note: detected 144 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
Note: NumExpr detected 144 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
NumExpr defaulting to 8 threads.

  ___              ____ __  __          _       _   ___            
 | _ \ _  _  _ _  |__ /|  \/  | ___  __| | ___ | | / __| ___  _ _  
 |   /| || || ' \  |_ \| |\/| |/ _ \/ _` |/ -_)| || (_ |/ -_)| ' \ 
 |_|_\ \_,_||_||_||___/|_|  |_|\___/\__,_|\___||_| \___|\___||_||_|
                                                                   

2024-08-28 20:04:58 [info     ] Initialised ModelGenerator with: [Run3ModelGen.modelgen]
2024-08-28 20:04:58 [info     ] 	config_file = /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/data/new_config.yaml [Run3ModelGen.modelgen]
2024-08-28 20:04:58 [info     ] 	scan_dir = /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/scans/scan_20 [Run3ModelGen.modelgen]
2024-08-28 20:04:58 [info     ] 	rawfilen = /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/data/raw.slha [Run3ModelGen.modelgen]
2024-08-28 20:04:58 [info     ] 	seed = 123                    [Run3ModelGen.modelgen]
2024-08-28 20:04:58 [info     ] 	points = {}                   [Run3ModelGen.modelgen]
2024-08-28 20:04:58 [info     ] 	prior = fixed                 [Run3ModelGen.modelgen]
2024-08-28 20:04:58 [info     ] 	num_models = 1                [Run3ModelGen.modelgen]
2024-08-28 20:04:58 [info     ] 	isGMSB = False                [Run3ModelGen.modelgen]
2024-08-28 20:04:58 [info     ] 	parameters =                  [Run3ModelGen.modelgen]
2024-08-28 20:04:58 [info     ] 		tanb: [60]                   [Run3ModelGen.modelgen]
2024-08-28 20:04:58 [info     ] 		M_1: [-2000.0]               [Run3ModelGen.modelgen]
2024-08-28 20:04:58 [info     ] 		M_2: [2000]                  [Run3ModelGen.modelgen]
2024-08-28 20:04:58 [info     ] 		M_3: [4000]                  [Run3ModelGen.modelgen]
2024-08-28 20:04:58 [info     ] 		AT: [4000]                   [Run3ModelGen.modelgen]
2024-08-28 20:04:58 [info     ] 		Ab: [2000]                   [Run3ModelGen.modelgen]
2024-08-28 20:04:58 [info     ] 		Atau: [2000]                 [Run3ModelGen.modelgen]
2024-08-28 20:04:58 [info     ] 		mu: [2000]                   [Run3ModelGen.modelgen]
2024-08-28 20:04:58 [info     ] 		mA: [2000]                   [Run3ModelGen.modelgen]
2024-08-28 20:04:58 [info     ] 		meL: [2000]                  [Run3ModelGen.modelgen]
2024-08-28 20:04:58 [info     ] 		mtauL: [2000]                [Run3ModelGen.modelgen]
2024-08-28 20:04:58 [info     ] 		meR: [2000]                  [Run3ModelGen.modelgen]
2024-08-28 20:04:58 [info     ] 		mtauR: [2000]                [Run3ModelGen.modelgen]
2024-08-28 20:04:58 [info     ] 		mqL1: [4000]                 [Run3ModelGen.modelgen]
2024-08-28 20:04:58 [info     ] 		mqL3: [4000]                 [Run3ModelGen.modelgen]
2024-08-28 20:04:58 [info     ] 		muR: [4000]                  [Run3ModelGen.modelgen]
2024-08-28 20:04:58 [info     ] 		mtR: [4000]                  [Run3ModelGen.modelgen]
2024-08-28 20:04:58 [info     ] 		mdR: [4000]                  [Run3ModelGen.modelgen]
2024-08-28 20:04:58 [info     ] 		mbR: [4000]                  [Run3ModelGen.modelgen]
2024-08-28 20:04:58 [info     ] 	steps =                       [Run3ModelGen.modelgen]
2024-08-28 20:04:58 [info     ] 		{'name': 'prep_input', 'output_dir': 'input', 'prefix': 'IN'} [Run3ModelGen.modelgen]
2024-08-28 20:04:58 [info     ] 		{'name': 'SPheno', 'input_dir': 'input', 'output_dir': 'SPheno', 'log_dir': 'SPheno_log', 'prefix': 'SP'} [Run3ModelGen.modelgen]
2024-08-28 20:04:58 [info     ] 		{'name': 'softsusy', 'input_dir': 'input', 'output_dir': 'softsusy', 'prefix': 'SS'} [Run3ModelGen.modelgen]
2024-08-28 20:04:58 [info     ] 		{'name': 'micromegas', 'input_dir': 'SPheno', 'output_dir': 'micromegas', 'prefix': 'MO'} [Run3ModelGen.modelgen]
2024-08-28 20:04:58 [info     ] 		{'name': 'superiso', 'input_dir': 'SPheno', 'output_dir': 'superiso', 'prefix': 'SI'} [Run3ModelGen.modelgen]
2024-08-28 20:04:58 [info     ] 		{'name': 'gm2calc', 'input_dir': 'SPheno', 'output_dir': 'gm2calc', 'prefix': 'GM2'} [Run3ModelGen.modelgen]
2024-08-28 20:04:58 [info     ] 		{'name': 'evade', 'input_dir': 'SPheno', 'output_dir': 'evade', 'prefix': 'EV'} [Run3ModelGen.modelgen]
2024-08-28 20:04:58 [info     ] Starting Model Generation...   [Run3ModelGen.modelgen]
2024-08-28 20:04:58 [info     ] Setting up output directories for scan [Run3ModelGen.modelgen]
2024-08-28 20:04:58 [info     ] Generating Model: 0.slha (1/1) [Run3ModelGen.modelgen]
2024-08-28 20:04:58 [info     ] 	running step: prep_input      [Run3ModelGen.modelgen]
2024-08-28 20:04:58 [info     ] 	running step: SPheno          [Run3ModelGen.modelgen]
2024-08-28 20:04:58 [info     ] 	running step: softsusy        [Run3ModelGen.modelgen]
2024-08-28 20:04:58 [info     ] 	running step: micromegas      [Run3ModelGen.modelgen]
2024-08-28 20:04:59 [info     ] 	running step: superiso        [Run3ModelGen.modelgen]
2024-08-28 20:05:00 [info     ] 	running step: gm2calc         [Run3ModelGen.modelgen]
2024-08-28 20:05:00 [info     ] 	running step: evade           [Run3ModelGen.modelgen]
2024-08-28 20:05:00 [info     ] Finished generating models! Stats for successful generations (successful/attempted): [Run3ModelGen.modelgen]
2024-08-28 20:05:00 [info     ] 	prep_input: 1/1.              [Run3ModelGen.modelgen]
2024-08-28 20:05:00 [info     ] 	SPheno: 1/1.                  [Run3ModelGen.modelgen]
2024-08-28 20:05:00 [info     ] 	softsusy: 1/1.                [Run3ModelGen.modelgen]
2024-08-28 20:05:00 [info     ] 	micromegas: 1/1.              [Run3ModelGen.modelgen]
2024-08-28 20:05:00 [info     ] 	superiso: 1/1.                [Run3ModelGen.modelgen]
2024-08-28 20:05:00 [info     ] 	gm2calc: 1/1.                 [Run3ModelGen.modelgen]
2024-08-28 20:05:00 [info     ] 	evade: 0/1.                   [Run3ModelGen.modelgen]
2024-08-28 20:05:00 [info     ] Will make NTuple for: /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/scans/scan_20, with 1 models [Run3ModelGen.ntupling]
2024-08-28 20:05:00 [info     ] 	Filling values for model: 0   [Run3ModelGen.ntupling]
2024-08-28 20:05:00 [info     ] Saving NTuple in /u/dvoss/al_pmssmwithgp/Run3ModelGen/source/Run3ModelGen/scans/scan_20/ntuple.0.0.root ... [Run3ModelGen.ntupling]
Completed iteration 20
All iterations completed.
                                  
================================= 
Global information about the job: 
================================= 
  
Job owner: dvoss(54815)
Job name:  test
Node list: ravg1012
Job start: Wed Aug 28 19:49:39 CEST 2024
Job end:   Wed Aug 28 20:05:06 CEST 2024
Work dir:  /raven/u/dvoss/al_pmssmwithgp/model
Command:   /raven/u/dvoss/al_pmssmwithgp/model/slurm/single_job.sbatch
  
  
  
==========================================================================================
Information on jobsteps (Note: MaxRSS/AveRSS is the maximum/average over all 
tasks of the per-task memory high-water marks; cf. "man sacct"): 
==========================================================================================
  
JobID            JobName NNodes NTasks  NCPUS       MaxRSS       AveRSS    Elapsed ExitCode
------------- ---------- ------ ------ ------ ------------ ------------ ---------- --------
12544767            test      1             4                             00:15:27      0:0
12544767.0        job.sh      1      1      4     2394.35M     2394.35M   00:15:12      0:0
  
Maximum memory per node: 2.510657 GB (defined as MaxRSS*Ntasks/NNodes)
CPU utilization: 23.7 %
  
